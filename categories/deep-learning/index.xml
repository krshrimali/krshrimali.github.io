<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Deep Learning on Kushashwa Ravi Shrimali (Kush)</title>
    <link>https://krshrimali.github.io/categories/deep-learning/</link>
    <description>Recent content in Deep Learning on Kushashwa Ravi Shrimali (Kush)</description>
    <generator>Hugo -- gohugo.io</generator>
    <copyright>&lt;a href=&#34;https://creativecommons.org/licenses/by-nc/4.0/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;CC BY-NC 4.0&lt;/a&gt;</copyright>
    <lastBuildDate>Tue, 15 Sep 2020 00:00:00 +0000</lastBuildDate><atom:link href="https://krshrimali.github.io/categories/deep-learning/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>Releasing Docker Container and Binder for using Xeus-Cling, Libtorch and OpenCV in C&#43;&#43;</title>
      <link>https://krshrimali.github.io/posts/2020/09/releasing-docker-container-and-binder-for-using-xeus-cling-libtorch-and-opencv-in-c-/</link>
      <pubDate>Tue, 15 Sep 2020 00:00:00 +0000</pubDate>
      
      <guid>https://krshrimali.github.io/posts/2020/09/releasing-docker-container-and-binder-for-using-xeus-cling-libtorch-and-opencv-in-c-/</guid>
      <description>
          
            &lt;h4&gt;Releasing Docker Container and Binder for using Xeus-Cling, Libtorch and OpenCV in C++&lt;/h4&gt;
          
          &lt;p&gt;Today, I am elated to share Docker image for &lt;code&gt;OpenCV&lt;/code&gt;, &lt;code&gt;Libtorch&lt;/code&gt; and &lt;code&gt;Xeus-Cling&lt;/code&gt;. We&amp;rsquo;ll discuss how to use the &lt;code&gt;dockerfile&lt;/code&gt; and &lt;code&gt;binder&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/krshrimali/blog/main/assets/cover-images/Cover-Docker-Binder.jpg&#34; alt=&#34;&#34;&gt;&lt;/p&gt;
&lt;p&gt;Before I move on, the credits for creating and maintaining Docker image goes to &lt;a href=&#34;https://github.com/vishwesh5&#34;&gt;Vishwesh Ravi Shrimali&lt;/a&gt;. He has been working on some cool stuff, please do get in touch with him if you&amp;rsquo;re interested to know.&lt;/p&gt;
&lt;p&gt;First question in your mind would be, &lt;strong&gt;Why use Docker or Binder?&lt;/strong&gt; The answer to it lies in the frequency of queries on &lt;a href=&#34;http://www.discuss.pytorch.org&#34;&gt;the discussion forum of PyTorch&lt;/a&gt; and Stackoverflow on &lt;strong&gt;Installation of Libtorch with OpenCV in Windows/Linux/OSX&lt;/strong&gt;. I&amp;rsquo;ve had nightmares setting up the Windows system myself for &lt;code&gt;Libtorch&lt;/code&gt; and nothing could be better than using &lt;code&gt;Docker&lt;/code&gt;. Read on, to know why.&lt;/p&gt;
&lt;h2 id=&#34;installing-docker-on-mac-os&#34;&gt;Installing Docker on Mac OS&lt;/h2&gt;
&lt;p&gt;To install docker (community edition - CE) desktop in Mac OS system, simply navigate to the Stable Channel section &lt;a href=&#34;https://docs.docker.com/v17.12/docker-for-mac/install/#download-docker-for-mac&#34;&gt;here&lt;/a&gt;. Once setup, you can use docker (command line and desktop). Once done, navigate to &lt;a href=&#34;https://docs.docker.com/v17.12/docker-for-mac/install/#install-and-run-docker-for-mac&#34;&gt;Install and run Docker for Mac&lt;/a&gt; section and get used to the commands.&lt;/p&gt;
&lt;h2 id=&#34;installing-docker-on-ubuntu&#34;&gt;Installing Docker on Ubuntu&lt;/h2&gt;
&lt;p&gt;Before moving on, please consider reading the requirements to install Docker Community Edition](&lt;a href=&#34;https://docs.docker.com/v17.12/install/linux/docker-ce/ubuntu/)&#34;&gt;https://docs.docker.com/v17.12/install/linux/docker-ce/ubuntu/)&lt;/a&gt;. For the steps to install &lt;code&gt;Docker CE&lt;/code&gt;, refer &lt;a href=&#34;https://docs.docker.com/v17.12/install/linux/docker-ce/ubuntu/#install-docker-ce-1&#34;&gt;this&lt;/a&gt;.&lt;/p&gt;
&lt;h2 id=&#34;installing-docker-on-windows&#34;&gt;Installing Docker on Windows&lt;/h2&gt;
&lt;p&gt;To install Docker on Windows, download docker (stable channel) from &lt;a href=&#34;https://docs.docker.com/v17.12/docker-for-windows/install/#download-docker-for-windows&#34;&gt;here&lt;/a&gt;. The installation steps to install &lt;code&gt;Docker Desktop&lt;/code&gt; on Windows can be found &lt;a href=&#34;https://docs.docker.com/v17.12/docker-for-windows/install/#install-docker-for-windows-desktop-app&#34;&gt;here&lt;/a&gt;.&lt;/p&gt;
&lt;h2 id=&#34;using-docker-image&#34;&gt;Using Docker Image&lt;/h2&gt;
&lt;ol&gt;
&lt;li&gt;Fetch the docker image: &lt;code&gt;docker pull vishwesh5/libtorch-opencv:opencv-4-1-0&lt;/code&gt;. This shall take a lot of time, so sit back and relax.&lt;/li&gt;
&lt;li&gt;Run: &lt;code&gt;docker run -p 5000:5000 -p 8888:8888 -it vishwesh5/libtorch-opencv:opencv-4-1-0 /bin/bash&lt;/code&gt;.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;To know more about these commands, check out the references section.&lt;/p&gt;
&lt;p&gt;Once done, you&amp;rsquo;ll see your terminal showing another username: &lt;code&gt;jovyan&lt;/code&gt;. You&amp;rsquo;ve entered the docker image, congratulations! No need to setup &lt;code&gt;OpenCV&lt;/code&gt; or &lt;code&gt;Libtorch&lt;/code&gt;. Vishwesh has done it for you!&lt;/p&gt;
&lt;p&gt;Now since you have entered the docker container successfully, it should look something similar to this:&lt;/p&gt;
&lt;!-- raw HTML omitted --&gt;
&lt;p&gt;Time to test &lt;code&gt;Libtorch&lt;/code&gt;. Let&amp;rsquo;s go ahead and test a simple VGG-Net on MNIST dataset using Libtorch.&lt;/p&gt;
&lt;h2 id=&#34;testing-docker-image&#34;&gt;Testing Docker Image&lt;/h2&gt;
&lt;ol&gt;
&lt;li&gt;Clone the repository containing code for &lt;strong&gt;Digit Classification using Libtorch on MNIST dataset&lt;/strong&gt;: &lt;code&gt;git clone https://github.com/krshrimali/Digit-Recognition-MNIST-SVHN-PyTorch-CPP.git&lt;/code&gt;. Change directory to the cloned repository.&lt;/li&gt;
&lt;li&gt;Download the MNIST data from &lt;a href=&#34;http://yann.lecun.com/exdb/mnist/&#34;&gt;http://yann.lecun.com/exdb/mnist/&lt;/a&gt;. Download &lt;code&gt;train-images-idx3-ubyte.gz&lt;/code&gt; and &lt;code&gt;train-labels-idx1-ubyte.gz&lt;/code&gt; files for training the VGG-Net. You can skip downloading the test data for now. Use &lt;code&gt;gunzip &amp;lt;file_path&amp;gt;&lt;/code&gt; to extract the training images and labels, and put them in the &lt;code&gt;data/&lt;/code&gt; folder inside the clones repository.&lt;/li&gt;
&lt;li&gt;Create a &lt;code&gt;build&lt;/code&gt; folder: &lt;code&gt;mkdir build&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;Run the CMake Configuration using: &lt;code&gt;cmake -DCMAKE_PREFIX_PATH=/opt/libtorch ..&lt;/code&gt;. The result should be similar to something in the figure below.&lt;/li&gt;
&lt;li&gt;Build the code using &lt;code&gt;make&lt;/code&gt; command: &lt;code&gt;make&lt;/code&gt;.&lt;/li&gt;
&lt;li&gt;Execute the code, and that&amp;rsquo;s it. Have fun learning.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/krshrimali/blog/main/assets/blogs/Docker-Image-2.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt;
&lt;h2 id=&#34;testing-docker-image-with-xeus-cling&#34;&gt;Testing Docker Image with Xeus-Cling&lt;/h2&gt;
&lt;p&gt;Let&amp;rsquo;s test the Docker Image with Xeus-Cling.&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Run &lt;code&gt;jupyter notebook&lt;/code&gt; command in the console and copy the token from the url provided.&lt;/li&gt;
&lt;li&gt;Open &lt;code&gt;http://localhost:8888&lt;/code&gt; in your browser. Note that the port address (&lt;code&gt;8888&lt;/code&gt;) comes from &lt;code&gt;-p 8888:8888&lt;/code&gt; in the &lt;code&gt;docker run&lt;/code&gt; command. You can change that if you want. Enter the token when asked.&lt;/li&gt;
&lt;li&gt;Start a new notebook using &lt;code&gt;C++XX&lt;/code&gt; kernel.&lt;/li&gt;
&lt;li&gt;Include and load libraries in the first cell using: &lt;code&gt;#include &amp;quot;includeLibraries.h&amp;quot;&lt;/code&gt;. This should do all the stuff for you.&lt;/li&gt;
&lt;li&gt;Start doing experiments using Xeus-Cling now.&lt;/li&gt;
&lt;/ol&gt;
&lt;h2 id=&#34;using-binder&#34;&gt;Using Binder&lt;/h2&gt;
&lt;p&gt;And! What if you just want to try &lt;code&gt;Libtorch&lt;/code&gt; or show it to the students? What if you are on a remote PC, and can&amp;rsquo;t install Docker? Well, here is the &lt;code&gt;Binder&lt;/code&gt;: &lt;a href=&#34;https://mybinder.org/v2/gh/vishwesh5/torch-binder/master&#34;&gt;https://mybinder.org/v2/gh/vishwesh5/torch-binder/master&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;Go to the above link and a notebook shall open.&lt;/p&gt;
&lt;p&gt;Create a new notebook and start with: &lt;code&gt;#include &amp;quot;includeLibraries.h&amp;quot;&lt;/code&gt; first and then start testing.&lt;/p&gt;
&lt;h2 id=&#34;acknowledgements&#34;&gt;Acknowledgements&lt;/h2&gt;
&lt;p&gt;Thanks to Vishwesh Ravi Shrimali, for creating the docker container and binder for this post.&lt;/p&gt;
&lt;h2 id=&#34;references&#34;&gt;References&lt;/h2&gt;
&lt;ol&gt;
&lt;li&gt;&lt;a href=&#34;https://www.learnopencv.com/install-opencv-docker-image-ubuntu-macos-windows&#34;&gt;Install OpenCV Docker Image on Ubuntu, MacOS or Windows by Vishwesh Ravi Shrimali&lt;/a&gt;.&lt;/li&gt;
&lt;/ol&gt;

      </description>
    </item>
    
    <item>
      <title>[Training and Results] Deep Convolutional Generative Adversarial Networks on CelebA Dataset using PyTorch C&#43;&#43; API</title>
      <link>https://krshrimali.github.io/posts/2020/02/training-and-results-deep-convolutional-generative-adversarial-networks-on-celeba-dataset-using-pytorch-c-api/</link>
      <pubDate>Sun, 23 Feb 2020 00:00:00 +0000</pubDate>
      
      <guid>https://krshrimali.github.io/posts/2020/02/training-and-results-deep-convolutional-generative-adversarial-networks-on-celeba-dataset-using-pytorch-c-api/</guid>
      <description>
          
            &lt;h4&gt;[Training and Results] Deep Convolutional Generative Adversarial Networks on CelebA Dataset using PyTorch C++ API&lt;/h4&gt;
          
          &lt;p&gt;It&amp;rsquo;s been around 5 months since I released my last blog on &lt;a href=&#34;https://krshrimali.github.io/posts/2019/09/deep-convolutional-generative-adversarial-networks-review-and-implementation-using-pytorch-c-api/&#34;&gt;DCGAN Review and Implementation using PyTorch C++ API&lt;/a&gt; and I&amp;rsquo;ve missed writing blogs badly! Straight the to the point, I&amp;rsquo;m back!&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/krshrimali/blog/main/assets/cover-images/Cover-DCGAN-2.jpg&#34; alt=&#34;&#34;&gt;&lt;/p&gt;
&lt;p&gt;But before we start, the PyTorch C++ Frontend has gone through several changes and thanks to the awesome contributors around the world, it resembles the Python API more than it ever did! Since a lot of things have changed, I have also updated my previous blogs (tested on 1.4 Stable build).&lt;/p&gt;
&lt;h2 id=&#34;what-has-changed&#34;&gt;What has changed?&lt;/h2&gt;
&lt;p&gt;There have been major changes in the PyTorch C++ Frontend API (Libtorch) and we&amp;rsquo;ll be discussing some of them which were related to our implementation on DCGAN. Let&amp;rsquo;s see, what parts of our code have changed in the recent Libtorch version. Well, the frontend API of PyTorch in C++ resembles closely to Python now:&lt;/p&gt;
&lt;p&gt;For what concerns our code on DCGAN, quoting the author (Will Feng) of PR &lt;a href=&#34;https://github.com/pytorch/pytorch/pull/28917&#34;&gt;#28917&lt;/a&gt;.&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;In Conv{1,2,3}dOptions:
- with_bias is renamed to bias.
- input_channels is renamed to in_channels.
- output_channels is renamed to out_channels.
- The value of transposed doesn&amp;rsquo;t affect the behavior of Conv{1,2,3}d layers anymore. Users should migrate their code to use ConvTranspose{1,2,3}d layers instead.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;So, starting first, we need to change &lt;code&gt;with_bias&lt;/code&gt; to &lt;code&gt;bias&lt;/code&gt; in our model definitions. The generator class in DCGAN uses Transposed Convolutions, and that&amp;rsquo;s why we need to migrate from &lt;code&gt;torch::nn::Conv2dOptions&lt;/code&gt; class to &lt;code&gt;torch::nn::ConvTranspose2dOptions&lt;/code&gt; (this is because using &lt;code&gt;.transposed(true/false)&lt;/code&gt; does not work anymore on &lt;code&gt;torch::nn::Conv2dOptions&lt;/code&gt;).&lt;/p&gt;
&lt;p&gt;That is all for the changes we needed to make. To make it easy to track changes and use the code I wrote, I&amp;rsquo;ve made the project public on &lt;a href=&#34;https://github.com/BuffetCodes/DCGAN-CelebA-PyTorch-CPP.git&#34;&gt;GitHub&lt;/a&gt;. Feel free to file an issue in case you hit a bug/error.&lt;/p&gt;
&lt;p&gt;Time to talk about results!&lt;/p&gt;
&lt;h2 id=&#34;results&#34;&gt;Results&lt;/h2&gt;
&lt;p&gt;The aim of this blog is to get DCGAN running on our celebA dataset using PyTorch C++ Frontend API. I&amp;rsquo;m in no way aiming to produce the best possible results. I trained the DCGAN network on celebA dataset for 10 epochs. In order to visualize results, for every checkpoint (where we save our models), we pass a sample noise image (64 images here) to the generator and save the output:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-cpp&#34; data-lang=&#34;cpp&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#75715e&#34;&gt;// equivalent to using torch.no_grad() in Python
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#75715e&#34;&gt;&lt;/span&gt;&lt;span style=&#34;color:#66d9ef&#34;&gt;auto&lt;/span&gt; options &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; torch&lt;span style=&#34;color:#f92672&#34;&gt;::&lt;/span&gt;TensorOptions().device(device).requires_grad(false);
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#75715e&#34;&gt;// netG is our sequential generator network
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#75715e&#34;&gt;// args.nz = 100 in my case
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#75715e&#34;&gt;&lt;/span&gt;torch&lt;span style=&#34;color:#f92672&#34;&gt;::&lt;/span&gt;Tensor samples &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; netG&lt;span style=&#34;color:#f92672&#34;&gt;-&amp;gt;&lt;/span&gt;forward(torch&lt;span style=&#34;color:#f92672&#34;&gt;::&lt;/span&gt;randn({&lt;span style=&#34;color:#ae81ff&#34;&gt;64&lt;/span&gt;, args.nz, &lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;}, options));
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#75715e&#34;&gt;// save the output
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#75715e&#34;&gt;&lt;/span&gt;torch&lt;span style=&#34;color:#f92672&#34;&gt;::&lt;/span&gt;save(samples, torch&lt;span style=&#34;color:#f92672&#34;&gt;::&lt;/span&gt;str(&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;dcgan-sample-&amp;#34;&lt;/span&gt;, &lt;span style=&#34;color:#f92672&#34;&gt;++&lt;/span&gt;checkpoint_counter, &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;.pt&amp;#34;&lt;/span&gt;));
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;Once we have the saved output, we can load the file and produce output (find the &lt;code&gt;display_samples.py&lt;/code&gt; file in the &lt;a href=&#34;https://krshrimali.github.io/posts/2019/09/deep-convolutional-generative-adversarial-networks-review-and-implementation-using-pytorch-c-api/&#34;&gt;GitHub repo for this blog&lt;/a&gt;). Here is how the output looks like, after 10 epochs of training:&lt;/p&gt;
&lt;!-- raw HTML omitted --&gt;
&lt;p&gt;And how about an animation?&lt;/p&gt;
&lt;!-- raw HTML omitted --&gt;
&lt;p&gt;Isn&amp;rsquo;t this amazing?&lt;/p&gt;
&lt;p&gt;That&amp;rsquo;s it for this blog. See you around! :)&lt;/p&gt;

      </description>
    </item>
    
    <item>
      <title>Deep Convolutional Generative Adversarial Networks: Review and Implementation using PyTorch C&#43;&#43; API</title>
      <link>https://krshrimali.github.io/posts/2019/09/deep-convolutional-generative-adversarial-networks-review-and-implementation-using-pytorch-c-api/</link>
      <pubDate>Sun, 15 Sep 2019 00:00:00 +0000</pubDate>
      
      <guid>https://krshrimali.github.io/posts/2019/09/deep-convolutional-generative-adversarial-networks-review-and-implementation-using-pytorch-c-api/</guid>
      <description>
          
            &lt;h4&gt;Deep Convolutional Generative Adversarial Networks: Review and Implementation using PyTorch C++ API&lt;/h4&gt;
          
          &lt;p&gt;I&amp;rsquo;m pleased to start a series of blogs on GANs and their implementation with PyTorch C++ API. We&amp;rsquo;ll be starting with one of the initial GANs - DCGANs (Deep Convolutional Generative Adversarial Networks).&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/krshrimali/blog/main/assets/cover-images/Cover-DCGAN.jpg&#34; alt=&#34;&#34;&gt;&lt;/p&gt;
&lt;p&gt;The authors (Soumith Chintala, Radford and Luke Metz) in &lt;a href=&#34;https://arxiv.org/pdf/1511.06434.pdf&#34;&gt;this&lt;/a&gt; Seminal Paper on DCGANs introduced DCGANs to the world like this:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;We introduce a class of CNNs called deep convolutional generative adversarial networks (DCGANs), that have certain architectural constraints, and demonstrate that they are a strong candidate for unsupervised learning. Training on various image datasets, we show convincing evidence that our deep convolutional adversarial pair learns a hierarchy of representations from object parts to scenes in both the generator and discriminator. Additionally, we use the learned features for novel tasks - demonstrating their applicability as general image representations.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;Even though, the introduction to DCGANs is quite lucid, but here are some points to note:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;DCGANs are a class of Convolutional Neural Networks.&lt;/li&gt;
&lt;li&gt;They are a strong candidate for Unsupervised Learning.&lt;/li&gt;
&lt;li&gt;They are applicable as general image representations as well.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;Let&amp;rsquo;s go ahead and see what exactly is DCGAN?&lt;/p&gt;
&lt;h2 id=&#34;introduction-to-dcgan&#34;&gt;Introduction to DCGAN&lt;/h2&gt;
&lt;p&gt;At the time when this paper was released, there was quite a focus on Supervised Learning. The paper aimed at bridging the gap between Unsupervised Learning and Supervised Learning. DCGANs are a way to understand and extract important feature representations from a dataset and generate good image representations by training.&lt;/p&gt;
&lt;p&gt;Any Generative Adversarial Network has 2 major components: a Generator and a Discriminator. The tasks for both of them are simple.&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;&lt;strong&gt;Generator&lt;/strong&gt;: Generates Images similar to the data distribution such that Discriminator can not distinguish it with the original data.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Discriminator&lt;/strong&gt;: Discriminator has a task on accurately distinguishing between the image from the generator and from the data distribution. It basically has to recognize an image as fake or real, correctly.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;Both Generator and Discriminator tasks can be represented beautifully with the following equation:&lt;/p&gt;
&lt;!-- raw HTML omitted --&gt;
&lt;p&gt;The above equation, shows how the Generator and Discriminator plays min-max game.&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;The &lt;strong&gt;Generator tries to minimize the loss function.&lt;/strong&gt; It follows up with two cases:
&lt;ol&gt;
&lt;li&gt;&lt;strong&gt;When the data is from the data distribution:&lt;/strong&gt; Generator has a task of forcing the Discriminator to predict the data as fake.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;When data is from the Generator:&lt;/strong&gt; Generator has a task of forcing the Discriminator to predict the data as real.&lt;/li&gt;
&lt;/ol&gt;
&lt;/li&gt;
&lt;li&gt;The &lt;strong&gt;Discriminator tries to maximize the loss function.&lt;/strong&gt; It follows up with two cases:
&lt;ol&gt;
&lt;li&gt;&lt;strong&gt;When the data is from the data distribution:&lt;/strong&gt; Discriminator tries to predict the data as real.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;When the data is from the Generator:&lt;/strong&gt; Discriminator tries to predict the data as fake.&lt;/li&gt;
&lt;/ol&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;Fundamentally, the Generator is trying to fool the Discriminator and the Discriminator is trying not to get fooled with. Because of it&amp;rsquo;s analogy, it&amp;rsquo;s also called a police-thief game. (Police is the Discriminator and thief is the Generator).&lt;/p&gt;
&lt;p&gt;We have good enough discussion on GANs, to kickstart discussion on DCGANs. Let&amp;rsquo;s go ahead and see what changes they proposed on common CNNs:&lt;/p&gt;
&lt;p&gt;Changes in the &lt;strong&gt;Generator&lt;/strong&gt;:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Spatial Pooling Layers such as MaxPool Layers were replaced with Fractional-Strided Convolutions (a.k.a Transposed Convolutions). This allows the network to learn it&amp;rsquo;s own spatial downsampling, instead of explicitly mentioning the downsampling parameters by Max Pooling.&lt;/li&gt;
&lt;li&gt;Use BatchNorm in the Generator.&lt;/li&gt;
&lt;li&gt;Remove Fully Connected layers for deeper architectures.&lt;/li&gt;
&lt;li&gt;Use ReLU activation function for all the layers except the output layer (which uses Tanh activation function).&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;Changes in the &lt;strong&gt;Discriminator&lt;/strong&gt;:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Spatial Pooling Layers such as MaxPool layers were replaced with Strided Convolutions.&lt;/li&gt;
&lt;li&gt;Use BatchNorm in the Discriminator.&lt;/li&gt;
&lt;li&gt;Remove FC layers for deeper architectures.&lt;/li&gt;
&lt;li&gt;Use LeakyReLU activation function for all the layers in the Discriminator.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;&lt;!-- raw HTML omitted --&gt;&lt;!-- raw HTML omitted --&gt;Generator of the DCGAN used for LSUN scene modeling. Source: &lt;a href=&#34;https://arxiv.org/pdf/1511.06434.pdf&#34;&gt;https://arxiv.org/pdf/1511.06434.pdf&lt;/a&gt;&lt;!-- raw HTML omitted --&gt;&lt;/p&gt;
&lt;p&gt;As you would note in the above architecture, there is absence of spatial pooling layers and fully connected layers.&lt;/p&gt;
&lt;p&gt;&lt;!-- raw HTML omitted --&gt;&lt;!-- raw HTML omitted --&gt;Discriminator of the DCGAN used for LSUN scene modeling. Source: &lt;a href=&#34;https://github.com/ChengBinJin/DCGAN-TensorFlow&#34;&gt;https://github.com/ChengBinJin/DCGAN-TensorFlow&lt;/a&gt;&lt;!-- raw HTML omitted --&gt;&lt;/p&gt;
&lt;p&gt;Notably again, there are no pooling and fully connected layers (except the last layer).&lt;/p&gt;
&lt;p&gt;Let&amp;rsquo;s start with defining the architectures of both Generators and Discriminators using PyTorch C++ API. I used the Object Oriented approach by making class, each for Generator and Discriminator. Note that each of them are a type of CNNs, and also inherit functions (or methods) from &lt;code&gt;torch::nn::Module&lt;/code&gt; class.&lt;/p&gt;
&lt;p&gt;As mentioned before, Generator uses Transposed Convolutional Layers and has no pooling and FC layers. It also uses ReLU Activation Function (except the last layer). The parameters used for the Generator include:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;&lt;code&gt;dataroot&lt;/code&gt;: (type: &lt;code&gt;std::string&lt;/code&gt;) Path of the dataset&amp;rsquo;s root directory.&lt;/li&gt;
&lt;li&gt;&lt;code&gt;workers&lt;/code&gt;: (type: &lt;code&gt;int&lt;/code&gt;) Having more &lt;code&gt;workers&lt;/code&gt; will increase CPU memory usage. (Check this link &lt;a href=&#34;https://discuss.pytorch.org/t/guidelines-for-assigning-num-workers-to-dataloader/813/2&#34;&gt;for more details&lt;/a&gt;)&lt;/li&gt;
&lt;li&gt;&lt;code&gt;batch_size&lt;/code&gt;: (type: &lt;code&gt;int&lt;/code&gt;) Batch Size to consider.&lt;/li&gt;
&lt;li&gt;&lt;code&gt;image_size&lt;/code&gt;: (type: &lt;code&gt;int&lt;/code&gt;) Size of the image to resize it to.&lt;/li&gt;
&lt;li&gt;&lt;code&gt;nc&lt;/code&gt;: (type: &lt;code&gt;int&lt;/code&gt;) Number of channels in the Input Image.&lt;/li&gt;
&lt;li&gt;&lt;code&gt;nz&lt;/code&gt;: (type: &lt;code&gt;int&lt;/code&gt;) Length of latent vector, from which the input image is taken.&lt;/li&gt;
&lt;li&gt;&lt;code&gt;ngf&lt;/code&gt;: (type &lt;code&gt;int&lt;/code&gt;) Depth of feature maps carried through the generator.&lt;/li&gt;
&lt;li&gt;&lt;code&gt;num_epochs&lt;/code&gt;: (type &lt;code&gt;int&lt;/code&gt;) Number of epochs for which the model is trained.&lt;/li&gt;
&lt;li&gt;&lt;code&gt;lr&lt;/code&gt;: (type &lt;code&gt;float&lt;/code&gt;) Learning Rate for training. Authors described it to be 0.0002&lt;/li&gt;
&lt;li&gt;&lt;code&gt;beta1&lt;/code&gt;: (type: &lt;code&gt;float&lt;/code&gt;) Hyperparameter for Optimizer used (Adam).&lt;/li&gt;
&lt;li&gt;&lt;code&gt;ngpu&lt;/code&gt;: (type: &lt;code&gt;int&lt;/code&gt;) Number of GPUs available to use. (use &lt;code&gt;0&lt;/code&gt; if no GPU available)&lt;/li&gt;
&lt;/ol&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-cpp&#34; data-lang=&#34;cpp&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#66d9ef&#34;&gt;class&lt;/span&gt; &lt;span style=&#34;color:#a6e22e&#34;&gt;Generator&lt;/span&gt; &lt;span style=&#34;color:#f92672&#34;&gt;:&lt;/span&gt; &lt;span style=&#34;color:#66d9ef&#34;&gt;public&lt;/span&gt; torch&lt;span style=&#34;color:#f92672&#34;&gt;::&lt;/span&gt;nn&lt;span style=&#34;color:#f92672&#34;&gt;::&lt;/span&gt;Module {
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#66d9ef&#34;&gt;private&lt;/span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;:&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    std&lt;span style=&#34;color:#f92672&#34;&gt;::&lt;/span&gt;string dataroot;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#66d9ef&#34;&gt;int&lt;/span&gt; workers;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#66d9ef&#34;&gt;int&lt;/span&gt; batch_size;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#66d9ef&#34;&gt;int&lt;/span&gt; image_size;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#66d9ef&#34;&gt;int&lt;/span&gt; nc;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#66d9ef&#34;&gt;int&lt;/span&gt; nz;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#66d9ef&#34;&gt;int&lt;/span&gt; ngf;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#66d9ef&#34;&gt;int&lt;/span&gt; num_epochs;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#66d9ef&#34;&gt;float&lt;/span&gt; lr;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#66d9ef&#34;&gt;float&lt;/span&gt; beta1;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#66d9ef&#34;&gt;int&lt;/span&gt; ngpu;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#66d9ef&#34;&gt;public&lt;/span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;:&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    torch&lt;span style=&#34;color:#f92672&#34;&gt;::&lt;/span&gt;nn&lt;span style=&#34;color:#f92672&#34;&gt;::&lt;/span&gt;Sequential main;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    Generator(std&lt;span style=&#34;color:#f92672&#34;&gt;::&lt;/span&gt;string dataroot_ &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;data/celeba&amp;#34;&lt;/span&gt;, &lt;span style=&#34;color:#66d9ef&#34;&gt;int&lt;/span&gt; workers_ &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; &lt;span style=&#34;color:#ae81ff&#34;&gt;2&lt;/span&gt;, &lt;span style=&#34;color:#66d9ef&#34;&gt;int&lt;/span&gt; batch_size_ &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; &lt;span style=&#34;color:#ae81ff&#34;&gt;128&lt;/span&gt;, &lt;span style=&#34;color:#66d9ef&#34;&gt;int&lt;/span&gt; image_size_ &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; &lt;span style=&#34;color:#ae81ff&#34;&gt;64&lt;/span&gt;, &lt;span style=&#34;color:#66d9ef&#34;&gt;int&lt;/span&gt; nc_ &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; &lt;span style=&#34;color:#ae81ff&#34;&gt;3&lt;/span&gt;, &lt;span style=&#34;color:#66d9ef&#34;&gt;int&lt;/span&gt; nz_ &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; &lt;span style=&#34;color:#ae81ff&#34;&gt;100&lt;/span&gt;, &lt;span style=&#34;color:#66d9ef&#34;&gt;int&lt;/span&gt; ngf_ &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; &lt;span style=&#34;color:#ae81ff&#34;&gt;64&lt;/span&gt;, &lt;span style=&#34;color:#66d9ef&#34;&gt;int&lt;/span&gt; ndf_ &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; &lt;span style=&#34;color:#ae81ff&#34;&gt;64&lt;/span&gt;, &lt;span style=&#34;color:#66d9ef&#34;&gt;int&lt;/span&gt; num_epochs_ &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; &lt;span style=&#34;color:#ae81ff&#34;&gt;5&lt;/span&gt;, &lt;span style=&#34;color:#66d9ef&#34;&gt;float&lt;/span&gt; lr_ &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; &lt;span style=&#34;color:#ae81ff&#34;&gt;0.0002&lt;/span&gt;, &lt;span style=&#34;color:#66d9ef&#34;&gt;float&lt;/span&gt; beta1_ &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; &lt;span style=&#34;color:#ae81ff&#34;&gt;0.5&lt;/span&gt;, &lt;span style=&#34;color:#66d9ef&#34;&gt;int&lt;/span&gt; ngpu_ &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; &lt;span style=&#34;color:#ae81ff&#34;&gt;0&lt;/span&gt;) {
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        &lt;span style=&#34;color:#75715e&#34;&gt;// Set the arguments
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#75715e&#34;&gt;&lt;/span&gt;        dataroot &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; dataroot_;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        workers &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; workers_;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        batch_size &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; batch_size_;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        image_size &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; image_size_;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        nc &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; nc_;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        nz &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; nz_;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        ngf &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; ngf_;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        ndf &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; ndf_;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        num_epochs &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; num_epochs_;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        lr &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; lr_;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        beta1 &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; beta1_;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        ngpu &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; ngpu_;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        main &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; torch&lt;span style=&#34;color:#f92672&#34;&gt;::&lt;/span&gt;nn&lt;span style=&#34;color:#f92672&#34;&gt;::&lt;/span&gt;Sequential(
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;           torch&lt;span style=&#34;color:#f92672&#34;&gt;::&lt;/span&gt;nn&lt;span style=&#34;color:#f92672&#34;&gt;::&lt;/span&gt;Conv2d(torch&lt;span style=&#34;color:#f92672&#34;&gt;::&lt;/span&gt;nn&lt;span style=&#34;color:#f92672&#34;&gt;::&lt;/span&gt;Conv2dOptions(nz, ngf&lt;span style=&#34;color:#f92672&#34;&gt;*&lt;/span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;8&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;4&lt;/span&gt;).stride(&lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;).padding(&lt;span style=&#34;color:#ae81ff&#34;&gt;0&lt;/span&gt;).with_bias(false).transposed(true)),
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;             torch&lt;span style=&#34;color:#f92672&#34;&gt;::&lt;/span&gt;nn&lt;span style=&#34;color:#f92672&#34;&gt;::&lt;/span&gt;BatchNorm(ngf&lt;span style=&#34;color:#f92672&#34;&gt;*&lt;/span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;8&lt;/span&gt;),
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;             torch&lt;span style=&#34;color:#f92672&#34;&gt;::&lt;/span&gt;nn&lt;span style=&#34;color:#f92672&#34;&gt;::&lt;/span&gt;Functional(torch&lt;span style=&#34;color:#f92672&#34;&gt;::&lt;/span&gt;relu),
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;             torch&lt;span style=&#34;color:#f92672&#34;&gt;::&lt;/span&gt;nn&lt;span style=&#34;color:#f92672&#34;&gt;::&lt;/span&gt;Conv2d(torch&lt;span style=&#34;color:#f92672&#34;&gt;::&lt;/span&gt;nn&lt;span style=&#34;color:#f92672&#34;&gt;::&lt;/span&gt;Conv2dOptions(ngf&lt;span style=&#34;color:#f92672&#34;&gt;*&lt;/span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;8&lt;/span&gt;, ngf&lt;span style=&#34;color:#f92672&#34;&gt;*&lt;/span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;4&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;4&lt;/span&gt;).stride(&lt;span style=&#34;color:#ae81ff&#34;&gt;2&lt;/span&gt;).padding(&lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;).with_bias(false).transposed(true)),
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;             torch&lt;span style=&#34;color:#f92672&#34;&gt;::&lt;/span&gt;nn&lt;span style=&#34;color:#f92672&#34;&gt;::&lt;/span&gt;BatchNorm(ngf&lt;span style=&#34;color:#f92672&#34;&gt;*&lt;/span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;4&lt;/span&gt;),
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;             torch&lt;span style=&#34;color:#f92672&#34;&gt;::&lt;/span&gt;nn&lt;span style=&#34;color:#f92672&#34;&gt;::&lt;/span&gt;Functional(torch&lt;span style=&#34;color:#f92672&#34;&gt;::&lt;/span&gt;relu),
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;             torch&lt;span style=&#34;color:#f92672&#34;&gt;::&lt;/span&gt;nn&lt;span style=&#34;color:#f92672&#34;&gt;::&lt;/span&gt;Conv2d(torch&lt;span style=&#34;color:#f92672&#34;&gt;::&lt;/span&gt;nn&lt;span style=&#34;color:#f92672&#34;&gt;::&lt;/span&gt;Conv2dOptions(ngf&lt;span style=&#34;color:#f92672&#34;&gt;*&lt;/span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;4&lt;/span&gt;, ngf&lt;span style=&#34;color:#f92672&#34;&gt;*&lt;/span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;2&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;4&lt;/span&gt;).stride(&lt;span style=&#34;color:#ae81ff&#34;&gt;2&lt;/span&gt;).padding(&lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;).with_bias(false).transposed(true)),
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;             torch&lt;span style=&#34;color:#f92672&#34;&gt;::&lt;/span&gt;nn&lt;span style=&#34;color:#f92672&#34;&gt;::&lt;/span&gt;BatchNorm(ngf&lt;span style=&#34;color:#f92672&#34;&gt;*&lt;/span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;2&lt;/span&gt;),
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;             torch&lt;span style=&#34;color:#f92672&#34;&gt;::&lt;/span&gt;nn&lt;span style=&#34;color:#f92672&#34;&gt;::&lt;/span&gt;Functional(torch&lt;span style=&#34;color:#f92672&#34;&gt;::&lt;/span&gt;relu),
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;             torch&lt;span style=&#34;color:#f92672&#34;&gt;::&lt;/span&gt;nn&lt;span style=&#34;color:#f92672&#34;&gt;::&lt;/span&gt;Conv2d(torch&lt;span style=&#34;color:#f92672&#34;&gt;::&lt;/span&gt;nn&lt;span style=&#34;color:#f92672&#34;&gt;::&lt;/span&gt;Conv2dOptions(ngf&lt;span style=&#34;color:#f92672&#34;&gt;*&lt;/span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;2&lt;/span&gt;, ngf, &lt;span style=&#34;color:#ae81ff&#34;&gt;4&lt;/span&gt;).stride(&lt;span style=&#34;color:#ae81ff&#34;&gt;2&lt;/span&gt;).padding(&lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;).with_bias(false).transposed(true)),
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;             torch&lt;span style=&#34;color:#f92672&#34;&gt;::&lt;/span&gt;nn&lt;span style=&#34;color:#f92672&#34;&gt;::&lt;/span&gt;BatchNorm(ngf),
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;             torch&lt;span style=&#34;color:#f92672&#34;&gt;::&lt;/span&gt;nn&lt;span style=&#34;color:#f92672&#34;&gt;::&lt;/span&gt;Functional(torch&lt;span style=&#34;color:#f92672&#34;&gt;::&lt;/span&gt;relu),
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;             torch&lt;span style=&#34;color:#f92672&#34;&gt;::&lt;/span&gt;nn&lt;span style=&#34;color:#f92672&#34;&gt;::&lt;/span&gt;Conv2d(torch&lt;span style=&#34;color:#f92672&#34;&gt;::&lt;/span&gt;nn&lt;span style=&#34;color:#f92672&#34;&gt;::&lt;/span&gt;Conv2dOptions(ngf, nc, &lt;span style=&#34;color:#ae81ff&#34;&gt;4&lt;/span&gt;).stride(&lt;span style=&#34;color:#ae81ff&#34;&gt;2&lt;/span&gt;).padding(&lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;).with_bias(false).transposed(true)),
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;             torch&lt;span style=&#34;color:#f92672&#34;&gt;::&lt;/span&gt;nn&lt;span style=&#34;color:#f92672&#34;&gt;::&lt;/span&gt;Functional(torch&lt;span style=&#34;color:#f92672&#34;&gt;::&lt;/span&gt;tanh)
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        );
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    }
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    torch&lt;span style=&#34;color:#f92672&#34;&gt;::&lt;/span&gt;nn&lt;span style=&#34;color:#f92672&#34;&gt;::&lt;/span&gt;Sequential main_func() {
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    	&lt;span style=&#34;color:#75715e&#34;&gt;// Returns Sequential Model of the Generator
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#75715e&#34;&gt;&lt;/span&gt;        &lt;span style=&#34;color:#66d9ef&#34;&gt;return&lt;/span&gt; main;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    }
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;};
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;Note how we used Transposed Convolution, by passing &lt;code&gt;.transposed(true)&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;Similarly, we define the class for Discriminator.&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-cpp&#34; data-lang=&#34;cpp&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#66d9ef&#34;&gt;class&lt;/span&gt; &lt;span style=&#34;color:#a6e22e&#34;&gt;Discriminator&lt;/span&gt; &lt;span style=&#34;color:#f92672&#34;&gt;:&lt;/span&gt; &lt;span style=&#34;color:#66d9ef&#34;&gt;public&lt;/span&gt; torch&lt;span style=&#34;color:#f92672&#34;&gt;::&lt;/span&gt;nn&lt;span style=&#34;color:#f92672&#34;&gt;::&lt;/span&gt;Module {
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#66d9ef&#34;&gt;private&lt;/span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;:&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    std&lt;span style=&#34;color:#f92672&#34;&gt;::&lt;/span&gt;string dataroot;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#66d9ef&#34;&gt;int&lt;/span&gt; workers;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#66d9ef&#34;&gt;int&lt;/span&gt; batch_size;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#66d9ef&#34;&gt;int&lt;/span&gt; image_size;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#66d9ef&#34;&gt;int&lt;/span&gt; nc;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#66d9ef&#34;&gt;int&lt;/span&gt; nz;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#66d9ef&#34;&gt;int&lt;/span&gt; ndf;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#66d9ef&#34;&gt;int&lt;/span&gt; num_epochs;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#66d9ef&#34;&gt;float&lt;/span&gt; lr;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#66d9ef&#34;&gt;float&lt;/span&gt; beta1;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#66d9ef&#34;&gt;int&lt;/span&gt; ngpu;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#66d9ef&#34;&gt;public&lt;/span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;:&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    torch&lt;span style=&#34;color:#f92672&#34;&gt;::&lt;/span&gt;nn&lt;span style=&#34;color:#f92672&#34;&gt;::&lt;/span&gt;Sequential main;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    Discriminator(std&lt;span style=&#34;color:#f92672&#34;&gt;::&lt;/span&gt;string dataroot_ &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;data/celeba&amp;#34;&lt;/span&gt;, &lt;span style=&#34;color:#66d9ef&#34;&gt;int&lt;/span&gt; workers_ &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; &lt;span style=&#34;color:#ae81ff&#34;&gt;2&lt;/span&gt;, &lt;span style=&#34;color:#66d9ef&#34;&gt;int&lt;/span&gt; batch_size_ &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; &lt;span style=&#34;color:#ae81ff&#34;&gt;128&lt;/span&gt;, &lt;span style=&#34;color:#66d9ef&#34;&gt;int&lt;/span&gt; image_size_ &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; &lt;span style=&#34;color:#ae81ff&#34;&gt;64&lt;/span&gt;, &lt;span style=&#34;color:#66d9ef&#34;&gt;int&lt;/span&gt; nc_ &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; &lt;span style=&#34;color:#ae81ff&#34;&gt;3&lt;/span&gt;, &lt;span style=&#34;color:#66d9ef&#34;&gt;int&lt;/span&gt; nz_ &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; &lt;span style=&#34;color:#ae81ff&#34;&gt;100&lt;/span&gt;, &lt;span style=&#34;color:#66d9ef&#34;&gt;int&lt;/span&gt; ngf_ &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; &lt;span style=&#34;color:#ae81ff&#34;&gt;64&lt;/span&gt;, &lt;span style=&#34;color:#66d9ef&#34;&gt;int&lt;/span&gt; ndf_ &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; &lt;span style=&#34;color:#ae81ff&#34;&gt;64&lt;/span&gt;, &lt;span style=&#34;color:#66d9ef&#34;&gt;int&lt;/span&gt; num_epochs_ &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; &lt;span style=&#34;color:#ae81ff&#34;&gt;5&lt;/span&gt;, &lt;span style=&#34;color:#66d9ef&#34;&gt;float&lt;/span&gt; lr_ &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; &lt;span style=&#34;color:#ae81ff&#34;&gt;0.0002&lt;/span&gt;, &lt;span style=&#34;color:#66d9ef&#34;&gt;float&lt;/span&gt; beta1_ &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; &lt;span style=&#34;color:#ae81ff&#34;&gt;0.5&lt;/span&gt;, &lt;span style=&#34;color:#66d9ef&#34;&gt;int&lt;/span&gt; ngpu_ &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; &lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;) {
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        dataroot &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; dataroot_;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        workers &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; workers_;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        batch_size &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; batch_size_;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        image_size &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; image_size_;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        nc &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; nc_;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        nz &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; nz_;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        ngf &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; ngf_;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        ndf &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; ndf_;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        num_epochs &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; num_epochs_;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        lr &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; lr_;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        beta1 &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; beta1_;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        ngpu &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; ngpu_;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        main &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; torch&lt;span style=&#34;color:#f92672&#34;&gt;::&lt;/span&gt;nn&lt;span style=&#34;color:#f92672&#34;&gt;::&lt;/span&gt;Sequential(
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;           torch&lt;span style=&#34;color:#f92672&#34;&gt;::&lt;/span&gt;nn&lt;span style=&#34;color:#f92672&#34;&gt;::&lt;/span&gt;Conv2d(torch&lt;span style=&#34;color:#f92672&#34;&gt;::&lt;/span&gt;nn&lt;span style=&#34;color:#f92672&#34;&gt;::&lt;/span&gt;Conv2dOptions(nc, ndf, &lt;span style=&#34;color:#ae81ff&#34;&gt;4&lt;/span&gt;).stride(&lt;span style=&#34;color:#ae81ff&#34;&gt;2&lt;/span&gt;).padding(&lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;).with_bias(false)),
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;           torch&lt;span style=&#34;color:#f92672&#34;&gt;::&lt;/span&gt;nn&lt;span style=&#34;color:#f92672&#34;&gt;::&lt;/span&gt;Functional(torch&lt;span style=&#34;color:#f92672&#34;&gt;::&lt;/span&gt;leaky_relu, &lt;span style=&#34;color:#ae81ff&#34;&gt;0.2&lt;/span&gt;),
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;           torch&lt;span style=&#34;color:#f92672&#34;&gt;::&lt;/span&gt;nn&lt;span style=&#34;color:#f92672&#34;&gt;::&lt;/span&gt;Conv2d(torch&lt;span style=&#34;color:#f92672&#34;&gt;::&lt;/span&gt;nn&lt;span style=&#34;color:#f92672&#34;&gt;::&lt;/span&gt;Conv2dOptions(ndf, ndf&lt;span style=&#34;color:#f92672&#34;&gt;*&lt;/span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;2&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;4&lt;/span&gt;).stride(&lt;span style=&#34;color:#ae81ff&#34;&gt;2&lt;/span&gt;).padding(&lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;).with_bias(false)),
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;           torch&lt;span style=&#34;color:#f92672&#34;&gt;::&lt;/span&gt;nn&lt;span style=&#34;color:#f92672&#34;&gt;::&lt;/span&gt;BatchNorm(ndf&lt;span style=&#34;color:#f92672&#34;&gt;*&lt;/span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;2&lt;/span&gt;),
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;           torch&lt;span style=&#34;color:#f92672&#34;&gt;::&lt;/span&gt;nn&lt;span style=&#34;color:#f92672&#34;&gt;::&lt;/span&gt;Functional(torch&lt;span style=&#34;color:#f92672&#34;&gt;::&lt;/span&gt;leaky_relu, &lt;span style=&#34;color:#ae81ff&#34;&gt;0.2&lt;/span&gt;),
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;           torch&lt;span style=&#34;color:#f92672&#34;&gt;::&lt;/span&gt;nn&lt;span style=&#34;color:#f92672&#34;&gt;::&lt;/span&gt;Conv2d(torch&lt;span style=&#34;color:#f92672&#34;&gt;::&lt;/span&gt;nn&lt;span style=&#34;color:#f92672&#34;&gt;::&lt;/span&gt;Conv2dOptions(ndf&lt;span style=&#34;color:#f92672&#34;&gt;*&lt;/span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;2&lt;/span&gt;, ndf&lt;span style=&#34;color:#f92672&#34;&gt;*&lt;/span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;4&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;4&lt;/span&gt;).stride(&lt;span style=&#34;color:#ae81ff&#34;&gt;2&lt;/span&gt;).padding(&lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;).with_bias(false)),
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;           torch&lt;span style=&#34;color:#f92672&#34;&gt;::&lt;/span&gt;nn&lt;span style=&#34;color:#f92672&#34;&gt;::&lt;/span&gt;BatchNorm(ndf&lt;span style=&#34;color:#f92672&#34;&gt;*&lt;/span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;4&lt;/span&gt;),
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;           torch&lt;span style=&#34;color:#f92672&#34;&gt;::&lt;/span&gt;nn&lt;span style=&#34;color:#f92672&#34;&gt;::&lt;/span&gt;Functional(torch&lt;span style=&#34;color:#f92672&#34;&gt;::&lt;/span&gt;leaky_relu, &lt;span style=&#34;color:#ae81ff&#34;&gt;0.2&lt;/span&gt;),
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;           torch&lt;span style=&#34;color:#f92672&#34;&gt;::&lt;/span&gt;nn&lt;span style=&#34;color:#f92672&#34;&gt;::&lt;/span&gt;Conv2d(torch&lt;span style=&#34;color:#f92672&#34;&gt;::&lt;/span&gt;nn&lt;span style=&#34;color:#f92672&#34;&gt;::&lt;/span&gt;Conv2dOptions(ndf&lt;span style=&#34;color:#f92672&#34;&gt;*&lt;/span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;4&lt;/span&gt;, ndf&lt;span style=&#34;color:#f92672&#34;&gt;*&lt;/span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;8&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;4&lt;/span&gt;).stride(&lt;span style=&#34;color:#ae81ff&#34;&gt;2&lt;/span&gt;).padding(&lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;).with_bias(false)),
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;           torch&lt;span style=&#34;color:#f92672&#34;&gt;::&lt;/span&gt;nn&lt;span style=&#34;color:#f92672&#34;&gt;::&lt;/span&gt;BatchNorm(ndf&lt;span style=&#34;color:#f92672&#34;&gt;*&lt;/span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;8&lt;/span&gt;),
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;           torch&lt;span style=&#34;color:#f92672&#34;&gt;::&lt;/span&gt;nn&lt;span style=&#34;color:#f92672&#34;&gt;::&lt;/span&gt;Functional(torch&lt;span style=&#34;color:#f92672&#34;&gt;::&lt;/span&gt;leaky_relu, &lt;span style=&#34;color:#ae81ff&#34;&gt;0.2&lt;/span&gt;),
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;           torch&lt;span style=&#34;color:#f92672&#34;&gt;::&lt;/span&gt;nn&lt;span style=&#34;color:#f92672&#34;&gt;::&lt;/span&gt;Conv2d(torch&lt;span style=&#34;color:#f92672&#34;&gt;::&lt;/span&gt;nn&lt;span style=&#34;color:#f92672&#34;&gt;::&lt;/span&gt;Conv2dOptions(ndf&lt;span style=&#34;color:#f92672&#34;&gt;*&lt;/span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;8&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;4&lt;/span&gt;).stride(&lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;).padding(&lt;span style=&#34;color:#ae81ff&#34;&gt;0&lt;/span&gt;).with_bias(false)),
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;           torch&lt;span style=&#34;color:#f92672&#34;&gt;::&lt;/span&gt;nn&lt;span style=&#34;color:#f92672&#34;&gt;::&lt;/span&gt;Functional(torch&lt;span style=&#34;color:#f92672&#34;&gt;::&lt;/span&gt;sigmoid)
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        );
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    }
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    torch&lt;span style=&#34;color:#f92672&#34;&gt;::&lt;/span&gt;nn&lt;span style=&#34;color:#f92672&#34;&gt;::&lt;/span&gt;Sequential main_func() {
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        &lt;span style=&#34;color:#66d9ef&#34;&gt;return&lt;/span&gt; main;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    }
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;};
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;We can initialize these networks as shown below:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-cpp&#34; data-lang=&#34;cpp&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#75715e&#34;&gt;// Uses default arguments if no args passed
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#75715e&#34;&gt;&lt;/span&gt;Generator gen &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; Generator()
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;Discriminator dis &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; Discriminator()
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;torch&lt;span style=&#34;color:#f92672&#34;&gt;::&lt;/span&gt;nn&lt;span style=&#34;color:#f92672&#34;&gt;::&lt;/span&gt;Sequential gen_model &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; gen.main_func()
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;torch&lt;span style=&#34;color:#f92672&#34;&gt;::&lt;/span&gt;nn&lt;span style=&#34;color:#f92672&#34;&gt;::&lt;/span&gt;Sequential dis_model &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; dis.main_func()
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;In case you are using a GPU, you can convert the models:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-cpp&#34; data-lang=&#34;cpp&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;torch&lt;span style=&#34;color:#f92672&#34;&gt;::&lt;/span&gt;Device device &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; torch&lt;span style=&#34;color:#f92672&#34;&gt;::&lt;/span&gt;kCPU;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#66d9ef&#34;&gt;if&lt;/span&gt;(torch&lt;span style=&#34;color:#f92672&#34;&gt;::&lt;/span&gt;cuda&lt;span style=&#34;color:#f92672&#34;&gt;::&lt;/span&gt;is_available()) {
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    device &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; torch&lt;span style=&#34;color:#f92672&#34;&gt;::&lt;/span&gt;kCUDA;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;}
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;gen_model&lt;span style=&#34;color:#f92672&#34;&gt;-&amp;gt;&lt;/span&gt;to(device);
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;dis_model&lt;span style=&#34;color:#f92672&#34;&gt;-&amp;gt;&lt;/span&gt;to(device);
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;&lt;strong&gt;Note on Data Loading&lt;/strong&gt;: In the past blogs, I&amp;rsquo;ve discussed on loading custom data. Please refer to the previous blogs for a quick review on loading data.&lt;/p&gt;
&lt;p&gt;Let&amp;rsquo;s go ahead and define optimizers and train our model. We use the parameters defined by the authors, for optimizer (Adam, &lt;code&gt;beta&lt;/code&gt; = 0.5) and learning rate of &lt;code&gt;2e-4&lt;/code&gt;.&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-cpp&#34; data-lang=&#34;cpp&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;torch&lt;span style=&#34;color:#f92672&#34;&gt;::&lt;/span&gt;optim&lt;span style=&#34;color:#f92672&#34;&gt;::&lt;/span&gt;Adam gen_optimizer(gen_model&lt;span style=&#34;color:#f92672&#34;&gt;-&amp;gt;&lt;/span&gt;parameters(), torch&lt;span style=&#34;color:#f92672&#34;&gt;::&lt;/span&gt;optim&lt;span style=&#34;color:#f92672&#34;&gt;::&lt;/span&gt;AdamOptions(&lt;span style=&#34;color:#ae81ff&#34;&gt;2e-4&lt;/span&gt;).beta1(&lt;span style=&#34;color:#ae81ff&#34;&gt;0.5&lt;/span&gt;));
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;torch&lt;span style=&#34;color:#f92672&#34;&gt;::&lt;/span&gt;optim&lt;span style=&#34;color:#f92672&#34;&gt;::&lt;/span&gt;Adam dis_optimizer(dis_model&lt;span style=&#34;color:#f92672&#34;&gt;-&amp;gt;&lt;/span&gt;parameters(), torch&lt;span style=&#34;color:#f92672&#34;&gt;::&lt;/span&gt;optim&lt;span style=&#34;color:#f92672&#34;&gt;::&lt;/span&gt;AdamOptions(&lt;span style=&#34;color:#ae81ff&#34;&gt;2e-4&lt;/span&gt;).beta1(&lt;span style=&#34;color:#ae81ff&#34;&gt;0.5&lt;/span&gt;));
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;Time to write our training code. We are using &lt;code&gt;CelebA&lt;/code&gt; dataset which looks like this:&lt;/p&gt;
&lt;!-- raw HTML omitted --&gt;
&lt;p&gt;The dataset is huge, and contains 10,177 number of identities and around ~200k number of face images. It also contains annotations, but since GANs are a way of unsupervised learning, so they don&amp;rsquo;t actually consider annotations. Before we move on, we&amp;rsquo;ll see a quick step by step review on training the Discriminator and Generator simultaneously:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;&lt;strong&gt;Step-1: Train Discriminator&lt;/strong&gt;. Remember from above, the discriminator tries to maximize the loss function such that it predicts the fake images as fake and real images as real.
&lt;ol&gt;
&lt;li&gt;As the first step for every training process, we set the gradients to zero. This helps in calculating correct gradients, and not getting confused with gradients stored in the previous iteration.&lt;/li&gt;
&lt;li&gt;First calculate &lt;strong&gt;discriminator loss on real images&lt;/strong&gt; (that is, data from our dataset). We do this by getting data from the batch and labels as anything between 0.8 and 1.0 (since it&amp;rsquo;s real, we approximate it from 0.8 to 1.0).&lt;/li&gt;
&lt;li&gt;Do a forward pass to the discriminator network, and calculate output on the batch of data from our dataset.&lt;/li&gt;
&lt;li&gt;Calculate loss by using &lt;code&gt;torch::binary_cross_entropy&lt;/code&gt; and backpropagate the loss.&lt;/li&gt;
&lt;li&gt;We now calculate &lt;strong&gt;discriminator loss on fake images&lt;/strong&gt; (that is, data from the generator). For this, we take a noise of shape similar to the batch of data, and pass that noise to the generator.&lt;/li&gt;
&lt;li&gt;The labels are given zero values (as the images are fake).&lt;/li&gt;
&lt;li&gt;Again, calculate the loss by using &lt;code&gt;torch::binary_cross_entropy&lt;/code&gt; and backpropagate the loss.&lt;/li&gt;
&lt;li&gt;Sum both the losses, &lt;strong&gt;discriminator loss on real images&lt;/strong&gt; + &lt;strong&gt;discriminator loss on fake images&lt;/strong&gt;. This will be our discriminator loss.&lt;/li&gt;
&lt;li&gt;We then update our parameters using the optimizer.&lt;/li&gt;
&lt;/ol&gt;
&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Step-2: Train Generator&lt;/strong&gt;. The task of a Generator is to minimize the loss function. Since it has to produce images which can fool the discriminator, so it only has to consider fake images.
&lt;ol&gt;
&lt;li&gt;As the first step for every training process, we set the gradients to zero. This helps in calculating correct gradients, and not getting confused with gradients stored in the previous iteration.&lt;/li&gt;
&lt;li&gt;We use the fake images produced in the Step-1 and pass it to the discriminator.&lt;/li&gt;
&lt;li&gt;Fill the labels with 1. (since generator wants to fool the discriminator, by making it predict as real images).&lt;/li&gt;
&lt;li&gt;Calculate loss, by using &lt;code&gt;torch::binary_cross_entropy&lt;/code&gt; and backpropagate the loss.&lt;/li&gt;
&lt;li&gt;Update the parameters using optimizer of the Generator.&lt;/li&gt;
&lt;/ol&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-cpp&#34; data-lang=&#34;cpp&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#66d9ef&#34;&gt;for&lt;/span&gt;(&lt;span style=&#34;color:#66d9ef&#34;&gt;int&lt;/span&gt; epoch&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;; epoch&lt;span style=&#34;color:#f92672&#34;&gt;&amp;lt;=&lt;/span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;10&lt;/span&gt;; epoch&lt;span style=&#34;color:#f92672&#34;&gt;++&lt;/span&gt;) {
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#75715e&#34;&gt;// Store batch count in a variable
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#75715e&#34;&gt;&lt;/span&gt;    &lt;span style=&#34;color:#66d9ef&#34;&gt;int&lt;/span&gt; batch_count &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; &lt;span style=&#34;color:#ae81ff&#34;&gt;0&lt;/span&gt;;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#75715e&#34;&gt;// You can use torch::data::Example&amp;lt;&amp;gt;&amp;amp; batch: *data_loader
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#75715e&#34;&gt;&lt;/span&gt;    &lt;span style=&#34;color:#66d9ef&#34;&gt;for&lt;/span&gt;(&lt;span style=&#34;color:#66d9ef&#34;&gt;auto&lt;/span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;&amp;amp;&lt;/span&gt; batch: &lt;span style=&#34;color:#f92672&#34;&gt;*&lt;/span&gt;data_loader) {
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        &lt;span style=&#34;color:#75715e&#34;&gt;// Step-1: Train the Discriminator
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#75715e&#34;&gt;&lt;/span&gt;        &lt;span style=&#34;color:#75715e&#34;&gt;// Set gradients to zero
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#75715e&#34;&gt;&lt;/span&gt;        netD&lt;span style=&#34;color:#f92672&#34;&gt;-&amp;gt;&lt;/span&gt;zero_grad();
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        &lt;span style=&#34;color:#75715e&#34;&gt;// Calculating discriminator loss on real images
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#75715e&#34;&gt;&lt;/span&gt;        torch&lt;span style=&#34;color:#f92672&#34;&gt;::&lt;/span&gt;Tensor images_real &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; batch.data.to(device);
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        torch&lt;span style=&#34;color:#f92672&#34;&gt;::&lt;/span&gt;Tensor labels_real &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; torch&lt;span style=&#34;color:#f92672&#34;&gt;::&lt;/span&gt;empty(batch.data.size(&lt;span style=&#34;color:#ae81ff&#34;&gt;0&lt;/span&gt;), device).uniform_(&lt;span style=&#34;color:#ae81ff&#34;&gt;0.8&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;1.0&lt;/span&gt;));
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        &lt;span style=&#34;color:#75715e&#34;&gt;// Do a forward pass to the Discriminator network
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#75715e&#34;&gt;&lt;/span&gt;        torch&lt;span style=&#34;color:#f92672&#34;&gt;::&lt;/span&gt;Tensor output_D_real &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; netD&lt;span style=&#34;color:#f92672&#34;&gt;-&amp;gt;&lt;/span&gt;forward(images_real);
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        &lt;span style=&#34;color:#75715e&#34;&gt;// Calculate the loss
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#75715e&#34;&gt;&lt;/span&gt;        torch&lt;span style=&#34;color:#f92672&#34;&gt;::&lt;/span&gt;Tensor loss_real_D &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; torch&lt;span style=&#34;color:#f92672&#34;&gt;::&lt;/span&gt;binary_cross_entropy(output_D_real, labels_real);
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        loss_real_D.backward();
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        &lt;span style=&#34;color:#75715e&#34;&gt;// Calculate discriminator loss on fake images
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#75715e&#34;&gt;&lt;/span&gt;        &lt;span style=&#34;color:#75715e&#34;&gt;// Generate noise and do forward pass to generate fake images
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#75715e&#34;&gt;&lt;/span&gt;        torch:Tensor fake_random &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; torch&lt;span style=&#34;color:#f92672&#34;&gt;::&lt;/span&gt;randn({batch.data.size(&lt;span style=&#34;color:#ae81ff&#34;&gt;0&lt;/span&gt;), args.nz, &lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;}, device);
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        torch&lt;span style=&#34;color:#f92672&#34;&gt;::&lt;/span&gt;Tensor images_fake &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; netG&lt;span style=&#34;color:#f92672&#34;&gt;-&amp;gt;&lt;/span&gt;forward(images_fake);
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        torch&lt;span style=&#34;color:#f92672&#34;&gt;::&lt;/span&gt;Tensor labels_fake &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; torch&lt;span style=&#34;color:#f92672&#34;&gt;::&lt;/span&gt;zeros(batch.data.size(&lt;span style=&#34;color:#ae81ff&#34;&gt;0&lt;/span&gt;), device);
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        &lt;span style=&#34;color:#75715e&#34;&gt;// Do a forward pass to the Discriminator network
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#75715e&#34;&gt;&lt;/span&gt;        torch&lt;span style=&#34;color:#f92672&#34;&gt;::&lt;/span&gt;Tensor output_D_fake &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; netD&lt;span style=&#34;color:#f92672&#34;&gt;-&amp;gt;&lt;/span&gt;forward(images_fake);
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        &lt;span style=&#34;color:#75715e&#34;&gt;// Calculate the loss
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#75715e&#34;&gt;&lt;/span&gt;        torch&lt;span style=&#34;color:#f92672&#34;&gt;::&lt;/span&gt;Tensor loss_fake_D &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; torch&lt;span style=&#34;color:#f92672&#34;&gt;::&lt;/span&gt;binary_cross_entropy(output_D_fake, labels_fake);
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        loss_fake_D.backward();
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        &lt;span style=&#34;color:#75715e&#34;&gt;// Total discriminator loss
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#75715e&#34;&gt;&lt;/span&gt;        torch&lt;span style=&#34;color:#f92672&#34;&gt;::&lt;/span&gt;Tensor loss_discriminator &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; loss_real_D &lt;span style=&#34;color:#f92672&#34;&gt;+&lt;/span&gt; loss_fake_D;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        &lt;span style=&#34;color:#75715e&#34;&gt;// Update the parameters
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#75715e&#34;&gt;&lt;/span&gt;        dis_optimizer.step();
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        &lt;span style=&#34;color:#75715e&#34;&gt;// Step-2: Train the Generator
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#75715e&#34;&gt;&lt;/span&gt;        &lt;span style=&#34;color:#75715e&#34;&gt;// Set gradients to zero
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#75715e&#34;&gt;&lt;/span&gt;        netG&lt;span style=&#34;color:#f92672&#34;&gt;-&amp;gt;&lt;/span&gt;zero_grad();
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        &lt;span style=&#34;color:#75715e&#34;&gt;// calculating generator loss on fake images
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#75715e&#34;&gt;&lt;/span&gt;        &lt;span style=&#34;color:#75715e&#34;&gt;// Change labels_fake from all zeros to all ones
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#75715e&#34;&gt;&lt;/span&gt;        labels_fake.fill_(&lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;);
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        &lt;span style=&#34;color:#75715e&#34;&gt;// Do forward pass to the Discriminator on the fake images generated above
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#75715e&#34;&gt;&lt;/span&gt;        torch&lt;span style=&#34;color:#f92672&#34;&gt;::&lt;/span&gt;Tensor output_G_fake &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; netD&lt;span style=&#34;color:#f92672&#34;&gt;-&amp;gt;&lt;/span&gt;forward(images_fake);
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        &lt;span style=&#34;color:#75715e&#34;&gt;// Calculate loss
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#75715e&#34;&gt;&lt;/span&gt;        torch&lt;span style=&#34;color:#f92672&#34;&gt;::&lt;/span&gt;Tensor loss_generator &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; torch&lt;span style=&#34;color:#f92672&#34;&gt;::&lt;/span&gt;binary_cross_entropy(output_G_fake, labels_fake);
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        loss_generator.backward();
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        &lt;span style=&#34;color:#75715e&#34;&gt;// Update the parameters
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#75715e&#34;&gt;&lt;/span&gt;        gen_optimizer.step();
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        std&lt;span style=&#34;color:#f92672&#34;&gt;::&lt;/span&gt;cout &lt;span style=&#34;color:#f92672&#34;&gt;&amp;lt;&amp;lt;&lt;/span&gt; &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;Epoch: &amp;#34;&lt;/span&gt; &lt;span style=&#34;color:#f92672&#34;&gt;&amp;lt;&amp;lt;&lt;/span&gt; epoch &lt;span style=&#34;color:#f92672&#34;&gt;&amp;lt;&amp;lt;&lt;/span&gt; &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;, Batch: &amp;#34;&lt;/span&gt; &lt;span style=&#34;color:#f92672&#34;&gt;&amp;lt;&amp;lt;&lt;/span&gt; batch_count &lt;span style=&#34;color:#f92672&#34;&gt;&amp;lt;&amp;lt;&lt;/span&gt; &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;, Gen Loss: &amp;#34;&lt;/span&gt; &lt;span style=&#34;color:#f92672&#34;&gt;&amp;lt;&amp;lt;&lt;/span&gt; loss_generator.item&lt;span style=&#34;color:#f92672&#34;&gt;&amp;lt;&lt;/span&gt;&lt;span style=&#34;color:#66d9ef&#34;&gt;float&lt;/span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;&amp;gt;&lt;/span&gt;() &lt;span style=&#34;color:#f92672&#34;&gt;&amp;lt;&amp;lt;&lt;/span&gt; &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;, Discriminator Loss: &amp;#34;&lt;/span&gt; &lt;span style=&#34;color:#f92672&#34;&gt;&amp;lt;&amp;lt;&lt;/span&gt; loss_discriminator.item&lt;span style=&#34;color:#f92672&#34;&gt;&amp;lt;&lt;/span&gt;&lt;span style=&#34;color:#66d9ef&#34;&gt;float&lt;/span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;&amp;gt;&lt;/span&gt;() &lt;span style=&#34;color:#f92672&#34;&gt;&amp;lt;&amp;lt;&lt;/span&gt; std&lt;span style=&#34;color:#f92672&#34;&gt;::&lt;/span&gt;endl;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;	
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        batch_count&lt;span style=&#34;color:#f92672&#34;&gt;++&lt;/span&gt;;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    }
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;}
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;We are all set to train our first DCGAN in C++ using Libtorch. How amazing it is?&lt;/p&gt;
&lt;p&gt;In the coming blog, I&amp;rsquo;ll share the results and answer a few common questions on the architecture of DCGAN.&lt;/p&gt;
&lt;h2 id=&#34;acknowledgement-and-references&#34;&gt;Acknowledgement and References&lt;/h2&gt;
&lt;p&gt;I would like to thank &lt;a href=&#34;https://github.com/yf225&#34;&gt;Will Feng&lt;/a&gt; and &lt;a href=&#34;https://discuss.pytorch.org/u/ptrblck/summary&#34;&gt;Piotr&lt;/a&gt; for their useful suggestions. The code used in this blog, is partially analogous to the official &lt;a href=&#34;https://github.com/pytorch/examples/tree/master/cpp/dcgan&#34;&gt;PyTorch examples repo on DCGAN using LibTorch&lt;/a&gt;. I&amp;rsquo;ve also referred the original paper by &lt;a href=&#34;https://twitter.com/soumithchintala&#34;&gt;Soumith Chintala&lt;/a&gt; and others. The sources of reference images (for Network architectures) have been acknowledged in the captions of respective images.&lt;/p&gt;

      </description>
    </item>
    
    <item>
      <title>Setting up Jupyter Notebook (Xeus Cling) for Libtorch and OpenCV Libraries</title>
      <link>https://krshrimali.github.io/posts/2019/08/setting-up-jupyter-notebook-xeus-cling-for-libtorch-and-opencv-libraries/</link>
      <pubDate>Wed, 28 Aug 2019 00:00:00 +0000</pubDate>
      
      <guid>https://krshrimali.github.io/posts/2019/08/setting-up-jupyter-notebook-xeus-cling-for-libtorch-and-opencv-libraries/</guid>
      <description>
          
            &lt;h4&gt;Setting up Jupyter Notebook (Xeus Cling) for Libtorch and OpenCV Libraries&lt;/h4&gt;
          
          &lt;h2 id=&#34;introduction-to-xeus-cling&#34;&gt;Introduction to Xeus Cling&lt;/h2&gt;
&lt;p&gt;Today, we are going to run our C++ codes in the Jupyter Notebook. Sounds ambitious? Not much. Let&amp;rsquo;s see how we do it using Xeus Cling.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/krshrimali/blog/main/assets/cover-images/Cover-Xeus-Cling.jpg&#34; alt=&#34;&#34;&gt;&lt;/p&gt;
&lt;p&gt;I&amp;rsquo;ll quote the definition of Xeus Cling on the official &lt;a href=&#34;https://xeus-cling.readthedocs.io/en/latest/#targetText=xeus%2Dcling%20is%20a%20Jupyter,of%20the%20Jupyter%20protocol%20xeus&#34;&gt;documentation website&lt;/a&gt;.&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;xeus-cling is a Jupyter kernel for C++ based on the C++ interpreter cling and the native implementation of the Jupyter protocol xeus.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;Just like we use Python Kernel in the Jupyter Notebook, we can also use a C++ based interpreter cling combined with a Jupyter protocol called Xeus to reach closer to implementing C++ code in the notebook.&lt;/p&gt;
&lt;h2 id=&#34;installing-xeus-cling-using-anaconda&#34;&gt;Installing Xeus Cling using Anaconda&lt;/h2&gt;
&lt;p&gt;It&amp;rsquo;s pretty straight forward to install Xeus Cling using Anaconda. I&amp;rsquo;m assuming the user has Anaconda installed.  Use this command to install &lt;code&gt;Xeus Cling&lt;/code&gt; using Anaconda: &lt;code&gt;conda install -c conda-forge xeus-cling&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Note&lt;/strong&gt;: Before using &lt;code&gt;conda&lt;/code&gt; commands, you need to have it in your &lt;code&gt;PATH&lt;/code&gt; variable. Use this command to add the path to &lt;code&gt;conda&lt;/code&gt; to your system &lt;code&gt;PATH&lt;/code&gt; variable: &lt;code&gt;export PATH=~/anaconda3/bin/:$PATH&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;The conventional way to install any such library which can create conflicts with existing libraries, is to create an environment and then install it in the environment.&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Create a &lt;code&gt;conda&lt;/code&gt; environment: &lt;code&gt;conda create -n cpp-xeus-cling&lt;/code&gt;.&lt;/li&gt;
&lt;li&gt;Activate the environment you just created: &lt;code&gt;source activate cpp-xeus-cling&lt;/code&gt;.&lt;/li&gt;
&lt;li&gt;Install &lt;code&gt;xeus-cling&lt;/code&gt; using &lt;code&gt;conda&lt;/code&gt;: &lt;code&gt;conda install -c conda-forge xeus-cling&lt;/code&gt;.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;Once setup, let&amp;rsquo;s go ahead and get started with Jupyter Notebook. When creating a new notebook, you will see different options for the kernel. One of them would be &lt;code&gt;C++XX&lt;/code&gt; where XX is the &lt;code&gt;C++&lt;/code&gt; version.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/krshrimali/blog/main/assets/blogs/kernels-available.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt;
&lt;p&gt;Click on any of the kernel for C++ and let&amp;rsquo;s start setting up environment for PyTorch C++ API.&lt;/p&gt;
&lt;p&gt;You can try and implement some of the basic commands in C++.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/krshrimali/blog/main/assets/blogs/Jupyter-Notebook-Sample.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt;
&lt;p&gt;This looks great, right? Let&amp;rsquo;s go ahead and set up the Deep Learning environment.&lt;/p&gt;
&lt;h2 id=&#34;setting-up-libtorch-in-xeus-cling&#34;&gt;Setting up Libtorch in Xeus Cling&lt;/h2&gt;
&lt;p&gt;Just like we need to give path to &lt;code&gt;Libtorch&lt;/code&gt; libraries in &lt;code&gt;CMakeLists.txt&lt;/code&gt; or while setting up &lt;code&gt;XCode&lt;/code&gt; (for OS X users) or &lt;code&gt;Visual Studio&lt;/code&gt; (for Windows Users), we will also load the libraries in Xeus Cling.&lt;/p&gt;
&lt;p&gt;We will first give the &lt;code&gt;include_path&lt;/code&gt; of Header files and &lt;code&gt;library_path&lt;/code&gt; for the libraries. We will also do the same for &lt;code&gt;OpenCV&lt;/code&gt; as we need it to load images.&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-cpp&#34; data-lang=&#34;cpp&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#75715e&#34;&gt;#pragma cling add_library_path(&amp;#34;/Users/krshrimali/Downloads/libtorch/lib/&amp;#34;)
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#75715e&#34;&gt;#pragma cling add_include_path(&amp;#34;/Users/krshrimali/Downloads/libtorch/include&lt;/span&gt;&lt;span style=&#34;color:#75715e&#34;&gt;&amp;#34;)&lt;/span&gt;&lt;span style=&#34;color:#75715e&#34;&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#75715e&#34;&gt;#pragma cling add_include_path(&amp;#34;/Users/krshrimali/Downloads/libtorch/include&lt;/span&gt;&lt;span style=&#34;color:#75715e&#34;&gt;/torch/csrc/api/include/&amp;#34;)&lt;/span&gt;&lt;span style=&#34;color:#75715e&#34;&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#75715e&#34;&gt;#pragma cling add_library_path(&amp;#34;/usr/local/Cellar/opencv/4.1.0_2/lib&amp;#34;)
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#75715e&#34;&gt;#pragma cling add_include_path(&amp;#34;/usr/local/Cellar/opencv/4.1.0_2/include&lt;/span&gt;&lt;span style=&#34;color:#75715e&#34;&gt;/opencv4&amp;#34;)&lt;/span&gt;&lt;span style=&#34;color:#75715e&#34;&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;For OS X, the libtorch libraries will be in the format of &lt;code&gt;.dylib&lt;/code&gt;. Ignore the &lt;code&gt;.a&lt;/code&gt; files as we only need to load the &lt;code&gt;.dylib&lt;/code&gt; files. Similarly for Linux, load the libraries in &lt;code&gt;.so&lt;/code&gt; format located in the &lt;code&gt;lib/&lt;/code&gt; folder.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;For Mac&lt;/strong&gt;&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-cpp&#34; data-lang=&#34;cpp&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#75715e&#34;&gt;#pragma cling load(&amp;#34;/Users/krshrimali/Downloads/libtorch/lib/libiomp5.dylib&amp;#34;)
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#75715e&#34;&gt;#pragma cling load(&amp;#34;/Users/krshrimali/Downloads/libtorch/lib/libmklml.dylib&amp;#34;)
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#75715e&#34;&gt;#pragma cling load(&amp;#34;/Users/krshrimali/Downloads/libtorch/lib/libc10.dylib&amp;#34;)
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#75715e&#34;&gt;#pragma cling load(&amp;#34;/Users/krshrimali/Downloads/libtorch/lib/libtorch.dylib&amp;#34;)
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#75715e&#34;&gt;#pragma cling load(&amp;#34;/Users/krshrimali/Downloads/libtorch/lib/libshm.dylib&amp;#34;)
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;&lt;strong&gt;For Linux&lt;/strong&gt;&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-cpp&#34; data-lang=&#34;cpp&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#75715e&#34;&gt;#pragma cling load(&amp;#34;/opt/libtorch/lib/libc10.so&amp;#34;)
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#75715e&#34;&gt;#pragma cling load(&amp;#34;/opt/libtorch/lib/libgomp-4f651535.so.1&amp;#34;)
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#75715e&#34;&gt;#pragma cling load(&amp;#34;/opt/libtorch/lib/libtorch.so&amp;#34;)
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;For OpenCV, the list of libraries is long.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;For Mac&lt;/strong&gt;&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-cpp&#34; data-lang=&#34;cpp&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#75715e&#34;&gt;#pragma cling load(&amp;#34;/usr/local/Cellar/opencv/4.1.0_2/lib/libopencv_datasets.4.1.0.dylib&amp;#34;)
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#75715e&#34;&gt;#pragma cling load(&amp;#34;/usr/local/Cellar/opencv/4.1.0_2/lib/libopencv_aruco.4.1.0.dylib&amp;#34;)
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#75715e&#34;&gt;#pragma cling load(&amp;#34;/usr/local/Cellar/opencv/4.1.0_2/lib/libopencv_bgsegm.4.1.0.dylib&amp;#34;)
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#75715e&#34;&gt;#pragma cling load(&amp;#34;/usr/local/Cellar/opencv/4.1.0_2/lib/libopencv_bioinspired.4.1.0.dylib&amp;#34;)
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#75715e&#34;&gt;#pragma cling load(&amp;#34;/usr/local/Cellar/opencv/4.1.0_2/lib/libopencv_calib3d.4.1.0.dylib&amp;#34;)
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#75715e&#34;&gt;#pragma cling load(&amp;#34;/usr/local/Cellar/opencv/4.1.0_2/lib/libopencv_ccalib.4.1.0.dylib&amp;#34;)
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#75715e&#34;&gt;#pragma cling load(&amp;#34;/usr/local/Cellar/opencv/4.1.0_2/lib/libopencv_core.4.1.0.dylib&amp;#34;)
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#75715e&#34;&gt;#pragma cling load(&amp;#34;/usr/local/Cellar/opencv/4.1.0_2/lib/libopencv_dnn_objdetect.4.1.0.dylib&amp;#34;)
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#75715e&#34;&gt;#pragma cling load(&amp;#34;/usr/local/Cellar/opencv/4.1.0_2/lib/libopencv_dnn.4.1.0.dylib&amp;#34;)
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#75715e&#34;&gt;#pragma cling load(&amp;#34;/usr/local/Cellar/opencv/4.1.0_2/lib/libopencv_dpm.4.1.0.dylib&amp;#34;)
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#75715e&#34;&gt;#pragma cling load(&amp;#34;/usr/local/Cellar/opencv/4.1.0_2/lib/libopencv_face.4.1.0.dylib&amp;#34;)
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#75715e&#34;&gt;#pragma cling load(&amp;#34;/usr/local/Cellar/opencv/4.1.0_2/lib/libopencv_features2d.4.1.0.dylib&amp;#34;)
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#75715e&#34;&gt;#pragma cling load(&amp;#34;/usr/local/Cellar/opencv/4.1.0_2/lib/libopencv_flann.4.1.0.dylib&amp;#34;)
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#75715e&#34;&gt;#pragma cling load(&amp;#34;/usr/local/Cellar/opencv/4.1.0_2/lib/libopencv_freetype.4.1.0.dylib&amp;#34;)
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#75715e&#34;&gt;#pragma cling load(&amp;#34;/usr/local/Cellar/opencv/4.1.0_2/lib/libopencv_fuzzy.4.1.0.dylib&amp;#34;)
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#75715e&#34;&gt;#pragma cling load(&amp;#34;/usr/local/Cellar/opencv/4.1.0_2/lib/libopencv_gapi.4.1.0.dylib&amp;#34;)
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#75715e&#34;&gt;#pragma cling load(&amp;#34;/usr/local/Cellar/opencv/4.1.0_2/lib/libopencv_hfs.4.1.0.dylib&amp;#34;)
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#75715e&#34;&gt;#pragma cling load(&amp;#34;/usr/local/Cellar/opencv/4.1.0_2/lib/libopencv_highgui.4.1.0.dylib&amp;#34;)
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#75715e&#34;&gt;#pragma cling load(&amp;#34;/usr/local/Cellar/opencv/4.1.0_2/lib/libopencv_img_hash.4.1.0.dylib&amp;#34;)
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#75715e&#34;&gt;#pragma cling load(&amp;#34;/usr/local/Cellar/opencv/4.1.0_2/lib/libopencv_imgcodecs.4.1.0.dylib&amp;#34;)
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#75715e&#34;&gt;#pragma cling load(&amp;#34;/usr/local/Cellar/opencv/4.1.0_2/lib/libopencv_imgproc.4.1.0.dylib&amp;#34;)
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#75715e&#34;&gt;#pragma cling load(&amp;#34;/usr/local/Cellar/opencv/4.1.0_2/lib/libopencv_line_descriptor.4.1.0.dylib&amp;#34;)
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#75715e&#34;&gt;#pragma cling load(&amp;#34;/usr/local/Cellar/opencv/4.1.0_2/lib/libopencv_ml.4.1.0.dylib&amp;#34;)
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#75715e&#34;&gt;#pragma cling load(&amp;#34;/usr/local/Cellar/opencv/4.1.0_2/lib/libopencv_objdetect.4.1.0.dylib&amp;#34;)
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#75715e&#34;&gt;#pragma cling load(&amp;#34;/usr/local/Cellar/opencv/4.1.0_2/lib/libopencv_optflow.4.1.0.dylib&amp;#34;)
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#75715e&#34;&gt;#pragma cling load(&amp;#34;/usr/local/Cellar/opencv/4.1.0_2/lib/libopencv_phase_unwrapping.4.1.0.dylib&amp;#34;)
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#75715e&#34;&gt;#pragma cling load(&amp;#34;/usr/local/Cellar/opencv/4.1.0_2/lib/libopencv_photo.4.1.0.dylib&amp;#34;)
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#75715e&#34;&gt;#pragma cling load(&amp;#34;/usr/local/Cellar/opencv/4.1.0_2/lib/libopencv_plot.4.1.0.dylib&amp;#34;)
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#75715e&#34;&gt;#pragma cling load(&amp;#34;/usr/local/Cellar/opencv/4.1.0_2/lib/libopencv_quality.4.1.0.dylib&amp;#34;)
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#75715e&#34;&gt;#pragma cling load(&amp;#34;/usr/local/Cellar/opencv/4.1.0_2/lib/libopencv_reg.4.1.0.dylib&amp;#34;)
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#75715e&#34;&gt;#pragma cling load(&amp;#34;/usr/local/Cellar/opencv/4.1.0_2/lib/libopencv_rgbd.4.1.0.dylib&amp;#34;)
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#75715e&#34;&gt;#pragma cling load(&amp;#34;/usr/local/Cellar/opencv/4.1.0_2/lib/libopencv_saliency.4.1.0.dylib&amp;#34;)
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#75715e&#34;&gt;#pragma cling load(&amp;#34;/usr/local/Cellar/opencv/4.1.0_2/lib/libopencv_sfm.4.1.0.dylib&amp;#34;)
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#75715e&#34;&gt;#pragma cling load(&amp;#34;/usr/local/Cellar/opencv/4.1.0_2/lib/libopencv_shape.4.1.0.dylib&amp;#34;)
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#75715e&#34;&gt;#pragma cling load(&amp;#34;/usr/local/Cellar/opencv/4.1.0_2/lib/libopencv_stereo.4.1.0.dylib&amp;#34;)
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#75715e&#34;&gt;#pragma cling load(&amp;#34;/usr/local/Cellar/opencv/4.1.0_2/lib/libopencv_stitching.4.1.0.dylib&amp;#34;)
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#75715e&#34;&gt;#pragma cling load(&amp;#34;/usr/local/Cellar/opencv/4.1.0_2/lib/libopencv_structured_light.4.1.0.dylib&amp;#34;)
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#75715e&#34;&gt;#pragma cling load(&amp;#34;/usr/local/Cellar/opencv/4.1.0_2/lib/libopencv_superres.4.1.0.dylib&amp;#34;)
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#75715e&#34;&gt;#pragma cling load(&amp;#34;/usr/local/Cellar/opencv/4.1.0_2/lib/libopencv_surface_matching.4.1.0.dylib&amp;#34;)
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#75715e&#34;&gt;#pragma cling load(&amp;#34;/usr/local/Cellar/opencv/4.1.0_2/lib/libopencv_text.4.1.0.dylib&amp;#34;)
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#75715e&#34;&gt;#pragma cling load(&amp;#34;/usr/local/Cellar/opencv/4.1.0_2/lib/libopencv_tracking.4.1.0.dylib&amp;#34;)
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#75715e&#34;&gt;#pragma cling load(&amp;#34;/usr/local/Cellar/opencv/4.1.0_2/lib/libopencv_video.4.1.0.dylib&amp;#34;)
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#75715e&#34;&gt;#pragma cling load(&amp;#34;/usr/local/Cellar/opencv/4.1.0_2/lib/libopencv_videoio.4.1.0.dylib&amp;#34;)
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#75715e&#34;&gt;#pragma cling load(&amp;#34;/usr/local/Cellar/opencv/4.1.0_2/lib/libopencv_videostab.4.1.0.dylib&amp;#34;)
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#75715e&#34;&gt;#pragma cling load(&amp;#34;/usr/local/Cellar/opencv/4.1.0_2/lib/libopencv_xfeatures2d.4.1.0.dylib&amp;#34;)
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#75715e&#34;&gt;#pragma cling load(&amp;#34;/usr/local/Cellar/opencv/4.1.0_2/lib/libopencv_ximgproc.4.1.0.dylib&amp;#34;)
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#75715e&#34;&gt;#pragma cling load(&amp;#34;/usr/local/Cellar/opencv/4.1.0_2/lib/libopencv_xobjdetect.4.1.0.dylib&amp;#34;)
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#75715e&#34;&gt;#pragma cling load(&amp;#34;/usr/local/Cellar/opencv/4.1.0_2/lib/libopencv_xphoto.4.1.0.dylib&amp;#34;)
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;&lt;strong&gt;For Linux&lt;/strong&gt;&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-cpp&#34; data-lang=&#34;cpp&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#75715e&#34;&gt;#pragma cling load(&amp;#34;/usr/local/lib/libopencv_aruco.so.4.1.0&amp;#34;)
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#75715e&#34;&gt;#pragma cling load(&amp;#34;/usr/local/lib/libopencv_bgsegm.so.4.1.0&amp;#34;)
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#75715e&#34;&gt;#pragma cling load(&amp;#34;/usr/local/lib/libopencv_bioinspired.so.4.1.0&amp;#34;)
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#75715e&#34;&gt;#pragma cling load(&amp;#34;/usr/local/lib/libopencv_calib3d.so.4.1.0&amp;#34;)
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#75715e&#34;&gt;#pragma cling load(&amp;#34;/usr/local/lib/libopencv_ccalib.so.4.1.0&amp;#34;)
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#75715e&#34;&gt;#pragma cling load(&amp;#34;/usr/local/lib/libopencv_core.so.4.1.0&amp;#34;)
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#75715e&#34;&gt;#pragma cling load(&amp;#34;/usr/local/lib/libopencv_datasets.so.4.1.0&amp;#34;)
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#75715e&#34;&gt;#pragma cling load(&amp;#34;/usr/local/lib/libopencv_dnn_objdetect.so.4.1.0&amp;#34;)
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#75715e&#34;&gt;#pragma cling load(&amp;#34;/usr/local/lib/libopencv_dnn.so.4.1.0&amp;#34;)
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#75715e&#34;&gt;#pragma cling load(&amp;#34;/usr/local/lib/libopencv_dpm.so.4.1.0&amp;#34;)
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#75715e&#34;&gt;#pragma cling load(&amp;#34;/usr/local/lib/libopencv_face.so.4.1.0&amp;#34;)
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#75715e&#34;&gt;#pragma cling load(&amp;#34;/usr/local/lib/libopencv_features2d.so.4.1.0&amp;#34;)
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#75715e&#34;&gt;#pragma cling load(&amp;#34;/usr/local/lib/libopencv_flann.so.4.1.0&amp;#34;)
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#75715e&#34;&gt;#pragma cling load(&amp;#34;/usr/local/lib/libopencv_freetype.so.4.1.0&amp;#34;)
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#75715e&#34;&gt;#pragma cling load(&amp;#34;/usr/local/lib/libopencv_fuzzy.so.4.1.0&amp;#34;)
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#75715e&#34;&gt;#pragma cling load(&amp;#34;/usr/local/lib/libopencv_gapi.so.4.1.0&amp;#34;)
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#75715e&#34;&gt;#pragma cling load(&amp;#34;/usr/local/lib/libopencv_hdf.so.4.1.0&amp;#34;)
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#75715e&#34;&gt;#pragma cling load(&amp;#34;/usr/local/lib/libopencv_hfs.so.4.1.0&amp;#34;)
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#75715e&#34;&gt;#pragma cling load(&amp;#34;/usr/local/lib/libopencv_highgui.so.4.1.0&amp;#34;)
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#75715e&#34;&gt;#pragma cling load(&amp;#34;/usr/local/lib/libopencv_imgcodecs.so.4.1.0&amp;#34;)
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#75715e&#34;&gt;#pragma cling load(&amp;#34;/usr/local/lib/libopencv_img_hash.so.4.1.0&amp;#34;)
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#75715e&#34;&gt;#pragma cling load(&amp;#34;/usr/local/lib/libopencv_imgproc.so.4.1.0&amp;#34;)
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#75715e&#34;&gt;#pragma cling load(&amp;#34;/usr/local/lib/libopencv_line_descriptor.so.4.1.0&amp;#34;)
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#75715e&#34;&gt;#pragma cling load(&amp;#34;/usr/local/lib/libopencv_ml.so.4.1.0&amp;#34;)
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#75715e&#34;&gt;#pragma cling load(&amp;#34;/usr/local/lib/libopencv_objdetect.so.4.1.0&amp;#34;)
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#75715e&#34;&gt;#pragma cling load(&amp;#34;/usr/local/lib/libopencv_optflow.so.4.1.0&amp;#34;)
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#75715e&#34;&gt;#pragma cling load(&amp;#34;/usr/local/lib/libopencv_phase_unwrapping.so.4.1.0&amp;#34;)
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#75715e&#34;&gt;#pragma cling load(&amp;#34;/usr/local/lib/libopencv_photo.so.4.1.0&amp;#34;)
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#75715e&#34;&gt;#pragma cling load(&amp;#34;/usr/local/lib/libopencv_plot.so.4.1.0&amp;#34;)
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#75715e&#34;&gt;#pragma cling load(&amp;#34;/usr/local/lib/libopencv_reg.so.4.1.0&amp;#34;)
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#75715e&#34;&gt;#pragma cling load(&amp;#34;/usr/local/lib/libopencv_rgbd.so.4.1.0&amp;#34;)
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#75715e&#34;&gt;#pragma cling load(&amp;#34;/usr/local/lib/libopencv_saliency.so.4.1.0&amp;#34;)
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#75715e&#34;&gt;#pragma cling load(&amp;#34;/usr/local/lib/libopencv_sfm.so.4.1.0&amp;#34;)
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#75715e&#34;&gt;#pragma cling load(&amp;#34;/usr/local/lib/libopencv_shape.so.4.1.0&amp;#34;)
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#75715e&#34;&gt;#pragma cling load(&amp;#34;/usr/local/lib/libopencv_stereo.so.4.1.0&amp;#34;)
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#75715e&#34;&gt;#pragma cling load(&amp;#34;/usr/local/lib/libopencv_stitching.so.4.1.0&amp;#34;)
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#75715e&#34;&gt;#pragma cling load(&amp;#34;/usr/local/lib/libopencv_structured_light.so.4.1.0&amp;#34;)
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#75715e&#34;&gt;#pragma cling load(&amp;#34;/usr/local/lib/libopencv_superres.so.4.1.0&amp;#34;)
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#75715e&#34;&gt;#pragma cling load(&amp;#34;/usr/local/lib/libopencv_surface_matching.so.4.1.0&amp;#34;)
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#75715e&#34;&gt;#pragma cling load(&amp;#34;/usr/local/lib/libopencv_text.so.4.1.0&amp;#34;)
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#75715e&#34;&gt;#pragma cling load(&amp;#34;/usr/local/lib/libopencv_tracking.so.4.1.0&amp;#34;)
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#75715e&#34;&gt;#pragma cling load(&amp;#34;/usr/local/lib/libopencv_videoio.so.4.1.0&amp;#34;)
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#75715e&#34;&gt;#pragma cling load(&amp;#34;/usr/local/lib/libopencv_video.so.4.1.0&amp;#34;)
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#75715e&#34;&gt;#pragma cling load(&amp;#34;/usr/local/lib/libopencv_videostab.so.4.1.0&amp;#34;)
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#75715e&#34;&gt;#pragma cling load(&amp;#34;/usr/local/lib/libopencv_xfeatures2d.so.4.1.0&amp;#34;)
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#75715e&#34;&gt;#pragma cling load(&amp;#34;/usr/local/lib/libopencv_ximgproc.so.4.1.0&amp;#34;)
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#75715e&#34;&gt;#pragma cling load(&amp;#34;/usr/local/lib/libopencv_xobjdetect.so.4.1.0&amp;#34;)
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#75715e&#34;&gt;#pragma cling load(&amp;#34;/usr/local/lib/libopencv_xphoto.so.4.1.0&amp;#34;)
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;Once done, run the cell and that&amp;rsquo;s it. We have successfully setup the environment for &lt;code&gt;Libtorch&lt;/code&gt; and &lt;code&gt;OpenCV&lt;/code&gt;.&lt;/p&gt;
&lt;h2 id=&#34;testing-xeus-cling-notebook&#34;&gt;Testing Xeus Cling Notebook&lt;/h2&gt;
&lt;p&gt;Let&amp;rsquo;s go ahead and include the libraries. I&amp;rsquo;ll be sharing the code snippets as well as the screenshots to make it easy for the readers to reproduce results.&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-cpp&#34; data-lang=&#34;cpp&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#75715e&#34;&gt;#include&lt;/span&gt; &lt;span style=&#34;color:#75715e&#34;&gt;&amp;lt;torch/torch.h&amp;gt;&lt;/span&gt;&lt;span style=&#34;color:#75715e&#34;&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#75715e&#34;&gt;#include&lt;/span&gt; &lt;span style=&#34;color:#75715e&#34;&gt;&amp;lt;torch/script.h&amp;gt;&lt;/span&gt;&lt;span style=&#34;color:#75715e&#34;&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#75715e&#34;&gt;#include&lt;/span&gt; &lt;span style=&#34;color:#75715e&#34;&gt;&amp;lt;iostream&amp;gt;&lt;/span&gt;&lt;span style=&#34;color:#75715e&#34;&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#75715e&#34;&gt;#include&lt;/span&gt; &lt;span style=&#34;color:#75715e&#34;&gt;&amp;lt;dirent.h&amp;gt;&lt;/span&gt;&lt;span style=&#34;color:#75715e&#34;&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#75715e&#34;&gt;#include&lt;/span&gt; &lt;span style=&#34;color:#75715e&#34;&gt;&amp;lt;opencv2/opencv.hpp&amp;gt;&lt;/span&gt;&lt;span style=&#34;color:#75715e&#34;&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/krshrimali/blog/main/assets/blogs/Include-Libraries.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt;
&lt;p&gt;After successfully importing libraries, we can define functions, write code and use the utilities Jupyter provides. Let&amp;rsquo;s start with playing with Tensors and the code snippets mentioned in the official &lt;a href=&#34;https://pytorch.org/cppdocs&#34;&gt;PyTorch C++ Frontend Docs&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;Starting with using &lt;code&gt;ATen&lt;/code&gt; tensor library. We&amp;rsquo;ll create two tensors and add them together. &lt;code&gt;ATen&lt;/code&gt; comes up with functionalities of mathematical operations on the Tensors.&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-cpp&#34; data-lang=&#34;cpp&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#75715e&#34;&gt;#include&lt;/span&gt; &lt;span style=&#34;color:#75715e&#34;&gt;&amp;lt;ATen/ATen.h&amp;gt;&lt;/span&gt;&lt;span style=&#34;color:#75715e&#34;&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#75715e&#34;&gt;&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;at&lt;span style=&#34;color:#f92672&#34;&gt;::&lt;/span&gt;Tensor a &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; at&lt;span style=&#34;color:#f92672&#34;&gt;::&lt;/span&gt;ones({&lt;span style=&#34;color:#ae81ff&#34;&gt;2&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;2&lt;/span&gt;}, at&lt;span style=&#34;color:#f92672&#34;&gt;::&lt;/span&gt;kInt);
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;at&lt;span style=&#34;color:#f92672&#34;&gt;::&lt;/span&gt;Tensor b &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; at&lt;span style=&#34;color:#f92672&#34;&gt;::&lt;/span&gt;randn({&lt;span style=&#34;color:#ae81ff&#34;&gt;2&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;2&lt;/span&gt;});
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#66d9ef&#34;&gt;auto&lt;/span&gt; c &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; a &lt;span style=&#34;color:#f92672&#34;&gt;+&lt;/span&gt; b.to(at&lt;span style=&#34;color:#f92672&#34;&gt;::&lt;/span&gt;kInt);
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;std&lt;span style=&#34;color:#f92672&#34;&gt;::&lt;/span&gt;cout &lt;span style=&#34;color:#f92672&#34;&gt;&amp;lt;&amp;lt;&lt;/span&gt; &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;a: &amp;#34;&lt;/span&gt; &lt;span style=&#34;color:#f92672&#34;&gt;&amp;lt;&amp;lt;&lt;/span&gt; a &lt;span style=&#34;color:#f92672&#34;&gt;&amp;lt;&amp;lt;&lt;/span&gt; std&lt;span style=&#34;color:#f92672&#34;&gt;::&lt;/span&gt;endl;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;std&lt;span style=&#34;color:#f92672&#34;&gt;::&lt;/span&gt;cout &lt;span style=&#34;color:#f92672&#34;&gt;&amp;lt;&amp;lt;&lt;/span&gt; std&lt;span style=&#34;color:#f92672&#34;&gt;::&lt;/span&gt;endl;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;std&lt;span style=&#34;color:#f92672&#34;&gt;::&lt;/span&gt;cout &lt;span style=&#34;color:#f92672&#34;&gt;&amp;lt;&amp;lt;&lt;/span&gt; &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;b: &amp;#34;&lt;/span&gt; &lt;span style=&#34;color:#f92672&#34;&gt;&amp;lt;&amp;lt;&lt;/span&gt; b &lt;span style=&#34;color:#f92672&#34;&gt;&amp;lt;&amp;lt;&lt;/span&gt; std&lt;span style=&#34;color:#f92672&#34;&gt;::&lt;/span&gt;endl;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;std&lt;span style=&#34;color:#f92672&#34;&gt;::&lt;/span&gt;cout &lt;span style=&#34;color:#f92672&#34;&gt;&amp;lt;&amp;lt;&lt;/span&gt; std&lt;span style=&#34;color:#f92672&#34;&gt;::&lt;/span&gt;endl;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;std&lt;span style=&#34;color:#f92672&#34;&gt;::&lt;/span&gt;cout &lt;span style=&#34;color:#f92672&#34;&gt;&amp;lt;&amp;lt;&lt;/span&gt; &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;c: &amp;#34;&lt;/span&gt; &lt;span style=&#34;color:#f92672&#34;&gt;&amp;lt;&amp;lt;&lt;/span&gt; c &lt;span style=&#34;color:#f92672&#34;&gt;&amp;lt;&amp;lt;&lt;/span&gt; std&lt;span style=&#34;color:#f92672&#34;&gt;::&lt;/span&gt;endl;
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/krshrimali/blog/main/assets/blogs/ATen-Example.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt;
&lt;p&gt;One of the reasons why &lt;code&gt;Xeus-Cling&lt;/code&gt; is useful is, that you can print the outputs of intermediate steps and debug. Let&amp;rsquo;s go ahead and experiment with &lt;code&gt;Autograd&lt;/code&gt; system of PyTorch C++ API.&lt;/p&gt;
&lt;p&gt;For those who don&amp;rsquo;t know, automatic differentiation is the most important function of Deep Learning algorithms to backpropagte the loss we calculate.&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-cpp&#34; data-lang=&#34;cpp&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#75715e&#34;&gt;#include&lt;/span&gt; &lt;span style=&#34;color:#75715e&#34;&gt;&amp;lt;torch/csrc/autograd/variable.h&amp;gt;&lt;/span&gt;&lt;span style=&#34;color:#75715e&#34;&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#75715e&#34;&gt;#include&lt;/span&gt; &lt;span style=&#34;color:#75715e&#34;&gt;&amp;lt;torch/csrc/autograd/function.h&amp;gt;&lt;/span&gt;&lt;span style=&#34;color:#75715e&#34;&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#75715e&#34;&gt;&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;torch&lt;span style=&#34;color:#f92672&#34;&gt;::&lt;/span&gt;Tensor a_tensor &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; torch&lt;span style=&#34;color:#f92672&#34;&gt;::&lt;/span&gt;ones({&lt;span style=&#34;color:#ae81ff&#34;&gt;2&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;2&lt;/span&gt;}, torch&lt;span style=&#34;color:#f92672&#34;&gt;::&lt;/span&gt;requires_grad());
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;torch&lt;span style=&#34;color:#f92672&#34;&gt;::&lt;/span&gt;Tensor b_tensor &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; torch&lt;span style=&#34;color:#f92672&#34;&gt;::&lt;/span&gt;randn({&lt;span style=&#34;color:#ae81ff&#34;&gt;2&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;2&lt;/span&gt;});
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;std&lt;span style=&#34;color:#f92672&#34;&gt;::&lt;/span&gt;cout &lt;span style=&#34;color:#f92672&#34;&gt;&amp;lt;&amp;lt;&lt;/span&gt; a_tensor &lt;span style=&#34;color:#f92672&#34;&gt;&amp;lt;&amp;lt;&lt;/span&gt; std&lt;span style=&#34;color:#f92672&#34;&gt;::&lt;/span&gt;endl;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;std&lt;span style=&#34;color:#f92672&#34;&gt;::&lt;/span&gt;cout &lt;span style=&#34;color:#f92672&#34;&gt;&amp;lt;&amp;lt;&lt;/span&gt; b_tensor &lt;span style=&#34;color:#f92672&#34;&gt;&amp;lt;&amp;lt;&lt;/span&gt; std&lt;span style=&#34;color:#f92672&#34;&gt;::&lt;/span&gt;endl;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#66d9ef&#34;&gt;auto&lt;/span&gt; c_tensor &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; a_tensor &lt;span style=&#34;color:#f92672&#34;&gt;+&lt;/span&gt; b_tensor;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;c_tensor.backward(); &lt;span style=&#34;color:#75715e&#34;&gt;// a.grad() will now hold the gradient of c w.r.t a
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#75715e&#34;&gt;&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;std&lt;span style=&#34;color:#f92672&#34;&gt;::&lt;/span&gt;cout &lt;span style=&#34;color:#f92672&#34;&gt;&amp;lt;&amp;lt;&lt;/span&gt; c_tensor &lt;span style=&#34;color:#f92672&#34;&gt;&amp;lt;&amp;lt;&lt;/span&gt; std&lt;span style=&#34;color:#f92672&#34;&gt;::&lt;/span&gt;endl;
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/krshrimali/blog/main/assets/blogs/Autograd-Example-1.png&#34; alt=&#34;&#34;&gt;
&lt;img src=&#34;https://raw.githubusercontent.com/krshrimali/blog/main/assets/blogs/Autograd-Example-2.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt;
&lt;p&gt;How about debugging? As you can see in the figure below, I get an error stating &lt;code&gt;no member named &#39;size&#39; in namespace &#39;cv&#39;&lt;/code&gt;. This is because namespace &lt;code&gt;cv&lt;/code&gt; has member called &lt;code&gt;Size&lt;/code&gt; and not &lt;code&gt;size&lt;/code&gt;.&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-cpp&#34; data-lang=&#34;cpp&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;torch&lt;span style=&#34;color:#f92672&#34;&gt;::&lt;/span&gt;Tensor read_images(std&lt;span style=&#34;color:#f92672&#34;&gt;::&lt;/span&gt;string location) {
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;	cv&lt;span style=&#34;color:#f92672&#34;&gt;::&lt;/span&gt;Mat img &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; cv&lt;span style=&#34;color:#f92672&#34;&gt;::&lt;/span&gt;imread(location, &lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;);
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;	cv&lt;span style=&#34;color:#f92672&#34;&gt;::&lt;/span&gt;resize(img, img, cv&lt;span style=&#34;color:#f92672&#34;&gt;::&lt;/span&gt;size(&lt;span style=&#34;color:#ae81ff&#34;&gt;224&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;224&lt;/span&gt;), cv&lt;span style=&#34;color:#f92672&#34;&gt;::&lt;/span&gt;INTER_CUBIC);
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;	torch&lt;span style=&#34;color:#f92672&#34;&gt;::&lt;/span&gt;Tensor img_tensor &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; torch&lt;span style=&#34;color:#f92672&#34;&gt;::&lt;/span&gt;from_blob(img.data, {img.rows, img.cols, &lt;span style=&#34;color:#ae81ff&#34;&gt;3&lt;/span&gt;}, torch&lt;span style=&#34;color:#f92672&#34;&gt;::&lt;/span&gt;kByte);
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;	img_tensor &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; img_tensor.permute({&lt;span style=&#34;color:#ae81ff&#34;&gt;2&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;0&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;});
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;	&lt;span style=&#34;color:#66d9ef&#34;&gt;return&lt;/span&gt; img_tensor.clone();
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;}
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/krshrimali/blog/main/assets/blogs/Debug-Example.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt;
&lt;p&gt;To solve, we can simply change the member from &lt;code&gt;size&lt;/code&gt; to &lt;code&gt;Size&lt;/code&gt;. One important point to consider is, that since this works on the top of Jupyter Interface, so whenever you re-run a cell, the variable names need to be changed as it will return an error of re-defining the variables which have already been defined.&lt;/p&gt;
&lt;p&gt;For testing, I have implemented Transfer Learning example that we discussed in the &lt;a href=&#34;https://krshrimali.github.io/posts/2019/08/applying-transfer-learning-on-dogs-vs-cats-dataset-resnet18-using-pytorch-c-api/&#34;&gt;previous blog&lt;/a&gt;. This comes handy as I don&amp;rsquo;t need to load the dataset again and again.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/krshrimali/blog/main/assets/blogs/Training-Image.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt;
&lt;h2 id=&#34;bonus&#34;&gt;Bonus!&lt;/h2&gt;
&lt;p&gt;With this blog, I&amp;rsquo;m also happy to share a Notebook file with implementation of Transfer Learning using ResNet18 Model on Dogs vs Cats Dataset. Additionally, I&amp;rsquo;m elated to open source the code for Transfer Learning using ResNet18 Model using PyTorch C++ API.&lt;/p&gt;
&lt;p&gt;The source code and the notebook file can be found &lt;a href=&#34;https://github.com/krshrimali/Transfer-Learning-Dogs-Cats-Libtorch.git&#34;&gt;here&lt;/a&gt;.&lt;/p&gt;
&lt;h2 id=&#34;debugging---osx-systems&#34;&gt;Debugging - OSX Systems&lt;/h2&gt;
&lt;p&gt;In case of OSX Systems, if you see any errors similar to: &lt;code&gt;You are probably missing the definition of &amp;lt;function_name&amp;gt;&lt;/code&gt;, then try any (or all) of the following points:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Use &lt;code&gt;Xeus-Cling&lt;/code&gt; on a virtual environment as this might be because of conflicts with the existing libraries.&lt;/li&gt;
&lt;li&gt;Although, OSX Systems shouldn&amp;rsquo;t have &lt;code&gt;C++ ABI Compatability&lt;/code&gt; Issues but you can still try this if problem persists.
&lt;ol&gt;
&lt;li&gt;Go to &lt;code&gt;TorchCONFIG.cmake&lt;/code&gt; file (it should be present in &lt;code&gt;&amp;lt;torch_folder&amp;gt;/share/cmake/Torch/&lt;/code&gt;).&lt;/li&gt;
&lt;li&gt;Change &lt;code&gt;set(TORCH_CXX_FLAGS &amp;quot;-D_GLIBCXX_USE_CXX11_ABI=&amp;quot;)&lt;/code&gt; to &lt;code&gt;set(TORCH_CXX_FLAGS &amp;quot;-D_GLIBCXX_USE_CXX11_ABI=1&amp;quot;)&lt;/code&gt; and reload the libraries and header files.&lt;/li&gt;
&lt;/ol&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;h2 id=&#34;references&#34;&gt;References&lt;/h2&gt;
&lt;ol&gt;
&lt;li&gt;&lt;a href=&#34;https://www.learnopencv.com/xeus-cling-run-c-code-in-jupyter-notebook/&#34;&gt;Xeus-Cling: Run C++ code in Jupyter Notebook by Vishwesh Ravi Shrimali&lt;/a&gt;.&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://xeus-cling.readthedocs.io/en/latest&#34;&gt;Documentation of Xeus-Cling&lt;/a&gt;.&lt;/li&gt;
&lt;/ol&gt;

      </description>
    </item>
    
    <item>
      <title>Applying Transfer Learning on Dogs vs Cats Dataset (ResNet18) using PyTorch C&#43;&#43; API</title>
      <link>https://krshrimali.github.io/posts/2019/08/applying-transfer-learning-on-dogs-vs-cats-dataset-resnet18-using-pytorch-c-api/</link>
      <pubDate>Fri, 16 Aug 2019 00:00:00 +0000</pubDate>
      
      <guid>https://krshrimali.github.io/posts/2019/08/applying-transfer-learning-on-dogs-vs-cats-dataset-resnet18-using-pytorch-c-api/</guid>
      <description>
          
            &lt;h4&gt;Applying Transfer Learning on Dogs vs Cats Dataset (ResNet18) using PyTorch C++ API&lt;/h4&gt;
          
          &lt;h2 id=&#34;transfer-learning&#34;&gt;Transfer Learning&lt;/h2&gt;
&lt;p&gt;&amp;ndash;
Before we go ahead and discuss the &lt;strong&gt;Why&lt;/strong&gt; question of Transfer Learning, let&amp;rsquo;s have a look at &lt;strong&gt;What is Transfer Learning?&lt;/strong&gt; Let&amp;rsquo;s have a look at the &lt;a href=&#34;http://cs231n.github.io/transfer-learning&#34;&gt;Notes&lt;/a&gt; from CS231n on Transfer Learning:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;In practice, very few people train an entire Convolutional Network from scratch (with random initialization), because it is relatively rare to have a dataset of sufficient size. Instead, it is common to pretrain a ConvNet on a very large dataset (e.g. ImageNet, which contains 1.2 million images with 1000 categories), and then use the ConvNet either as an initialization or a fixed feature extractor for the task of interest.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/krshrimali/blog/main/assets/cover-images/Cover-Transfer-Learning.jpg&#34; alt=&#34;&#34;&gt;&lt;/p&gt;
&lt;p&gt;There are 3 scenarios possible:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;When the data you have is similar (but not enough) to data trained on pre-trained model: Take an example of a pre-trained model trained on ImageNet dataset (containing 1000 classes). And the data we have has Dogs and Cats classes. Fortunate enough, ImageNet has some of the classes of Dog and Cat breeds and thus the model must have learned important features from the data. Let&amp;rsquo;s say we don&amp;rsquo;t have enough data but since the data is similar to the breeds in the ImageNet data set, we can simply use the ConvNet (except the last FC layer) to extract features from our dataset and train only the last Linear (FC) layer.&lt;/li&gt;
&lt;li&gt;When you have enough data (and is similar to the data trained with pre-trained model): Then you might go for fine tuning the weights of all the layers in the network. This is largely due to the reason that we know we won&amp;rsquo;t overfit because we have enough data.&lt;/li&gt;
&lt;li&gt;Using pre-trained models might just be enough if you have the data which matches the classes in the original data set.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;Transfer Learning came into existence (the answer of &lt;strong&gt;Why Transfer Learning?&lt;/strong&gt;) because of some major reasons, which include:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Lack of resources or data set to train a CNN. At times, we either don&amp;rsquo;t have enough data or we don&amp;rsquo;t have enough resources to train a CNN from scratch.&lt;/li&gt;
&lt;li&gt;Random Initialization of weights vs Initialization of weights from the pre-trained model. Sometimes, it&amp;rsquo;s just better to initialize weights from the pre-trained model (as it must have learned the generic features from it&amp;rsquo;s data set) instead of randomly initializing the weights.&lt;/li&gt;
&lt;/ol&gt;
&lt;h2 id=&#34;setting-up-the-data-with-pytorch-c-api&#34;&gt;Setting up the data with PyTorch C++ API&lt;/h2&gt;
&lt;p&gt;At every stage, we will compare the Python and C++ codes to do the same thing, to make the analogy easier and understandable. Starting with setting up the data we have. Note that we do have enough data and it is also similar to the original data set of ImageNet, but since I don&amp;rsquo;t have enough resources to fine tune through the whole network, we perform Transfer Learning on the final FC layer only.&lt;/p&gt;
&lt;p&gt;Starting with loading the dataset, as discussed in the blogs before, I&amp;rsquo;ll just post a flow chart of procedure.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/krshrimali/blog/main/assets/blogs/Steps-Loading-Data-PyTorch.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt;
&lt;p&gt;Once done, we can initialize the &lt;code&gt;CustomDataset&lt;/code&gt; class:&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;C++&lt;/strong&gt;&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-cpp&#34; data-lang=&#34;cpp&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#75715e&#34;&gt;// List of images of Dogs and Cats, use load_data_from_folder function explained in previous blogs
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#75715e&#34;&gt;&lt;/span&gt;std&lt;span style=&#34;color:#f92672&#34;&gt;::&lt;/span&gt;vector&lt;span style=&#34;color:#f92672&#34;&gt;&amp;lt;&lt;/span&gt;std&lt;span style=&#34;color:#f92672&#34;&gt;::&lt;/span&gt;string&lt;span style=&#34;color:#f92672&#34;&gt;&amp;gt;&lt;/span&gt; list_images; 
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#75715e&#34;&gt;// List of labels of the images
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#75715e&#34;&gt;&lt;/span&gt;std&lt;span style=&#34;color:#f92672&#34;&gt;::&lt;/span&gt;vector&lt;span style=&#34;color:#f92672&#34;&gt;&amp;lt;&lt;/span&gt;&lt;span style=&#34;color:#66d9ef&#34;&gt;int&lt;/span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;&amp;gt;&lt;/span&gt; list_labels; 
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#66d9ef&#34;&gt;auto&lt;/span&gt; custom_dataset &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; CustomDataset(list_images, list_labels).map(torch&lt;span style=&#34;color:#f92672&#34;&gt;::&lt;/span&gt;data&lt;span style=&#34;color:#f92672&#34;&gt;::&lt;/span&gt;transforms&lt;span style=&#34;color:#f92672&#34;&gt;::&lt;/span&gt;Stack&lt;span style=&#34;color:#f92672&#34;&gt;&amp;lt;&amp;gt;&lt;/span&gt;());
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;&lt;strong&gt;Python&lt;/strong&gt;&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;from&lt;/span&gt; torchvision &lt;span style=&#34;color:#f92672&#34;&gt;import&lt;/span&gt; datasets, transforms
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;import&lt;/span&gt; torch
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;folder_path &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;/Users/krshrimali/Documents/dataset/train/&amp;#34;&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;transform &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; transforms&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;Compose([transforms&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;CenterCrop(&lt;span style=&#34;color:#ae81ff&#34;&gt;224&lt;/span&gt;), transforms&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;ToTensor())
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;data &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; datasets&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;ImageFolder(root &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; os&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;path&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;join(folder_path), transform &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; transform)
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;We then use &lt;code&gt;RandomSampler&lt;/code&gt; to make our data loader: (Note: it&amp;rsquo;s important to use &lt;code&gt;RandomSampler&lt;/code&gt; as we load the images sequentially and we want mixture of images in each batch of data passed to the network in an epoch)&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;C++&lt;/strong&gt;&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-cpp&#34; data-lang=&#34;cpp&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#66d9ef&#34;&gt;int&lt;/span&gt; batch_size &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; &lt;span style=&#34;color:#ae81ff&#34;&gt;4&lt;/span&gt;;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#66d9ef&#34;&gt;auto&lt;/span&gt; data_loader &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; torch&lt;span style=&#34;color:#f92672&#34;&gt;::&lt;/span&gt;data&lt;span style=&#34;color:#f92672&#34;&gt;::&lt;/span&gt;make_data_loader&lt;span style=&#34;color:#f92672&#34;&gt;&amp;lt;&lt;/span&gt;torch&lt;span style=&#34;color:#f92672&#34;&gt;::&lt;/span&gt;data&lt;span style=&#34;color:#f92672&#34;&gt;::&lt;/span&gt;samplers&lt;span style=&#34;color:#f92672&#34;&gt;::&lt;/span&gt;RandomSampler&lt;span style=&#34;color:#f92672&#34;&gt;&amp;gt;&lt;/span&gt;(std&lt;span style=&#34;color:#f92672&#34;&gt;::&lt;/span&gt;move(custom_dataset), batch_size);
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;&lt;strong&gt;Python&lt;/strong&gt;&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;batch_size &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; &lt;span style=&#34;color:#ae81ff&#34;&gt;4&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;data_loader &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; torch&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;utils&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;data&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;DataLoader(dataset&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;data, batch_size &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; batch_size, shuffle &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; &lt;span style=&#34;color:#66d9ef&#34;&gt;True&lt;/span&gt;)
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;h2 id=&#34;loading-the-pre-trained-model&#34;&gt;Loading the pre-trained model&lt;/h2&gt;
&lt;p&gt;The steps to load the pre-trained model and perform Transfer Learning are listed below:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Download the pre-trained model of &lt;strong&gt;ResNet18&lt;/strong&gt;.&lt;/li&gt;
&lt;li&gt;Load pre-trained model.&lt;/li&gt;
&lt;li&gt;Change output features of the final FC layer of the model loaded. (Number of classes would change from 1000 - ImageNet to 2 - Dogs vs Cats).&lt;/li&gt;
&lt;li&gt;Define optimizer on parameters from the final FC layer to be trained.&lt;/li&gt;
&lt;li&gt;Train the FC layer on Dogs vs Cats dataset.&lt;/li&gt;
&lt;li&gt;Save the model.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;Let&amp;rsquo;s go step by step.&lt;/p&gt;
&lt;h3 id=&#34;step-1-download-the-pre-trained-model-of-resnet18&#34;&gt;Step-1: Download the pre-trained model of ResNet18&lt;/h3&gt;
&lt;p&gt;Thanks to the developers, we do have C++ models available in torchvision
(&lt;a href=&#34;https://github.com/pytorch/vision/pull/728&#34;&gt;https://github.com/pytorch/vision/pull/728&lt;/a&gt;) but for this tutorial, transferring the pre- trained model from Python to C++ using torch.jit is a good idea, as most PyTorch models in the wild are written in Python right now, and people can use this tutorial to learn how to trace their Python model and transfer it to C++.&lt;/p&gt;
&lt;p&gt;First we download the pre-trained model and save it in the form of &lt;code&gt;torch.jit.trace&lt;/code&gt; format to our local drive.&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#75715e&#34;&gt;# Reference: #TODO- Add Link&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;from&lt;/span&gt; torchvision &lt;span style=&#34;color:#f92672&#34;&gt;import&lt;/span&gt; models
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#75715e&#34;&gt;# Download and load the pre-trained model&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;model &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; models&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;resnet18(pretrained&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#66d9ef&#34;&gt;True&lt;/span&gt;)
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#75715e&#34;&gt;# Set upgrading the gradients to False&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#66d9ef&#34;&gt;for&lt;/span&gt; param &lt;span style=&#34;color:#f92672&#34;&gt;in&lt;/span&gt; model&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;parameters():
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;	param&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;requires_grad &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; &lt;span style=&#34;color:#66d9ef&#34;&gt;False&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#75715e&#34;&gt;# Save the model except the final FC Layer&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;resnet18 &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; torch&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;nn&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;Sequential(&lt;span style=&#34;color:#f92672&#34;&gt;*&lt;/span&gt;list(resnet18&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;children())[:&lt;span style=&#34;color:#f92672&#34;&gt;-&lt;/span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;])
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;example_input &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; torch&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;rand(&lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;3&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;224&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;224&lt;/span&gt;)
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;script_module &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; torch&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;jit&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;trace(resnet18, example_input)
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;script_module&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;save(&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;resnet18_without_last_layer.pt&amp;#39;&lt;/span&gt;)
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;We will be using &lt;code&gt;resnet18_without_last_layer.pt&lt;/code&gt; model file as our pre-trained model for transfer learning.&lt;/p&gt;
&lt;h3 id=&#34;step-2-load-the-pre-trained-model&#34;&gt;Step-2: Load the pre-trained model&lt;/h3&gt;
&lt;p&gt;Let&amp;rsquo;s go ahead and load the pre-trained model using &lt;code&gt;torch::jit&lt;/code&gt; module. Note that the reason we have converted &lt;code&gt;torch.nn.Module&lt;/code&gt; to &lt;code&gt;torch.jit.ScriptModule&lt;/code&gt; type, is because C++ API currently does not support loading Python &lt;code&gt;torch.nn.Module&lt;/code&gt; models directly.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;C++&lt;/strong&gt;:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-cpp&#34; data-lang=&#34;cpp&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;torch&lt;span style=&#34;color:#f92672&#34;&gt;::&lt;/span&gt;jit&lt;span style=&#34;color:#f92672&#34;&gt;::&lt;/span&gt;script&lt;span style=&#34;color:#f92672&#34;&gt;::&lt;/span&gt;Module module;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#75715e&#34;&gt;// argv[1] should be the path to the model
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#75715e&#34;&gt;&lt;/span&gt;module &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; torch&lt;span style=&#34;color:#f92672&#34;&gt;::&lt;/span&gt;jit&lt;span style=&#34;color:#f92672&#34;&gt;::&lt;/span&gt;load(argv[&lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;]); 
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#75715e&#34;&gt;/* We need to convert last layer input and output features from (512, 1000) to (512, 2) since we only have 2 classes */&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;torch&lt;span style=&#34;color:#f92672&#34;&gt;::&lt;/span&gt;nn&lt;span style=&#34;color:#f92672&#34;&gt;::&lt;/span&gt;Linear linear_layer(&lt;span style=&#34;color:#ae81ff&#34;&gt;512&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;2&lt;/span&gt;);
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#75715e&#34;&gt;// Define the optimizer on parameters of linear_layer with learning_rate = 1e-3
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#75715e&#34;&gt;&lt;/span&gt;torch&lt;span style=&#34;color:#f92672&#34;&gt;::&lt;/span&gt;optim&lt;span style=&#34;color:#f92672&#34;&gt;::&lt;/span&gt;Adam optimizer(linear_layer&lt;span style=&#34;color:#f92672&#34;&gt;-&amp;gt;&lt;/span&gt;parameters(), torch&lt;span style=&#34;color:#f92672&#34;&gt;::&lt;/span&gt;optim&lt;span style=&#34;color:#f92672&#34;&gt;::&lt;/span&gt;AdamOptions(&lt;span style=&#34;color:#ae81ff&#34;&gt;1e-3&lt;/span&gt;))
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;&lt;strong&gt;Python&lt;/strong&gt;:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#75715e&#34;&gt;# We will directly load the torch.nn pre-trained model&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;model &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; models&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;resnet18(pretrained &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; &lt;span style=&#34;color:#66d9ef&#34;&gt;True&lt;/span&gt;)
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#66d9ef&#34;&gt;for&lt;/span&gt; param &lt;span style=&#34;color:#f92672&#34;&gt;in&lt;/span&gt; model&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;parameters():
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    param&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;requires_grad &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; &lt;span style=&#34;color:#66d9ef&#34;&gt;False&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;model&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;fc &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; torch&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;nn&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;Linear(&lt;span style=&#34;color:#ae81ff&#34;&gt;512&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;2&lt;/span&gt;)
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#66d9ef&#34;&gt;for&lt;/span&gt; param &lt;span style=&#34;color:#f92672&#34;&gt;in&lt;/span&gt; model&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;fc&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;parameters():
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;	param&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;requires_grad &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; &lt;span style=&#34;color:#66d9ef&#34;&gt;True&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;optimizer &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; torch&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;optim&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;Adam(model&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;fc&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;parameters())
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;cost &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; torch&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;nn&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;CrossEntropyLoss()
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;h2 id=&#34;trainining-the-fc-layer&#34;&gt;Trainining the FC Layer&lt;/h2&gt;
&lt;p&gt;Let&amp;rsquo;s first have a look at ResNet18 Network Architecture&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/krshrimali/blog/main/assets/blogs/ResNet18-Architecture.png&#34; alt=&#34;https://www.researchgate.net/figure/ResNet-18-Architecture_tbl1_322476121&#34;&gt;&lt;/p&gt;
&lt;p&gt;The final step is to train the Fully Connected layer that we inserted at the end of the network (&lt;code&gt;linear_layer&lt;/code&gt;). This one should be pretty straight forward, let&amp;rsquo;s see how to do it.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;C++&lt;/strong&gt;:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-cpp&#34; data-lang=&#34;cpp&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#66d9ef&#34;&gt;void&lt;/span&gt; &lt;span style=&#34;color:#a6e22e&#34;&gt;train&lt;/span&gt;(torch&lt;span style=&#34;color:#f92672&#34;&gt;::&lt;/span&gt;jit&lt;span style=&#34;color:#f92672&#34;&gt;::&lt;/span&gt;script&lt;span style=&#34;color:#f92672&#34;&gt;::&lt;/span&gt;Module net, torch&lt;span style=&#34;color:#f92672&#34;&gt;::&lt;/span&gt;nn&lt;span style=&#34;color:#f92672&#34;&gt;::&lt;/span&gt;Linear lin, Dataloader&lt;span style=&#34;color:#f92672&#34;&gt;&amp;amp;&lt;/span&gt; data_loader, torch&lt;span style=&#34;color:#f92672&#34;&gt;::&lt;/span&gt;optim&lt;span style=&#34;color:#f92672&#34;&gt;::&lt;/span&gt;Optimizer&lt;span style=&#34;color:#f92672&#34;&gt;&amp;amp;&lt;/span&gt; optimizer, size_t dataset_size) {
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#75715e&#34;&gt;/*
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#75715e&#34;&gt;     This function trains the network on our data loader using optimizer for given number of epochs.
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#75715e&#34;&gt;     
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#75715e&#34;&gt;     Parameters
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#75715e&#34;&gt;     ==================
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#75715e&#34;&gt;     torch::jit::script::Module net: Pre-trained model
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#75715e&#34;&gt;     torch::nn::Linear lin: Linear layer
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#75715e&#34;&gt;     DataLoader&amp;amp; data_loader: Training data loader
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#75715e&#34;&gt;     torch::optim::Optimizer&amp;amp; optimizer: Optimizer like Adam, SGD etc.
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#75715e&#34;&gt;     size_t dataset_size: Size of training dataset
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#75715e&#34;&gt;     */&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#66d9ef&#34;&gt;float&lt;/span&gt; batch_index &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; &lt;span style=&#34;color:#ae81ff&#34;&gt;0&lt;/span&gt;;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#66d9ef&#34;&gt;for&lt;/span&gt;(&lt;span style=&#34;color:#66d9ef&#34;&gt;int&lt;/span&gt; i&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;0&lt;/span&gt;; i&lt;span style=&#34;color:#f92672&#34;&gt;&amp;lt;&lt;/span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;15&lt;/span&gt;; i&lt;span style=&#34;color:#f92672&#34;&gt;++&lt;/span&gt;) {
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        &lt;span style=&#34;color:#66d9ef&#34;&gt;float&lt;/span&gt; mse &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; &lt;span style=&#34;color:#ae81ff&#34;&gt;0&lt;/span&gt;;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        &lt;span style=&#34;color:#66d9ef&#34;&gt;float&lt;/span&gt; Acc &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; &lt;span style=&#34;color:#ae81ff&#34;&gt;0.0&lt;/span&gt;;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        &lt;span style=&#34;color:#66d9ef&#34;&gt;for&lt;/span&gt;(&lt;span style=&#34;color:#66d9ef&#34;&gt;auto&lt;/span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;&amp;amp;&lt;/span&gt; batch: &lt;span style=&#34;color:#f92672&#34;&gt;*&lt;/span&gt;data_loader) {
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;            &lt;span style=&#34;color:#66d9ef&#34;&gt;auto&lt;/span&gt; data &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; batch.data;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;            &lt;span style=&#34;color:#66d9ef&#34;&gt;auto&lt;/span&gt; target &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; batch.target.squeeze();
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;            
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;            &lt;span style=&#34;color:#75715e&#34;&gt;// Should be of length: batch_size
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#75715e&#34;&gt;&lt;/span&gt;            data &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; data.to(torch&lt;span style=&#34;color:#f92672&#34;&gt;::&lt;/span&gt;kF32);
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;            target &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; target.to(torch&lt;span style=&#34;color:#f92672&#34;&gt;::&lt;/span&gt;kInt64);
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;            
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;            std&lt;span style=&#34;color:#f92672&#34;&gt;::&lt;/span&gt;vector&lt;span style=&#34;color:#f92672&#34;&gt;&amp;lt;&lt;/span&gt;torch&lt;span style=&#34;color:#f92672&#34;&gt;::&lt;/span&gt;jit&lt;span style=&#34;color:#f92672&#34;&gt;::&lt;/span&gt;IValue&lt;span style=&#34;color:#f92672&#34;&gt;&amp;gt;&lt;/span&gt; input;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;            input.push_back(data);
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;            optimizer.zero_grad();
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;            &lt;span style=&#34;color:#66d9ef&#34;&gt;auto&lt;/span&gt; output &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; net.forward(input).toTensor();
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;            &lt;span style=&#34;color:#75715e&#34;&gt;// For transfer learning
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#75715e&#34;&gt;&lt;/span&gt;            output &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; output.view({output.size(&lt;span style=&#34;color:#ae81ff&#34;&gt;0&lt;/span&gt;), &lt;span style=&#34;color:#f92672&#34;&gt;-&lt;/span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;});
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;            output &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; lin(output);
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;            &lt;span style=&#34;color:#75715e&#34;&gt;// Explicitly calculate torch::log_softmax of output from the FC Layer
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#75715e&#34;&gt;&lt;/span&gt;            &lt;span style=&#34;color:#66d9ef&#34;&gt;auto&lt;/span&gt; loss &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; torch&lt;span style=&#34;color:#f92672&#34;&gt;::&lt;/span&gt;nll_loss(torch&lt;span style=&#34;color:#f92672&#34;&gt;::&lt;/span&gt;log_softmax(output, &lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;), target);
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;           	
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;            loss.backward();
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;            optimizer.step();
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;            
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;            &lt;span style=&#34;color:#66d9ef&#34;&gt;auto&lt;/span&gt; acc &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; output.argmax(&lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;).eq(target).sum();
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;            Acc &lt;span style=&#34;color:#f92672&#34;&gt;+=&lt;/span&gt; acc.&lt;span style=&#34;color:#66d9ef&#34;&gt;template&lt;/span&gt; item&lt;span style=&#34;color:#f92672&#34;&gt;&amp;lt;&lt;/span&gt;&lt;span style=&#34;color:#66d9ef&#34;&gt;float&lt;/span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;&amp;gt;&lt;/span&gt;();
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;            mse &lt;span style=&#34;color:#f92672&#34;&gt;+=&lt;/span&gt; loss.&lt;span style=&#34;color:#66d9ef&#34;&gt;template&lt;/span&gt; item&lt;span style=&#34;color:#f92672&#34;&gt;&amp;lt;&lt;/span&gt;&lt;span style=&#34;color:#66d9ef&#34;&gt;float&lt;/span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;&amp;gt;&lt;/span&gt;();
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;            
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;            batch_index &lt;span style=&#34;color:#f92672&#34;&gt;+=&lt;/span&gt; &lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        }
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        mse &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; mse&lt;span style=&#34;color:#f92672&#34;&gt;/&lt;/span&gt;&lt;span style=&#34;color:#66d9ef&#34;&gt;float&lt;/span&gt;(batch_index); &lt;span style=&#34;color:#75715e&#34;&gt;// Take mean of loss
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#75715e&#34;&gt;&lt;/span&gt;        std&lt;span style=&#34;color:#f92672&#34;&gt;::&lt;/span&gt;cout &lt;span style=&#34;color:#f92672&#34;&gt;&amp;lt;&amp;lt;&lt;/span&gt; &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;Epoch: &amp;#34;&lt;/span&gt; &lt;span style=&#34;color:#f92672&#34;&gt;&amp;lt;&amp;lt;&lt;/span&gt; i  &lt;span style=&#34;color:#f92672&#34;&gt;&amp;lt;&amp;lt;&lt;/span&gt; &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;, &amp;#34;&lt;/span&gt; &lt;span style=&#34;color:#f92672&#34;&gt;&amp;lt;&amp;lt;&lt;/span&gt; &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;Accuracy: &amp;#34;&lt;/span&gt; &lt;span style=&#34;color:#f92672&#34;&gt;&amp;lt;&amp;lt;&lt;/span&gt; Acc&lt;span style=&#34;color:#f92672&#34;&gt;/&lt;/span&gt;dataset_size &lt;span style=&#34;color:#f92672&#34;&gt;&amp;lt;&amp;lt;&lt;/span&gt; &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;, &amp;#34;&lt;/span&gt; &lt;span style=&#34;color:#f92672&#34;&gt;&amp;lt;&amp;lt;&lt;/span&gt; &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;MSE: &amp;#34;&lt;/span&gt; &lt;span style=&#34;color:#f92672&#34;&gt;&amp;lt;&amp;lt;&lt;/span&gt; mse &lt;span style=&#34;color:#f92672&#34;&gt;&amp;lt;&amp;lt;&lt;/span&gt; std&lt;span style=&#34;color:#f92672&#34;&gt;::&lt;/span&gt;endl;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        net.save(&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;model.pt&amp;#34;&lt;/span&gt;);
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    }
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;}
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;&lt;strong&gt;Python&lt;/strong&gt;:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;n_epochs &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; &lt;span style=&#34;color:#ae81ff&#34;&gt;15&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#66d9ef&#34;&gt;for&lt;/span&gt; epoch &lt;span style=&#34;color:#f92672&#34;&gt;in&lt;/span&gt; range(n_epochs):
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    mse &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; &lt;span style=&#34;color:#ae81ff&#34;&gt;0.0&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    acc &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; &lt;span style=&#34;color:#ae81ff&#34;&gt;0&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    batch_index &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; &lt;span style=&#34;color:#ae81ff&#34;&gt;0&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#66d9ef&#34;&gt;for&lt;/span&gt; data_batch &lt;span style=&#34;color:#f92672&#34;&gt;in&lt;/span&gt; data_loader:
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        batch_index &lt;span style=&#34;color:#f92672&#34;&gt;+=&lt;/span&gt; &lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        image, label &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; data_batch
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        optimizer&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;zero_grad()
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        output &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; model(image)
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        _, predicted_label &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; torch&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;max(output&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;data, &lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;)
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        loss &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; cost(output, label)
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        loss&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;backward()
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        optimizer&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;step()
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        mse &lt;span style=&#34;color:#f92672&#34;&gt;+=&lt;/span&gt; loss&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;item() &lt;span style=&#34;color:#75715e&#34;&gt;# data[0]&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        acc &lt;span style=&#34;color:#f92672&#34;&gt;+=&lt;/span&gt; torch&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;sum(predicted_label &lt;span style=&#34;color:#f92672&#34;&gt;==&lt;/span&gt; label&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;data)
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    mse &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; mse&lt;span style=&#34;color:#f92672&#34;&gt;/&lt;/span&gt;len(data)
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    acc &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; &lt;span style=&#34;color:#ae81ff&#34;&gt;100&lt;/span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;*&lt;/span&gt;acc&lt;span style=&#34;color:#f92672&#34;&gt;/&lt;/span&gt;len(data)
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    print(&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;Epoch: &lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;{}&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;/&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;{}&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;, Loss: &lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;{:.4f}&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;, Accuracy: &lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;{:.4f}&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;&lt;/span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;format(epoch&lt;span style=&#34;color:#f92672&#34;&gt;+&lt;/span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;, n_epochs, mse, acc))
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;The code to test should also not change much except the need of optimizer.&lt;/p&gt;
&lt;h2 id=&#34;results&#34;&gt;Results&lt;/h2&gt;
&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/krshrimali/blog/main/assets/blogs/Training-Results.png&#34; alt=&#34;Results using PyTorch C++ API&#34;&gt;
&lt;img src=&#34;https://raw.githubusercontent.com/krshrimali/blog/main/assets/blogs/Training-Results-Python.png&#34; alt=&#34;Results using PyTorch in Python&#34;&gt;&lt;/p&gt;
&lt;p&gt;On a set of 400 images for training data, the maximum training Accuracy I could achieve was 91.25% in just less than 15 epochs using PyTorch C++ API and 89.0% using Python. (Note that this doesn&amp;rsquo;t conclude superiority in terms of accuracy between any of the two backends - C++ or Python)&lt;/p&gt;
&lt;p&gt;Let&amp;rsquo;s have a look at correct and wrong predictions.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Correct Predictions - Dogs&lt;/strong&gt;
&lt;img src=&#34;https://raw.githubusercontent.com/krshrimali/blog/main/assets/blogs/correct-predictions-dogs-transfer-learning.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Wrong Predictions - Dogs&lt;/strong&gt;
&lt;img src=&#34;https://raw.githubusercontent.com/krshrimali/blog/main/assets/blogs/wrong-predictions-dogs-transfer-learning.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Correct Predictions - Cats&lt;/strong&gt;
&lt;img src=&#34;https://raw.githubusercontent.com/krshrimali/blog/main/assets/blogs/correct-predictions-cats-transfer-learning.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Wrong Predictions - Cats&lt;/strong&gt;
&lt;img src=&#34;https://raw.githubusercontent.com/krshrimali/blog/main/assets/blogs/wrong-predictions-cats-transfer-learning.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt;
&lt;h2 id=&#34;acknowledgements&#34;&gt;Acknowledgements&lt;/h2&gt;
&lt;p&gt;I would like to thank a few people to help me bring this out to the community. Thanks to &lt;a href=&#34;https://github.com/ptrblck&#34;&gt;Piotr&lt;/a&gt; for his comments and answers in the PyTorch Discussion forum. Thanks to &lt;a href=&#34;https://github.com/yf225&#34;&gt;Will Feng&lt;/a&gt; for reviewing the blog and the code and also his constant motivation to bring this out to you all. Would like to thank my constant motivation behind all my work, &lt;a href=&#34;https://github.com/vishwesh5&#34;&gt;Vishwesh Ravi Shrimali&lt;/a&gt; for all his help to start with PyTorch C++ API and help the community. Special thanks to &lt;a href=&#34;https://github.com/krutikabapat&#34;&gt;Krutika Bapat&lt;/a&gt; as well, for reviewing the Python equivalent code and suggesting modifications.&lt;/p&gt;
&lt;p&gt;And shout out to all the readers, please share your feedback with me in the comments below. I would love to hear if this blog helped you!&lt;/p&gt;
&lt;p&gt;In the upcoming blog, I&amp;rsquo;ll be sharing something very exciting. Till then, happy learning!&lt;/p&gt;

      </description>
    </item>
    
    <item>
      <title>Classifying Dogs vs Cats using PyTorch C&#43;&#43;: Part 2</title>
      <link>https://krshrimali.github.io/posts/2019/07/classifying-dogs-vs-cats-using-pytorch-c-part-2/</link>
      <pubDate>Wed, 31 Jul 2019 00:00:00 +0000</pubDate>
      
      <guid>https://krshrimali.github.io/posts/2019/07/classifying-dogs-vs-cats-using-pytorch-c-part-2/</guid>
      <description>
          
            &lt;h4&gt;Classifying Dogs vs Cats using PyTorch C++: Part 2&lt;/h4&gt;
          
          &lt;p&gt;In the last blog, we had discussed all but training and results of our custom CNN network on Dogs vs Cats dataset. Today, we&amp;rsquo;ll be making some small changes in the network and discussing training and results of the task.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/krshrimali/blog/main/assets/cover-images/Classify-Dogs-Cats-Blog-05.jpg&#34; alt=&#34;&#34;&gt;&lt;/p&gt;
&lt;p&gt;I&amp;rsquo;ll start with the network overview again, where we used a network similar to VGG-16 (with one extra Fully Connected Layer in the end). While there are absolutely no problems with that network, but since the dataset contains a lot of images (25000 in training dataset) and we were using (200x200x3) input shape to the network (which is 120,000 floating point numbers), this leads to high memory consumption. In short, I was out of RAM to store these many images during program execution.&lt;/p&gt;
&lt;p&gt;So, I decided to change some minute things:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Input Shape to the network is now &lt;code&gt;64x64x3&lt;/code&gt; (12,288 parameters - around 10 times lesser than for &lt;code&gt;200x200x3&lt;/code&gt;). So, all the images are now resized to &lt;code&gt;64x64x3&lt;/code&gt;.&lt;/li&gt;
&lt;li&gt;Now, we only use 2 Convolutional Layers and 2 Max Pooling Layers to train our dataset. This helps to reduce the parameters for training, and also fastens the training process.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;But this comes with a tradeoff in accuracy, which will suffice for now as our target is not to get some X accuracy, but to learn how to train the network on our dataset using PyTorch C++ API.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Question&lt;/strong&gt;: Does reducing input resolution, affects accuracy?
&lt;strong&gt;Answer&lt;/strong&gt;: In this case, it will. The objects in our dataset (dogs and cats) have both high level and low level features, which are visible (provides more details) more to the network with high resolution. In this way, the network is allowed to learn more features out of the dataset. However, in cases like of MNIST, it&amp;rsquo;s just fine to use &lt;code&gt;64x64&lt;/code&gt; input resolution as it still allows the network to look at details of a digit and learn robust features.&lt;/p&gt;
&lt;p&gt;Let&amp;rsquo;s go ahead and see what has changed in the Network.&lt;/p&gt;
&lt;h2 id=&#34;network-overview&#34;&gt;Network Overview&lt;/h2&gt;
&lt;p&gt;If you don&amp;rsquo;t remember from the last time, this is how our network looked.&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-cpp&#34; data-lang=&#34;cpp&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#66d9ef&#34;&gt;struct&lt;/span&gt; &lt;span style=&#34;color:#a6e22e&#34;&gt;NetImpl&lt;/span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;:&lt;/span&gt; &lt;span style=&#34;color:#66d9ef&#34;&gt;public&lt;/span&gt; torch&lt;span style=&#34;color:#f92672&#34;&gt;::&lt;/span&gt;nn&lt;span style=&#34;color:#f92672&#34;&gt;::&lt;/span&gt;Module {
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    NetImpl() {
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        &lt;span style=&#34;color:#75715e&#34;&gt;// Initialize the network
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#75715e&#34;&gt;&lt;/span&gt;        &lt;span style=&#34;color:#75715e&#34;&gt;// On how to pass strides and padding: https://github.com/pytorch/pytorch/issues/12649#issuecomment-430156160
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#75715e&#34;&gt;&lt;/span&gt;        conv1_1 &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; register_module(&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;conv1_1&amp;#34;&lt;/span&gt;, torch&lt;span style=&#34;color:#f92672&#34;&gt;::&lt;/span&gt;nn&lt;span style=&#34;color:#f92672&#34;&gt;::&lt;/span&gt;Conv2d(torch&lt;span style=&#34;color:#f92672&#34;&gt;::&lt;/span&gt;nn&lt;span style=&#34;color:#f92672&#34;&gt;::&lt;/span&gt;Conv2dOptions(&lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;10&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;3&lt;/span&gt;).padding(&lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;)));
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        conv1_2 &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; register_module(&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;conv1_2&amp;#34;&lt;/span&gt;, torch&lt;span style=&#34;color:#f92672&#34;&gt;::&lt;/span&gt;nn&lt;span style=&#34;color:#f92672&#34;&gt;::&lt;/span&gt;Conv2d(torch&lt;span style=&#34;color:#f92672&#34;&gt;::&lt;/span&gt;nn&lt;span style=&#34;color:#f92672&#34;&gt;::&lt;/span&gt;Conv2dOptions(&lt;span style=&#34;color:#ae81ff&#34;&gt;10&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;20&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;3&lt;/span&gt;).padding(&lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;)));
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        &lt;span style=&#34;color:#75715e&#34;&gt;// Insert pool layer
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#75715e&#34;&gt;&lt;/span&gt;        conv2_1 &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; register_module(&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;conv2_1&amp;#34;&lt;/span&gt;, torch&lt;span style=&#34;color:#f92672&#34;&gt;::&lt;/span&gt;nn&lt;span style=&#34;color:#f92672&#34;&gt;::&lt;/span&gt;Conv2d(torch&lt;span style=&#34;color:#f92672&#34;&gt;::&lt;/span&gt;nn&lt;span style=&#34;color:#f92672&#34;&gt;::&lt;/span&gt;Conv2dOptions(&lt;span style=&#34;color:#ae81ff&#34;&gt;20&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;30&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;3&lt;/span&gt;).padding(&lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;)));
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        conv2_2 &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; register_module(&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;conv2_2&amp;#34;&lt;/span&gt;, torch&lt;span style=&#34;color:#f92672&#34;&gt;::&lt;/span&gt;nn&lt;span style=&#34;color:#f92672&#34;&gt;::&lt;/span&gt;Conv2d(torch&lt;span style=&#34;color:#f92672&#34;&gt;::&lt;/span&gt;nn&lt;span style=&#34;color:#f92672&#34;&gt;::&lt;/span&gt;Conv2dOptions(&lt;span style=&#34;color:#ae81ff&#34;&gt;30&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;40&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;3&lt;/span&gt;).padding(&lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;)));
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        &lt;span style=&#34;color:#75715e&#34;&gt;// Insert pool layer
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#75715e&#34;&gt;&lt;/span&gt;        conv3_1 &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; register_module(&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;conv3_1&amp;#34;&lt;/span&gt;, torch&lt;span style=&#34;color:#f92672&#34;&gt;::&lt;/span&gt;nn&lt;span style=&#34;color:#f92672&#34;&gt;::&lt;/span&gt;Conv2d(torch&lt;span style=&#34;color:#f92672&#34;&gt;::&lt;/span&gt;nn&lt;span style=&#34;color:#f92672&#34;&gt;::&lt;/span&gt;Conv2dOptions(&lt;span style=&#34;color:#ae81ff&#34;&gt;40&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;50&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;3&lt;/span&gt;).padding(&lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;)));
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        conv3_2 &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; register_module(&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;conv3_2&amp;#34;&lt;/span&gt;, torch&lt;span style=&#34;color:#f92672&#34;&gt;::&lt;/span&gt;nn&lt;span style=&#34;color:#f92672&#34;&gt;::&lt;/span&gt;Conv2d(torch&lt;span style=&#34;color:#f92672&#34;&gt;::&lt;/span&gt;nn&lt;span style=&#34;color:#f92672&#34;&gt;::&lt;/span&gt;Conv2dOptions(&lt;span style=&#34;color:#ae81ff&#34;&gt;50&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;60&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;3&lt;/span&gt;).padding(&lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;)));
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        conv3_3 &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; register_module(&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;conv3_3&amp;#34;&lt;/span&gt;, torch&lt;span style=&#34;color:#f92672&#34;&gt;::&lt;/span&gt;nn&lt;span style=&#34;color:#f92672&#34;&gt;::&lt;/span&gt;Conv2d(torch&lt;span style=&#34;color:#f92672&#34;&gt;::&lt;/span&gt;nn&lt;span style=&#34;color:#f92672&#34;&gt;::&lt;/span&gt;Conv2dOptions(&lt;span style=&#34;color:#ae81ff&#34;&gt;60&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;70&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;3&lt;/span&gt;).padding(&lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;)));
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        &lt;span style=&#34;color:#75715e&#34;&gt;// Insert pool layer
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#75715e&#34;&gt;&lt;/span&gt;        conv4_1 &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; register_module(&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;conv4_1&amp;#34;&lt;/span&gt;, torch&lt;span style=&#34;color:#f92672&#34;&gt;::&lt;/span&gt;nn&lt;span style=&#34;color:#f92672&#34;&gt;::&lt;/span&gt;Conv2d(torch&lt;span style=&#34;color:#f92672&#34;&gt;::&lt;/span&gt;nn&lt;span style=&#34;color:#f92672&#34;&gt;::&lt;/span&gt;Conv2dOptions(&lt;span style=&#34;color:#ae81ff&#34;&gt;70&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;80&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;3&lt;/span&gt;).padding(&lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;)));
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        conv4_2 &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; register_module(&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;conv4_2&amp;#34;&lt;/span&gt;, torch&lt;span style=&#34;color:#f92672&#34;&gt;::&lt;/span&gt;nn&lt;span style=&#34;color:#f92672&#34;&gt;::&lt;/span&gt;Conv2d(torch&lt;span style=&#34;color:#f92672&#34;&gt;::&lt;/span&gt;nn&lt;span style=&#34;color:#f92672&#34;&gt;::&lt;/span&gt;Conv2dOptions(&lt;span style=&#34;color:#ae81ff&#34;&gt;80&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;90&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;3&lt;/span&gt;).padding(&lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;)));
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        conv4_3 &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; register_module(&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;conv4_3&amp;#34;&lt;/span&gt;, torch&lt;span style=&#34;color:#f92672&#34;&gt;::&lt;/span&gt;nn&lt;span style=&#34;color:#f92672&#34;&gt;::&lt;/span&gt;Conv2d(torch&lt;span style=&#34;color:#f92672&#34;&gt;::&lt;/span&gt;nn&lt;span style=&#34;color:#f92672&#34;&gt;::&lt;/span&gt;Conv2dOptions(&lt;span style=&#34;color:#ae81ff&#34;&gt;90&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;100&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;3&lt;/span&gt;).padding(&lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;)));
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        &lt;span style=&#34;color:#75715e&#34;&gt;// Insert pool layer
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#75715e&#34;&gt;&lt;/span&gt;        conv5_1 &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; register_module(&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;conv5_1&amp;#34;&lt;/span&gt;, torch&lt;span style=&#34;color:#f92672&#34;&gt;::&lt;/span&gt;nn&lt;span style=&#34;color:#f92672&#34;&gt;::&lt;/span&gt;Conv2d(torch&lt;span style=&#34;color:#f92672&#34;&gt;::&lt;/span&gt;nn&lt;span style=&#34;color:#f92672&#34;&gt;::&lt;/span&gt;Conv2dOptions(&lt;span style=&#34;color:#ae81ff&#34;&gt;100&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;110&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;3&lt;/span&gt;).padding(&lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;)));
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        conv5_2 &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; register_module(&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;conv5_2&amp;#34;&lt;/span&gt;, torch&lt;span style=&#34;color:#f92672&#34;&gt;::&lt;/span&gt;nn&lt;span style=&#34;color:#f92672&#34;&gt;::&lt;/span&gt;Conv2d(torch&lt;span style=&#34;color:#f92672&#34;&gt;::&lt;/span&gt;nn&lt;span style=&#34;color:#f92672&#34;&gt;::&lt;/span&gt;Conv2dOptions(&lt;span style=&#34;color:#ae81ff&#34;&gt;110&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;120&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;3&lt;/span&gt;).padding(&lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;)));
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        conv5_3 &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; register_module(&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;conv5_3&amp;#34;&lt;/span&gt;, torch&lt;span style=&#34;color:#f92672&#34;&gt;::&lt;/span&gt;nn&lt;span style=&#34;color:#f92672&#34;&gt;::&lt;/span&gt;Conv2d(torch&lt;span style=&#34;color:#f92672&#34;&gt;::&lt;/span&gt;nn&lt;span style=&#34;color:#f92672&#34;&gt;::&lt;/span&gt;Conv2dOptions(&lt;span style=&#34;color:#ae81ff&#34;&gt;120&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;130&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;3&lt;/span&gt;).padding(&lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;)));
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        &lt;span style=&#34;color:#75715e&#34;&gt;// Insert pool layer
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#75715e&#34;&gt;&lt;/span&gt;        fc1 &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; register_module(&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;fc1&amp;#34;&lt;/span&gt;, torch&lt;span style=&#34;color:#f92672&#34;&gt;::&lt;/span&gt;nn&lt;span style=&#34;color:#f92672&#34;&gt;::&lt;/span&gt;Linear(&lt;span style=&#34;color:#ae81ff&#34;&gt;130&lt;/span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;*&lt;/span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;6&lt;/span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;*&lt;/span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;6&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;2000&lt;/span&gt;));
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        fc2 &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; register_module(&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;fc2&amp;#34;&lt;/span&gt;, torch&lt;span style=&#34;color:#f92672&#34;&gt;::&lt;/span&gt;nn&lt;span style=&#34;color:#f92672&#34;&gt;::&lt;/span&gt;Linear(&lt;span style=&#34;color:#ae81ff&#34;&gt;2000&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;1000&lt;/span&gt;));
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        fc3 &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; register_module(&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;fc3&amp;#34;&lt;/span&gt;, torch&lt;span style=&#34;color:#f92672&#34;&gt;::&lt;/span&gt;nn&lt;span style=&#34;color:#f92672&#34;&gt;::&lt;/span&gt;Linear(&lt;span style=&#34;color:#ae81ff&#34;&gt;1000&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;100&lt;/span&gt;));
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        fc4 &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; register_module(&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;fc4&amp;#34;&lt;/span&gt;, torch&lt;span style=&#34;color:#f92672&#34;&gt;::&lt;/span&gt;nn&lt;span style=&#34;color:#f92672&#34;&gt;::&lt;/span&gt;Linear(&lt;span style=&#34;color:#ae81ff&#34;&gt;100&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;2&lt;/span&gt;));
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    }
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#75715e&#34;&gt;// Implement Algorithm
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#75715e&#34;&gt;&lt;/span&gt;    torch&lt;span style=&#34;color:#f92672&#34;&gt;::&lt;/span&gt;Tensor forward(torch&lt;span style=&#34;color:#f92672&#34;&gt;::&lt;/span&gt;Tensor x) {
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        x &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; torch&lt;span style=&#34;color:#f92672&#34;&gt;::&lt;/span&gt;relu(conv1_1&lt;span style=&#34;color:#f92672&#34;&gt;-&amp;gt;&lt;/span&gt;forward(x));
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        x &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; torch&lt;span style=&#34;color:#f92672&#34;&gt;::&lt;/span&gt;relu(conv1_2&lt;span style=&#34;color:#f92672&#34;&gt;-&amp;gt;&lt;/span&gt;forward(x));
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        x &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; torch&lt;span style=&#34;color:#f92672&#34;&gt;::&lt;/span&gt;max_pool2d(x, &lt;span style=&#34;color:#ae81ff&#34;&gt;2&lt;/span&gt;);
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        x &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; torch&lt;span style=&#34;color:#f92672&#34;&gt;::&lt;/span&gt;relu(conv2_1&lt;span style=&#34;color:#f92672&#34;&gt;-&amp;gt;&lt;/span&gt;forward(x));
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        x &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; torch&lt;span style=&#34;color:#f92672&#34;&gt;::&lt;/span&gt;relu(conv2_2&lt;span style=&#34;color:#f92672&#34;&gt;-&amp;gt;&lt;/span&gt;forward(x));
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        x &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; torch&lt;span style=&#34;color:#f92672&#34;&gt;::&lt;/span&gt;max_pool2d(x, &lt;span style=&#34;color:#ae81ff&#34;&gt;2&lt;/span&gt;);
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        x &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; torch&lt;span style=&#34;color:#f92672&#34;&gt;::&lt;/span&gt;relu(conv3_1&lt;span style=&#34;color:#f92672&#34;&gt;-&amp;gt;&lt;/span&gt;forward(x));
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        x &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; torch&lt;span style=&#34;color:#f92672&#34;&gt;::&lt;/span&gt;relu(conv3_2&lt;span style=&#34;color:#f92672&#34;&gt;-&amp;gt;&lt;/span&gt;forward(x));
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        x &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; torch&lt;span style=&#34;color:#f92672&#34;&gt;::&lt;/span&gt;relu(conv3_3&lt;span style=&#34;color:#f92672&#34;&gt;-&amp;gt;&lt;/span&gt;forward(x));
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        x &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; torch&lt;span style=&#34;color:#f92672&#34;&gt;::&lt;/span&gt;max_pool2d(x, &lt;span style=&#34;color:#ae81ff&#34;&gt;2&lt;/span&gt;);
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        x &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; torch&lt;span style=&#34;color:#f92672&#34;&gt;::&lt;/span&gt;relu(conv4_1&lt;span style=&#34;color:#f92672&#34;&gt;-&amp;gt;&lt;/span&gt;forward(x));
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        x &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; torch&lt;span style=&#34;color:#f92672&#34;&gt;::&lt;/span&gt;relu(conv4_2&lt;span style=&#34;color:#f92672&#34;&gt;-&amp;gt;&lt;/span&gt;forward(x));
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        x &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; torch&lt;span style=&#34;color:#f92672&#34;&gt;::&lt;/span&gt;relu(conv4_3&lt;span style=&#34;color:#f92672&#34;&gt;-&amp;gt;&lt;/span&gt;forward(x));
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        x &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; torch&lt;span style=&#34;color:#f92672&#34;&gt;::&lt;/span&gt;max_pool2d(x, &lt;span style=&#34;color:#ae81ff&#34;&gt;2&lt;/span&gt;);
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        x &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; torch&lt;span style=&#34;color:#f92672&#34;&gt;::&lt;/span&gt;relu(conv5_1&lt;span style=&#34;color:#f92672&#34;&gt;-&amp;gt;&lt;/span&gt;forward(x));
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        x &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; torch&lt;span style=&#34;color:#f92672&#34;&gt;::&lt;/span&gt;relu(conv5_2&lt;span style=&#34;color:#f92672&#34;&gt;-&amp;gt;&lt;/span&gt;forward(x));
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        x &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; torch&lt;span style=&#34;color:#f92672&#34;&gt;::&lt;/span&gt;relu(conv5_3&lt;span style=&#34;color:#f92672&#34;&gt;-&amp;gt;&lt;/span&gt;forward(x));
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        x &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; torch&lt;span style=&#34;color:#f92672&#34;&gt;::&lt;/span&gt;max_pool2d(x, &lt;span style=&#34;color:#ae81ff&#34;&gt;2&lt;/span&gt;);
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        x &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; x.view({&lt;span style=&#34;color:#f92672&#34;&gt;-&lt;/span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;130&lt;/span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;*&lt;/span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;6&lt;/span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;*&lt;/span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;6&lt;/span&gt;});
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        x &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; torch&lt;span style=&#34;color:#f92672&#34;&gt;::&lt;/span&gt;relu(fc1&lt;span style=&#34;color:#f92672&#34;&gt;-&amp;gt;&lt;/span&gt;forward(x));
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        x &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; torch&lt;span style=&#34;color:#f92672&#34;&gt;::&lt;/span&gt;relu(fc2&lt;span style=&#34;color:#f92672&#34;&gt;-&amp;gt;&lt;/span&gt;forward(x));
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        x &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; torch&lt;span style=&#34;color:#f92672&#34;&gt;::&lt;/span&gt;relu(fc3&lt;span style=&#34;color:#f92672&#34;&gt;-&amp;gt;&lt;/span&gt;forward(x));
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        x &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; fc4&lt;span style=&#34;color:#f92672&#34;&gt;-&amp;gt;&lt;/span&gt;forward(x);
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        &lt;span style=&#34;color:#66d9ef&#34;&gt;return&lt;/span&gt; torch&lt;span style=&#34;color:#f92672&#34;&gt;::&lt;/span&gt;log_softmax(x, &lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;);
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    }
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#75715e&#34;&gt;// Declare layers
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#75715e&#34;&gt;&lt;/span&gt;    torch&lt;span style=&#34;color:#f92672&#34;&gt;::&lt;/span&gt;nn&lt;span style=&#34;color:#f92672&#34;&gt;::&lt;/span&gt;Conv2d conv1_1{&lt;span style=&#34;color:#66d9ef&#34;&gt;nullptr&lt;/span&gt;};
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    torch&lt;span style=&#34;color:#f92672&#34;&gt;::&lt;/span&gt;nn&lt;span style=&#34;color:#f92672&#34;&gt;::&lt;/span&gt;Conv2d conv1_2{&lt;span style=&#34;color:#66d9ef&#34;&gt;nullptr&lt;/span&gt;};
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    torch&lt;span style=&#34;color:#f92672&#34;&gt;::&lt;/span&gt;nn&lt;span style=&#34;color:#f92672&#34;&gt;::&lt;/span&gt;Conv2d conv2_1{&lt;span style=&#34;color:#66d9ef&#34;&gt;nullptr&lt;/span&gt;};
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    torch&lt;span style=&#34;color:#f92672&#34;&gt;::&lt;/span&gt;nn&lt;span style=&#34;color:#f92672&#34;&gt;::&lt;/span&gt;Conv2d conv2_2{&lt;span style=&#34;color:#66d9ef&#34;&gt;nullptr&lt;/span&gt;};
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    torch&lt;span style=&#34;color:#f92672&#34;&gt;::&lt;/span&gt;nn&lt;span style=&#34;color:#f92672&#34;&gt;::&lt;/span&gt;Conv2d conv3_1{&lt;span style=&#34;color:#66d9ef&#34;&gt;nullptr&lt;/span&gt;};
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    torch&lt;span style=&#34;color:#f92672&#34;&gt;::&lt;/span&gt;nn&lt;span style=&#34;color:#f92672&#34;&gt;::&lt;/span&gt;Conv2d conv3_2{&lt;span style=&#34;color:#66d9ef&#34;&gt;nullptr&lt;/span&gt;};
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    torch&lt;span style=&#34;color:#f92672&#34;&gt;::&lt;/span&gt;nn&lt;span style=&#34;color:#f92672&#34;&gt;::&lt;/span&gt;Conv2d conv3_3{&lt;span style=&#34;color:#66d9ef&#34;&gt;nullptr&lt;/span&gt;};
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    torch&lt;span style=&#34;color:#f92672&#34;&gt;::&lt;/span&gt;nn&lt;span style=&#34;color:#f92672&#34;&gt;::&lt;/span&gt;Conv2d conv4_1{&lt;span style=&#34;color:#66d9ef&#34;&gt;nullptr&lt;/span&gt;};
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    torch&lt;span style=&#34;color:#f92672&#34;&gt;::&lt;/span&gt;nn&lt;span style=&#34;color:#f92672&#34;&gt;::&lt;/span&gt;Conv2d conv4_2{&lt;span style=&#34;color:#66d9ef&#34;&gt;nullptr&lt;/span&gt;};
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    torch&lt;span style=&#34;color:#f92672&#34;&gt;::&lt;/span&gt;nn&lt;span style=&#34;color:#f92672&#34;&gt;::&lt;/span&gt;Conv2d conv4_3{&lt;span style=&#34;color:#66d9ef&#34;&gt;nullptr&lt;/span&gt;};
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    torch&lt;span style=&#34;color:#f92672&#34;&gt;::&lt;/span&gt;nn&lt;span style=&#34;color:#f92672&#34;&gt;::&lt;/span&gt;Conv2d conv5_1{&lt;span style=&#34;color:#66d9ef&#34;&gt;nullptr&lt;/span&gt;};
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    torch&lt;span style=&#34;color:#f92672&#34;&gt;::&lt;/span&gt;nn&lt;span style=&#34;color:#f92672&#34;&gt;::&lt;/span&gt;Conv2d conv5_2{&lt;span style=&#34;color:#66d9ef&#34;&gt;nullptr&lt;/span&gt;};
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    torch&lt;span style=&#34;color:#f92672&#34;&gt;::&lt;/span&gt;nn&lt;span style=&#34;color:#f92672&#34;&gt;::&lt;/span&gt;Conv2d conv5_3{&lt;span style=&#34;color:#66d9ef&#34;&gt;nullptr&lt;/span&gt;};
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    torch&lt;span style=&#34;color:#f92672&#34;&gt;::&lt;/span&gt;nn&lt;span style=&#34;color:#f92672&#34;&gt;::&lt;/span&gt;Linear fc1{&lt;span style=&#34;color:#66d9ef&#34;&gt;nullptr&lt;/span&gt;}, fc2{&lt;span style=&#34;color:#66d9ef&#34;&gt;nullptr&lt;/span&gt;}, fc3{&lt;span style=&#34;color:#66d9ef&#34;&gt;nullptr&lt;/span&gt;}, fc4{&lt;span style=&#34;color:#66d9ef&#34;&gt;nullptr&lt;/span&gt;};
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;};
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;As it&amp;rsquo;s visible, we had 13 Convolutional Layers, 5 Max Pooling Layers and 4 Fully Connected Layers. This leads of a lot of parameters to be trained.&lt;/p&gt;
&lt;p&gt;Therefore, our new network for experimentation purposes will be:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-cpp&#34; data-lang=&#34;cpp&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#66d9ef&#34;&gt;struct&lt;/span&gt; &lt;span style=&#34;color:#a6e22e&#34;&gt;NetworkImpl&lt;/span&gt; &lt;span style=&#34;color:#f92672&#34;&gt;:&lt;/span&gt; &lt;span style=&#34;color:#66d9ef&#34;&gt;public&lt;/span&gt; torch&lt;span style=&#34;color:#f92672&#34;&gt;::&lt;/span&gt;nn&lt;span style=&#34;color:#f92672&#34;&gt;::&lt;/span&gt;Module {
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;	NetImpl(&lt;span style=&#34;color:#66d9ef&#34;&gt;int64_t&lt;/span&gt; channels, &lt;span style=&#34;color:#66d9ef&#34;&gt;int64_t&lt;/span&gt; height, &lt;span style=&#34;color:#66d9ef&#34;&gt;int64_t&lt;/span&gt; width) {
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;		conv1_1 &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; register_module(&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;conv1&amp;#34;&lt;/span&gt;, torch&lt;span style=&#34;color:#f92672&#34;&gt;::&lt;/span&gt;nn&lt;span style=&#34;color:#f92672&#34;&gt;::&lt;/span&gt;Conv2d(torch&lt;span style=&#34;color:#f92672&#34;&gt;::&lt;/span&gt;nn&lt;span style=&#34;color:#f92672&#34;&gt;::&lt;/span&gt;Conv2dOptions(&lt;span style=&#34;color:#ae81ff&#34;&gt;3&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;50&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;5&lt;/span&gt;).stride(&lt;span style=&#34;color:#ae81ff&#34;&gt;2&lt;/span&gt;)));
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;		conv2_1 &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; register_module(&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;conv2&amp;#34;&lt;/span&gt;, torch&lt;span style=&#34;color:#f92672&#34;&gt;::&lt;/span&gt;nn&lt;span style=&#34;color:#f92672&#34;&gt;::&lt;/span&gt;Conv2d(torch&lt;span style=&#34;color:#f92672&#34;&gt;::&lt;/span&gt;nn&lt;span style=&#34;color:#f92672&#34;&gt;::&lt;/span&gt;Conv2dOptions(&lt;span style=&#34;color:#ae81ff&#34;&gt;50&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;100&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;7&lt;/span&gt;).stride(&lt;span style=&#34;color:#ae81ff&#34;&gt;2&lt;/span&gt;)));
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;		&lt;span style=&#34;color:#75715e&#34;&gt;// Used to find the output size till previous convolutional layers
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#75715e&#34;&gt;&lt;/span&gt;		n(get_output_shape(channels, height, width));
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;		fc1 &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; register_module(&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;fc1&amp;#34;&lt;/span&gt;, torch&lt;span style=&#34;color:#f92672&#34;&gt;::&lt;/span&gt;nn&lt;span style=&#34;color:#f92672&#34;&gt;::&lt;/span&gt;Linear(n, &lt;span style=&#34;color:#ae81ff&#34;&gt;120&lt;/span&gt;));
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;		fc2 &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; register_module(&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;fc2&amp;#34;&lt;/span&gt;, torch&lt;span style=&#34;color:#f92672&#34;&gt;::&lt;/span&gt;nn&lt;span style=&#34;color:#f92672&#34;&gt;::&lt;/span&gt;Linear(&lt;span style=&#34;color:#ae81ff&#34;&gt;120&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;100&lt;/span&gt;));
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;		fc3 &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; register_module(&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;fc3&amp;#34;&lt;/span&gt;, torch&lt;span style=&#34;color:#f92672&#34;&gt;::&lt;/span&gt;nn&lt;span style=&#34;color:#f92672&#34;&gt;::&lt;/span&gt;Linear(&lt;span style=&#34;color:#ae81ff&#34;&gt;100&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;2&lt;/span&gt;));
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;		register_module(&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;conv1&amp;#34;&lt;/span&gt;, conv1);
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;		register_module(&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;conv2&amp;#34;&lt;/span&gt;, conv2);
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;		register_module(&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;fc1&amp;#34;&lt;/span&gt;, fc1);
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;		register_module(&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;fc2&amp;#34;&lt;/span&gt;, fc2);
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;		register_module(&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;fc3&amp;#34;&lt;/span&gt;, fc3);
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    }
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#75715e&#34;&gt;// Implement forward pass of each batch to the network
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#75715e&#34;&gt;&lt;/span&gt;    torch&lt;span style=&#34;color:#f92672&#34;&gt;::&lt;/span&gt;Tensor forward(torch&lt;span style=&#34;color:#f92672&#34;&gt;::&lt;/span&gt;Tensor x) {
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        x &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; torch&lt;span style=&#34;color:#f92672&#34;&gt;::&lt;/span&gt;relu(torch&lt;span style=&#34;color:#f92672&#34;&gt;::&lt;/span&gt;max_pool2d(conv1(x), &lt;span style=&#34;color:#ae81ff&#34;&gt;2&lt;/span&gt;));
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        x &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; torch&lt;span style=&#34;color:#f92672&#34;&gt;::&lt;/span&gt;relu(torch&lt;span style=&#34;color:#f92672&#34;&gt;::&lt;/span&gt;max_pool2d(conv2(x), &lt;span style=&#34;color:#ae81ff&#34;&gt;2&lt;/span&gt;));
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        &lt;span style=&#34;color:#75715e&#34;&gt;// Flatten
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#75715e&#34;&gt;&lt;/span&gt;        x &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; x.view({&lt;span style=&#34;color:#f92672&#34;&gt;-&lt;/span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;, n});
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;		
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;		x &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; torch&lt;span style=&#34;color:#f92672&#34;&gt;::&lt;/span&gt;relu(fc1(x));
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        x &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; torch&lt;span style=&#34;color:#f92672&#34;&gt;::&lt;/span&gt;relu(fc2(x));
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        x &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; torch&lt;span style=&#34;color:#f92672&#34;&gt;::&lt;/span&gt;log_softmax(fc3(x), &lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;);
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        &lt;span style=&#34;color:#66d9ef&#34;&gt;return&lt;/span&gt; x;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    };
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#75715e&#34;&gt;// Function to calculate output size of input tensor after Convolutional layers
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#75715e&#34;&gt;&lt;/span&gt;    &lt;span style=&#34;color:#66d9ef&#34;&gt;int64_t&lt;/span&gt; &lt;span style=&#34;color:#a6e22e&#34;&gt;get_output_shape&lt;/span&gt;(&lt;span style=&#34;color:#66d9ef&#34;&gt;int64_t&lt;/span&gt; channels, &lt;span style=&#34;color:#66d9ef&#34;&gt;int64_t&lt;/span&gt; height, &lt;span style=&#34;color:#66d9ef&#34;&gt;int64_t&lt;/span&gt; width) {
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    	&lt;span style=&#34;color:#75715e&#34;&gt;// Initialize a Tensor with zeros of input shape
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#75715e&#34;&gt;&lt;/span&gt;    	torch&lt;span style=&#34;color:#f92672&#34;&gt;::&lt;/span&gt;Tensor x_sample &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; torch&lt;span style=&#34;color:#f92672&#34;&gt;::&lt;/span&gt;zeros({&lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;, channels, height, width});
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    	x_sample &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; torch&lt;span style=&#34;color:#f92672&#34;&gt;::&lt;/span&gt;max_pool2d(conv1(x_sample), &lt;span style=&#34;color:#ae81ff&#34;&gt;2&lt;/span&gt;);
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        x_sample &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; torch&lt;span style=&#34;color:#f92672&#34;&gt;::&lt;/span&gt;max_pool2d(conv2(x_sample), &lt;span style=&#34;color:#ae81ff&#34;&gt;2&lt;/span&gt;);
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        &lt;span style=&#34;color:#75715e&#34;&gt;// Return batch_size (here, 1) * channels * height * width of x_sample
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#75715e&#34;&gt;&lt;/span&gt;        &lt;span style=&#34;color:#66d9ef&#34;&gt;return&lt;/span&gt; x_sample.numel();
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    }
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;};
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;In our new network, we use 2 Convolutional Layers with Max Pooling and ReLU Activation, and 3 Fully Connected Layers. This, as we mentioned above, reduces the number of parameters for training.&lt;/p&gt;
&lt;p&gt;Let us train our network on the dataset now.&lt;/p&gt;
&lt;h2 id=&#34;training&#34;&gt;Training&lt;/h2&gt;
&lt;p&gt;Let&amp;rsquo;s discuss steps of training a CNN on our dataset:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Set network to training mode using &lt;code&gt;net-&amp;gt;train()&lt;/code&gt;.&lt;/li&gt;
&lt;li&gt;Iterate through every batch of our data loader:
&lt;ol&gt;
&lt;li&gt;Extract data and labels using:
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-cpp&#34; data-lang=&#34;cpp&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#66d9ef&#34;&gt;auto&lt;/span&gt; data &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; batch.data;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#66d9ef&#34;&gt;auto&lt;/span&gt; target &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; batch.target.squeeze();
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;``&lt;/li&gt;
&lt;li&gt;Clear gradients of optimizer: &lt;code&gt;optimizer.zero_grad()&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;Forward pass the batch of data to the network: &lt;code&gt;auto output = net-&amp;gt;forward(data);&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;Calculate Negative Log Likelihood loss (since we use &lt;code&gt;log_softmax()&lt;/code&gt; layer at the end): &lt;code&gt;auto loss = torch::nll_loss(output, target);&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;Backpropagate Loss: &lt;code&gt;auto loss = loss.backward()&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;Update the weights: &lt;code&gt;optimizer.step();&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;Calculate Training Accuracy and Mean Squared Error:
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-cpp&#34; data-lang=&#34;cpp&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#66d9ef&#34;&gt;auto&lt;/span&gt; acc &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; output.argmax(&lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;).eq(target).sum();
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;mse &lt;span style=&#34;color:#f92672&#34;&gt;+=&lt;/span&gt; loss.&lt;span style=&#34;color:#66d9ef&#34;&gt;template&lt;/span&gt; item&lt;span style=&#34;color:#f92672&#34;&gt;&amp;lt;&lt;/span&gt;&lt;span style=&#34;color:#66d9ef&#34;&gt;float&lt;/span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;&amp;gt;&lt;/span&gt;();
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;``&lt;/li&gt;
&lt;/ol&gt;
&lt;/li&gt;
&lt;li&gt;Save the model using &lt;code&gt;torch::save(net, &amp;quot;model.pt&amp;quot;);&lt;/code&gt;.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;Let&amp;rsquo;s try to convert the above steps to a &lt;code&gt;train()&lt;/code&gt; function.&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-cpp&#34; data-lang=&#34;cpp&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#66d9ef&#34;&gt;void&lt;/span&gt; &lt;span style=&#34;color:#a6e22e&#34;&gt;train&lt;/span&gt;(ConvNet&lt;span style=&#34;color:#f92672&#34;&gt;&amp;amp;&lt;/span&gt; net, DataLoader&lt;span style=&#34;color:#f92672&#34;&gt;&amp;amp;&lt;/span&gt; data_loader, torch&lt;span style=&#34;color:#f92672&#34;&gt;::&lt;/span&gt;optim&lt;span style=&#34;color:#f92672&#34;&gt;::&lt;/span&gt;Optimizer&lt;span style=&#34;color:#f92672&#34;&gt;&amp;amp;&lt;/span&gt; optimizer, size_t dataset_size, &lt;span style=&#34;color:#66d9ef&#34;&gt;int&lt;/span&gt; epoch) {
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;	&lt;span style=&#34;color:#75715e&#34;&gt;/*
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#75715e&#34;&gt;	This function trains the network on our data loader using optimizer for given number of epochs.
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#75715e&#34;&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#75715e&#34;&gt;	Parameters
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#75715e&#34;&gt;	==================
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#75715e&#34;&gt;	ConvNet&amp;amp; net: Network struct
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#75715e&#34;&gt;	DataLoader&amp;amp; data_loader: Training data loader
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#75715e&#34;&gt;	torch::optim::Optimizer&amp;amp; optimizer: Optimizer like Adam, SGD etc.
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#75715e&#34;&gt;	size_t dataset_size: Size of training dataset
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#75715e&#34;&gt;	int epoch: Number of epoch for training
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#75715e&#34;&gt;	*/&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;	net&lt;span style=&#34;color:#f92672&#34;&gt;-&amp;gt;&lt;/span&gt;train();
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    size_t batch_index &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; &lt;span style=&#34;color:#ae81ff&#34;&gt;0&lt;/span&gt;;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#66d9ef&#34;&gt;float&lt;/span&gt; mse &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; &lt;span style=&#34;color:#ae81ff&#34;&gt;0&lt;/span&gt;;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#66d9ef&#34;&gt;float&lt;/span&gt; Acc &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; &lt;span style=&#34;color:#ae81ff&#34;&gt;0.0&lt;/span&gt;;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#66d9ef&#34;&gt;for&lt;/span&gt;(&lt;span style=&#34;color:#66d9ef&#34;&gt;auto&lt;/span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;&amp;amp;&lt;/span&gt; batch: &lt;span style=&#34;color:#f92672&#34;&gt;*&lt;/span&gt;data_loader) {
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;      &lt;span style=&#34;color:#66d9ef&#34;&gt;auto&lt;/span&gt; data &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; batch.data;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;      &lt;span style=&#34;color:#66d9ef&#34;&gt;auto&lt;/span&gt; target &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; batch.target.squeeze();
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;      
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;      &lt;span style=&#34;color:#75715e&#34;&gt;// Should be of length: batch_size
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#75715e&#34;&gt;&lt;/span&gt;      data &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; data.to(torch&lt;span style=&#34;color:#f92672&#34;&gt;::&lt;/span&gt;kF32);
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;      target &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; target.to(torch&lt;span style=&#34;color:#f92672&#34;&gt;::&lt;/span&gt;kInt64);
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;      optimizer.zero_grad();
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;      &lt;span style=&#34;color:#66d9ef&#34;&gt;auto&lt;/span&gt; output &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; net&lt;span style=&#34;color:#f92672&#34;&gt;-&amp;gt;&lt;/span&gt;forward(data);
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;      &lt;span style=&#34;color:#66d9ef&#34;&gt;auto&lt;/span&gt; loss &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; torch&lt;span style=&#34;color:#f92672&#34;&gt;::&lt;/span&gt;nll_loss(output, target);
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;      loss.backward();
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;      optimizer.step();
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;      &lt;span style=&#34;color:#66d9ef&#34;&gt;auto&lt;/span&gt; acc &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; output.argmax(&lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;).eq(target).sum();
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;      Acc &lt;span style=&#34;color:#f92672&#34;&gt;+=&lt;/span&gt; acc.&lt;span style=&#34;color:#66d9ef&#34;&gt;template&lt;/span&gt; item&lt;span style=&#34;color:#f92672&#34;&gt;&amp;lt;&lt;/span&gt;&lt;span style=&#34;color:#66d9ef&#34;&gt;float&lt;/span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;&amp;gt;&lt;/span&gt;();
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;      mse &lt;span style=&#34;color:#f92672&#34;&gt;+=&lt;/span&gt; loss.&lt;span style=&#34;color:#66d9ef&#34;&gt;template&lt;/span&gt; item&lt;span style=&#34;color:#f92672&#34;&gt;&amp;lt;&lt;/span&gt;&lt;span style=&#34;color:#66d9ef&#34;&gt;float&lt;/span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;&amp;gt;&lt;/span&gt;();
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;      batch_index &lt;span style=&#34;color:#f92672&#34;&gt;+=&lt;/span&gt; &lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;      count&lt;span style=&#34;color:#f92672&#34;&gt;++&lt;/span&gt;;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    }
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    mse &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; mse&lt;span style=&#34;color:#f92672&#34;&gt;/&lt;/span&gt;&lt;span style=&#34;color:#66d9ef&#34;&gt;float&lt;/span&gt;(batch_index); &lt;span style=&#34;color:#75715e&#34;&gt;// Take mean of loss
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#75715e&#34;&gt;&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    std&lt;span style=&#34;color:#f92672&#34;&gt;::&lt;/span&gt;cout &lt;span style=&#34;color:#f92672&#34;&gt;&amp;lt;&amp;lt;&lt;/span&gt; &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;Epoch: &amp;#34;&lt;/span&gt; &lt;span style=&#34;color:#f92672&#34;&gt;&amp;lt;&amp;lt;&lt;/span&gt; epoch &lt;span style=&#34;color:#f92672&#34;&gt;&amp;lt;&amp;lt;&lt;/span&gt; &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;, &amp;#34;&lt;/span&gt; &lt;span style=&#34;color:#f92672&#34;&gt;&amp;lt;&amp;lt;&lt;/span&gt; &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;Accuracy: &amp;#34;&lt;/span&gt; &lt;span style=&#34;color:#f92672&#34;&gt;&amp;lt;&amp;lt;&lt;/span&gt; Acc&lt;span style=&#34;color:#f92672&#34;&gt;/&lt;/span&gt;dataset_size &lt;span style=&#34;color:#f92672&#34;&gt;&amp;lt;&amp;lt;&lt;/span&gt; &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;, &amp;#34;&lt;/span&gt; &lt;span style=&#34;color:#f92672&#34;&gt;&amp;lt;&amp;lt;&lt;/span&gt; &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;MSE: &amp;#34;&lt;/span&gt; &lt;span style=&#34;color:#f92672&#34;&gt;&amp;lt;&amp;lt;&lt;/span&gt; mse &lt;span style=&#34;color:#f92672&#34;&gt;&amp;lt;&amp;lt;&lt;/span&gt; std&lt;span style=&#34;color:#f92672&#34;&gt;::&lt;/span&gt;endl;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    torch&lt;span style=&#34;color:#f92672&#34;&gt;::&lt;/span&gt;save(net, &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;best_model_try.pt&amp;#34;&lt;/span&gt;);
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;}
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;Similarly, we also define a Test Function to test our network on the test dataset. The steps include:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Set network to &lt;code&gt;eval&lt;/code&gt; mode: &lt;code&gt;network-&amp;gt;eval()&lt;/code&gt;.&lt;/li&gt;
&lt;li&gt;Iterate through every batch of test data.
&lt;ol&gt;
&lt;li&gt;Extract data and labels.&lt;/li&gt;
&lt;li&gt;Forward pass the batch of data to the network.&lt;/li&gt;
&lt;li&gt;Calculate NLL Loss and Accuracy&lt;/li&gt;
&lt;/ol&gt;
&lt;/li&gt;
&lt;li&gt;Print test loss and accuracy.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;The code for the &lt;code&gt;test()&lt;/code&gt; function is below:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-cpp&#34; data-lang=&#34;cpp&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#66d9ef&#34;&gt;void&lt;/span&gt; &lt;span style=&#34;color:#a6e22e&#34;&gt;test&lt;/span&gt;(ConvNet&lt;span style=&#34;color:#f92672&#34;&gt;&amp;amp;&lt;/span&gt; network, DataLoader&lt;span style=&#34;color:#f92672&#34;&gt;&amp;amp;&lt;/span&gt; loader, size_t data_size) {
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;  size_t batch_index &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; &lt;span style=&#34;color:#ae81ff&#34;&gt;0&lt;/span&gt;;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;  
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;  network&lt;span style=&#34;color:#f92672&#34;&gt;-&amp;gt;&lt;/span&gt;eval();
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;  
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;  &lt;span style=&#34;color:#66d9ef&#34;&gt;float&lt;/span&gt; Loss &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; &lt;span style=&#34;color:#ae81ff&#34;&gt;0&lt;/span&gt;, Acc &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; &lt;span style=&#34;color:#ae81ff&#34;&gt;0&lt;/span&gt;;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;  &lt;span style=&#34;color:#66d9ef&#34;&gt;for&lt;/span&gt; (&lt;span style=&#34;color:#66d9ef&#34;&gt;const&lt;/span&gt; &lt;span style=&#34;color:#66d9ef&#34;&gt;auto&lt;/span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;&amp;amp;&lt;/span&gt; batch : &lt;span style=&#34;color:#f92672&#34;&gt;*&lt;/span&gt;loader) {
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#66d9ef&#34;&gt;auto&lt;/span&gt; data &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; batch.data;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#66d9ef&#34;&gt;auto&lt;/span&gt; targets &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; batch.target.view({&lt;span style=&#34;color:#f92672&#34;&gt;-&lt;/span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;});
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    data &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; data.to(torch&lt;span style=&#34;color:#f92672&#34;&gt;::&lt;/span&gt;kF32);
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;	targets &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; targets.to(torch&lt;span style=&#34;color:#f92672&#34;&gt;::&lt;/span&gt;kInt64);
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#66d9ef&#34;&gt;auto&lt;/span&gt; output &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; network&lt;span style=&#34;color:#f92672&#34;&gt;-&amp;gt;&lt;/span&gt;forward(data);
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#66d9ef&#34;&gt;auto&lt;/span&gt; loss &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; torch&lt;span style=&#34;color:#f92672&#34;&gt;::&lt;/span&gt;nll_loss(output, targets);
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#66d9ef&#34;&gt;auto&lt;/span&gt; acc &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; output.argmax(&lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;).eq(targets).sum();
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    Loss &lt;span style=&#34;color:#f92672&#34;&gt;+=&lt;/span&gt; loss.&lt;span style=&#34;color:#66d9ef&#34;&gt;template&lt;/span&gt; item&lt;span style=&#34;color:#f92672&#34;&gt;&amp;lt;&lt;/span&gt;&lt;span style=&#34;color:#66d9ef&#34;&gt;float&lt;/span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;&amp;gt;&lt;/span&gt;();
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    Acc &lt;span style=&#34;color:#f92672&#34;&gt;+=&lt;/span&gt; acc.&lt;span style=&#34;color:#66d9ef&#34;&gt;template&lt;/span&gt; item&lt;span style=&#34;color:#f92672&#34;&gt;&amp;lt;&lt;/span&gt;&lt;span style=&#34;color:#66d9ef&#34;&gt;float&lt;/span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;&amp;gt;&lt;/span&gt;();
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;  }
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;  cout &lt;span style=&#34;color:#f92672&#34;&gt;&amp;lt;&amp;lt;&lt;/span&gt; &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;Test Loss: &amp;#34;&lt;/span&gt; &lt;span style=&#34;color:#f92672&#34;&gt;&amp;lt;&amp;lt;&lt;/span&gt; Loss&lt;span style=&#34;color:#f92672&#34;&gt;/&lt;/span&gt;data_size &lt;span style=&#34;color:#f92672&#34;&gt;&amp;lt;&amp;lt;&lt;/span&gt; &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;, Acc:&amp;#34;&lt;/span&gt; &lt;span style=&#34;color:#f92672&#34;&gt;&amp;lt;&amp;lt;&lt;/span&gt; Acc&lt;span style=&#34;color:#f92672&#34;&gt;/&lt;/span&gt;data_size &lt;span style=&#34;color:#f92672&#34;&gt;&amp;lt;&amp;lt;&lt;/span&gt; endl;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;}
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;h2 id=&#34;results&#34;&gt;Results&lt;/h2&gt;
&lt;p&gt;I trained my network on the dataset for 100 Epochs.&lt;/p&gt;
&lt;p&gt;The best training and testing accuracies are given below:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;&lt;strong&gt;Best Training Accuracy:&lt;/strong&gt; 99.82%&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Best Testing Accuracy:&lt;/strong&gt; 82.43%&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;Let&amp;rsquo;s look at some of the correct and wrong predictions.&lt;/p&gt;
&lt;h2 id=&#34;correct-predictions-dogs&#34;&gt;Correct Predictions (Dogs)&lt;/h2&gt;
&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/krshrimali/blog/main/assets/blogs/Correct-Predictions-Dogs.png&#34; alt=&#34;Correct Predictions of Dogs&#34;&gt;&lt;/p&gt;
&lt;h2 id=&#34;correct-predictions-cats&#34;&gt;Correct Predictions (Cats)&lt;/h2&gt;
&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/krshrimali/blog/main/assets/blogs/Correct-Predictions-Cats.png&#34; alt=&#34;Correct Predictions of Cats&#34;&gt;
&lt;img src=&#34;https://krshrimali.github.io/assets/Correct-Predictions-Cats.png&#34; alt=&#34;Correct Predictions of Cats&#34;&gt;&lt;/p&gt;
&lt;h2 id=&#34;wrong-predictions-dogs&#34;&gt;Wrong Predictions (Dogs)&lt;/h2&gt;
&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/krshrimali/blog/main/assets/blogs/Wrong-Predictions-Dog.png&#34; alt=&#34;Wrong Predictions of Dogs&#34;&gt;&lt;/p&gt;
&lt;h2 id=&#34;wrong-predictions-cats&#34;&gt;Wrong Predictions (Cats)&lt;/h2&gt;
&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/krshrimali/blog/main/assets/blogs/Wrong-Predictions-Cats.png&#34; alt=&#34;Wrong Predictions of Cats&#34;&gt;&lt;/p&gt;
&lt;p&gt;Clearly, the network has done well for just a 2 Convolutional and 3 FC Layer Network. It seems to have focused more on the posture of the animal (and body). We can make the network learn more robust features, with a more deeper CNN (like VGG-16). We&amp;rsquo;ll be discussing on using pretrained weights on Dogs vs Cats Dataset using PyTorch C++ API and also Transfer Learning Approach in C++.&lt;/p&gt;
&lt;p&gt;Happy Learning!&lt;/p&gt;

      </description>
    </item>
    
    <item>
      <title>Classifying Dogs vs Cats using PyTorch C&#43;&#43; API: Part-1</title>
      <link>https://krshrimali.github.io/posts/2019/07/classifying-dogs-vs-cats-using-pytorch-c-api-part-1/</link>
      <pubDate>Tue, 23 Jul 2019 00:00:00 +0000</pubDate>
      
      <guid>https://krshrimali.github.io/posts/2019/07/classifying-dogs-vs-cats-using-pytorch-c-api-part-1/</guid>
      <description>
          
            &lt;h4&gt;Classifying Dogs vs Cats using PyTorch C++ API: Part-1&lt;/h4&gt;
          
          &lt;p&gt;Hi Everyone! So excited to be back with another blog in the series of PyTorch C++ Blogs.&lt;/p&gt;
&lt;p&gt;Today, we are going to see a practical example of applying a CNN to a Custom Dataset - Dogs vs Cats. This is going to be a short post of showing results and discussion about hyperparameters and loss functions for the task, as code snippets and explanation has been provided &lt;a href=&#34;https://krshrimali.github.io/Training-Network-Using-Custom-Dataset-PyTorch-CPP/&#34;&gt;here&lt;/a&gt;, &lt;a href=&#34;https://krshrimali.github.io/Custom-Data-Loading-Using-PyTorch-CPP-API/&#34;&gt;here&lt;/a&gt; and &lt;a href=&#34;https://krshrimali.github.io/PyTorch-C++-API/&#34;&gt;here&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/krshrimali/blog/main/assets/cover-images/Cover-Dogs-Cats.jpg&#34; alt=&#34;&#34;&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Note&lt;/strong&gt;: This is Part-1 of the blog on Dogs vs Cats Classification using PyTorch C++ API.&lt;/p&gt;
&lt;h2 id=&#34;dataset-overview&#34;&gt;Dataset Overview&lt;/h2&gt;
&lt;p&gt;Let&amp;rsquo;s have a look at the dataset and it&amp;rsquo;s statistics. &lt;strong&gt;Dogs vs Cats&lt;/strong&gt; dataset has been taken from the famous &lt;a href=&#34;https://www.kaggle.com/c/dogs-vs-cats&#34;&gt;Kaggle Competition&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;The training set contains 25k images combined of dogs and cats. The data can be downloaded from &lt;a href=&#34;https://www.kaggle.com/c/dogs-vs-cats/data&#34;&gt;this&lt;/a&gt; link.&lt;/p&gt;
&lt;p&gt;Let&amp;rsquo;s have a look at sample of the data:&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/krshrimali/krshrimali.github.io/master/assets/dogs-dataset.jpg&#34; alt=&#34;Figure 1: Sample of Dog Images in the Dataset&#34;&gt;&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/krshrimali/krshrimali.github.io/master/assets/cats-dataset.jpg&#34; alt=&#34;Figure 2: Sample of Cat Images in the Dataset&#34;&gt;&lt;/p&gt;
&lt;p&gt;As we can see, the dataset contains images of cats and dogs with multiple instances in the same sample as well.&lt;/p&gt;
&lt;h2 id=&#34;loading-data&#34;&gt;Loading Data&lt;/h2&gt;
&lt;p&gt;Although we have discussed this before (&lt;a href=&#34;https://krshrimali.github.io/Custom-Data-Loading-Using-PyTorch-CPP-API/&#34;&gt;here&lt;/a&gt;), but let&amp;rsquo;s just see how we load the data. Since this is a binary classification problem (2 classes: Dog and Cat), we will have labels as 0 (for a Cat) and 1 (for a Dog). The data comes in two zip files:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;&lt;code&gt;train.zip&lt;/code&gt;: Data to be used for training&lt;/li&gt;
&lt;li&gt;&lt;code&gt;test.zip&lt;/code&gt;: Data to be used for testing&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;The &lt;code&gt;train.zip&lt;/code&gt; file contains files with filenames like &lt;code&gt;&amp;lt;class&amp;gt;.&amp;lt;number&amp;gt;.jpg&lt;/code&gt; where:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;code&gt;class&lt;/code&gt; can be either cat or dog.&lt;/li&gt;
&lt;li&gt;&lt;code&gt;number&lt;/code&gt; represents the count of the sample.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;For example: &lt;code&gt;cat.100.jpg&lt;/code&gt; and &lt;code&gt;dog.1.jpg&lt;/code&gt;. In order to load the data, we will move the cat images to &lt;code&gt;train/cat&lt;/code&gt; folder and dog images to &lt;code&gt;train/dog&lt;/code&gt; folder. You can accomplish this using &lt;code&gt;shutil&lt;/code&gt; module:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#75715e&#34;&gt;# This code moves cat and dog images to train/cat and train/dog folders respectively&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;import&lt;/span&gt; shutil&lt;span style=&#34;color:#f92672&#34;&gt;,&lt;/span&gt; os
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;files &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; os&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;listdir(&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;train/&amp;#39;&lt;/span&gt;)
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;count_cat &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; &lt;span style=&#34;color:#ae81ff&#34;&gt;0&lt;/span&gt; &lt;span style=&#34;color:#75715e&#34;&gt;# Number representing count of the cat image&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;count_dog &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; &lt;span style=&#34;color:#ae81ff&#34;&gt;0&lt;/span&gt; &lt;span style=&#34;color:#75715e&#34;&gt;# Number representing count of the dog image&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#66d9ef&#34;&gt;for&lt;/span&gt; file &lt;span style=&#34;color:#f92672&#34;&gt;in&lt;/span&gt; files:
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;	&lt;span style=&#34;color:#66d9ef&#34;&gt;if&lt;/span&gt;(file&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;startswith(&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;cat&amp;#39;&lt;/span&gt;) &lt;span style=&#34;color:#f92672&#34;&gt;and&lt;/span&gt; file&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;endswith(&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;jpg&amp;#39;&lt;/span&gt;)):
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;		count_cat &lt;span style=&#34;color:#f92672&#34;&gt;+=&lt;/span&gt; &lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;		shutil&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;copy(&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;train/&amp;#39;&lt;/span&gt; &lt;span style=&#34;color:#f92672&#34;&gt;+&lt;/span&gt; file, &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;train/cat/&amp;#39;&lt;/span&gt; &lt;span style=&#34;color:#f92672&#34;&gt;+&lt;/span&gt; str(count_cat) &lt;span style=&#34;color:#f92672&#34;&gt;+&lt;/span&gt; &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;.jpg&amp;#34;&lt;/span&gt;)
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;	&lt;span style=&#34;color:#66d9ef&#34;&gt;elif&lt;/span&gt;(file&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;startswith(&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;dog&amp;#39;&lt;/span&gt;) &lt;span style=&#34;color:#f92672&#34;&gt;and&lt;/span&gt; file&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;endswith(&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;jpg&amp;#39;&lt;/span&gt;)):
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;		count_dog &lt;span style=&#34;color:#f92672&#34;&gt;+=&lt;/span&gt; &lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;		shutil&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;copy(&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;test/&amp;#39;&lt;/span&gt; &lt;span style=&#34;color:#f92672&#34;&gt;+&lt;/span&gt; file, &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;train/dog/&amp;#39;&lt;/span&gt; &lt;span style=&#34;color:#f92672&#34;&gt;+&lt;/span&gt; str(count_dog) &lt;span style=&#34;color:#f92672&#34;&gt;+&lt;/span&gt; &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;.jpg&amp;#39;&lt;/span&gt;)
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;Once done, let&amp;rsquo;s go ahead and load this data. Since we have discussed this &lt;a href=&#34;https://krshrimali.github.io/posts/2019/07/custom-data-loading-using-pytorch-c-api/&#34;&gt;before&lt;/a&gt;, I&amp;rsquo;ll just paste the snippet here.&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-cpp&#34; data-lang=&#34;cpp&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;torch&lt;span style=&#34;color:#f92672&#34;&gt;::&lt;/span&gt;Tensor read_data(std&lt;span style=&#34;color:#f92672&#34;&gt;::&lt;/span&gt;string loc) {
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;	&lt;span style=&#34;color:#75715e&#34;&gt;// Read Image from the location of image
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#75715e&#34;&gt;&lt;/span&gt;	cv&lt;span style=&#34;color:#f92672&#34;&gt;::&lt;/span&gt;Mat img &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; cv&lt;span style=&#34;color:#f92672&#34;&gt;::&lt;/span&gt;imread(loc, &lt;span style=&#34;color:#ae81ff&#34;&gt;0&lt;/span&gt;);
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;	cv&lt;span style=&#34;color:#f92672&#34;&gt;::&lt;/span&gt;resize(img, img, cv&lt;span style=&#34;color:#f92672&#34;&gt;::&lt;/span&gt;Size(&lt;span style=&#34;color:#ae81ff&#34;&gt;200&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;200&lt;/span&gt;), cv&lt;span style=&#34;color:#f92672&#34;&gt;::&lt;/span&gt;INTER_CUBIC);
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;	std&lt;span style=&#34;color:#f92672&#34;&gt;::&lt;/span&gt;cout &lt;span style=&#34;color:#f92672&#34;&gt;&amp;lt;&amp;lt;&lt;/span&gt; &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;Sizes: &amp;#34;&lt;/span&gt; &lt;span style=&#34;color:#f92672&#34;&gt;&amp;lt;&amp;lt;&lt;/span&gt; img.size() &lt;span style=&#34;color:#f92672&#34;&gt;&amp;lt;&amp;lt;&lt;/span&gt; std&lt;span style=&#34;color:#f92672&#34;&gt;::&lt;/span&gt;endl;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;	torch&lt;span style=&#34;color:#f92672&#34;&gt;::&lt;/span&gt;Tensor img_tensor &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; torch&lt;span style=&#34;color:#f92672&#34;&gt;::&lt;/span&gt;from_blob(img.data, {img.rows, img.cols, &lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;}, torch&lt;span style=&#34;color:#f92672&#34;&gt;::&lt;/span&gt;kByte);
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;	img_tensor &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; img_tensor.permute({&lt;span style=&#34;color:#ae81ff&#34;&gt;2&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;0&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;}); &lt;span style=&#34;color:#75715e&#34;&gt;// Channels x Height x Width
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#75715e&#34;&gt;&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;	&lt;span style=&#34;color:#66d9ef&#34;&gt;return&lt;/span&gt; img_tensor.clone();
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;};
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;torch&lt;span style=&#34;color:#f92672&#34;&gt;::&lt;/span&gt;Tensor read_label(&lt;span style=&#34;color:#66d9ef&#34;&gt;int&lt;/span&gt; label) {
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;	torch&lt;span style=&#34;color:#f92672&#34;&gt;::&lt;/span&gt;Tensor label_tensor &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; torch&lt;span style=&#34;color:#f92672&#34;&gt;::&lt;/span&gt;full({&lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;}, label);
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;	&lt;span style=&#34;color:#66d9ef&#34;&gt;return&lt;/span&gt; label_tensor.clone();
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;}
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;vector&lt;span style=&#34;color:#f92672&#34;&gt;&amp;lt;&lt;/span&gt;torch&lt;span style=&#34;color:#f92672&#34;&gt;::&lt;/span&gt;Tensor&lt;span style=&#34;color:#f92672&#34;&gt;&amp;gt;&lt;/span&gt; process_images(vector&lt;span style=&#34;color:#f92672&#34;&gt;&amp;lt;&lt;/span&gt;string&lt;span style=&#34;color:#f92672&#34;&gt;&amp;gt;&lt;/span&gt; list_images) {
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;	cout &lt;span style=&#34;color:#f92672&#34;&gt;&amp;lt;&amp;lt;&lt;/span&gt; &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;Reading images...&amp;#34;&lt;/span&gt; &lt;span style=&#34;color:#f92672&#34;&gt;&amp;lt;&amp;lt;&lt;/span&gt; endl;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;	vector&lt;span style=&#34;color:#f92672&#34;&gt;&amp;lt;&lt;/span&gt;torch&lt;span style=&#34;color:#f92672&#34;&gt;::&lt;/span&gt;Tensor&lt;span style=&#34;color:#f92672&#34;&gt;&amp;gt;&lt;/span&gt; states;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;	&lt;span style=&#34;color:#66d9ef&#34;&gt;for&lt;/span&gt; (std&lt;span style=&#34;color:#f92672&#34;&gt;::&lt;/span&gt;vector&lt;span style=&#34;color:#f92672&#34;&gt;&amp;lt;&lt;/span&gt;string&lt;span style=&#34;color:#f92672&#34;&gt;&amp;gt;::&lt;/span&gt;iterator it &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; list_images.begin(); it &lt;span style=&#34;color:#f92672&#34;&gt;!=&lt;/span&gt; list_images.end(); &lt;span style=&#34;color:#f92672&#34;&gt;++&lt;/span&gt;it) {
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        cout &lt;span style=&#34;color:#f92672&#34;&gt;&amp;lt;&amp;lt;&lt;/span&gt; &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;Location being read: &amp;#34;&lt;/span&gt; &lt;span style=&#34;color:#f92672&#34;&gt;&amp;lt;&amp;lt;&lt;/span&gt; &lt;span style=&#34;color:#f92672&#34;&gt;*&lt;/span&gt;it &lt;span style=&#34;color:#f92672&#34;&gt;&amp;lt;&amp;lt;&lt;/span&gt; endl;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;		torch&lt;span style=&#34;color:#f92672&#34;&gt;::&lt;/span&gt;Tensor img &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; read_data(&lt;span style=&#34;color:#f92672&#34;&gt;*&lt;/span&gt;it);
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;		states.push_back(img);
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;	}
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;	cout &lt;span style=&#34;color:#f92672&#34;&gt;&amp;lt;&amp;lt;&lt;/span&gt; &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;Reading and Processing images done!&amp;#34;&lt;/span&gt; &lt;span style=&#34;color:#f92672&#34;&gt;&amp;lt;&amp;lt;&lt;/span&gt; endl;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;	&lt;span style=&#34;color:#66d9ef&#34;&gt;return&lt;/span&gt; states;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;}
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;vector&lt;span style=&#34;color:#f92672&#34;&gt;&amp;lt;&lt;/span&gt;torch&lt;span style=&#34;color:#f92672&#34;&gt;::&lt;/span&gt;Tensor&lt;span style=&#34;color:#f92672&#34;&gt;&amp;gt;&lt;/span&gt; process_labels(vector&lt;span style=&#34;color:#f92672&#34;&gt;&amp;lt;&lt;/span&gt;&lt;span style=&#34;color:#66d9ef&#34;&gt;int&lt;/span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;&amp;gt;&lt;/span&gt; list_labels) {
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;	cout &lt;span style=&#34;color:#f92672&#34;&gt;&amp;lt;&amp;lt;&lt;/span&gt; &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;Reading labels...&amp;#34;&lt;/span&gt; &lt;span style=&#34;color:#f92672&#34;&gt;&amp;lt;&amp;lt;&lt;/span&gt; endl;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;	vector&lt;span style=&#34;color:#f92672&#34;&gt;&amp;lt;&lt;/span&gt;torch&lt;span style=&#34;color:#f92672&#34;&gt;::&lt;/span&gt;Tensor&lt;span style=&#34;color:#f92672&#34;&gt;&amp;gt;&lt;/span&gt; labels;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;	&lt;span style=&#34;color:#66d9ef&#34;&gt;for&lt;/span&gt; (std&lt;span style=&#34;color:#f92672&#34;&gt;::&lt;/span&gt;vector&lt;span style=&#34;color:#f92672&#34;&gt;&amp;lt;&lt;/span&gt;&lt;span style=&#34;color:#66d9ef&#34;&gt;int&lt;/span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;&amp;gt;::&lt;/span&gt;iterator it &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; list_labels.begin(); it &lt;span style=&#34;color:#f92672&#34;&gt;!=&lt;/span&gt; list_labels.end(); &lt;span style=&#34;color:#f92672&#34;&gt;++&lt;/span&gt;it) {
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;		torch&lt;span style=&#34;color:#f92672&#34;&gt;::&lt;/span&gt;Tensor label &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; read_label(&lt;span style=&#34;color:#f92672&#34;&gt;*&lt;/span&gt;it);
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;		labels.push_back(label);
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;	}
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;	cout &lt;span style=&#34;color:#f92672&#34;&gt;&amp;lt;&amp;lt;&lt;/span&gt; &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;Labels reading done!&amp;#34;&lt;/span&gt; &lt;span style=&#34;color:#f92672&#34;&gt;&amp;lt;&amp;lt;&lt;/span&gt; endl;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;	&lt;span style=&#34;color:#66d9ef&#34;&gt;return&lt;/span&gt; labels;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;}
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#75715e&#34;&gt;/* This function returns a pair of vector of images paths (strings) and labels (integers) */&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;std&lt;span style=&#34;color:#f92672&#34;&gt;::&lt;/span&gt;pair&lt;span style=&#34;color:#f92672&#34;&gt;&amp;lt;&lt;/span&gt;vector&lt;span style=&#34;color:#f92672&#34;&gt;&amp;lt;&lt;/span&gt;string&lt;span style=&#34;color:#f92672&#34;&gt;&amp;gt;&lt;/span&gt;,vector&lt;span style=&#34;color:#f92672&#34;&gt;&amp;lt;&lt;/span&gt;&lt;span style=&#34;color:#66d9ef&#34;&gt;int&lt;/span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;&amp;gt;&amp;gt;&lt;/span&gt; load_data_from_folder(vector&lt;span style=&#34;color:#f92672&#34;&gt;&amp;lt;&lt;/span&gt;string&lt;span style=&#34;color:#f92672&#34;&gt;&amp;gt;&lt;/span&gt; folders_name) {
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;	vector&lt;span style=&#34;color:#f92672&#34;&gt;&amp;lt;&lt;/span&gt;string&lt;span style=&#34;color:#f92672&#34;&gt;&amp;gt;&lt;/span&gt; list_images;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;	vector&lt;span style=&#34;color:#f92672&#34;&gt;&amp;lt;&lt;/span&gt;&lt;span style=&#34;color:#66d9ef&#34;&gt;int&lt;/span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;&amp;gt;&lt;/span&gt; list_labels;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;	&lt;span style=&#34;color:#66d9ef&#34;&gt;int&lt;/span&gt; label &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; &lt;span style=&#34;color:#ae81ff&#34;&gt;0&lt;/span&gt;;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;	&lt;span style=&#34;color:#66d9ef&#34;&gt;for&lt;/span&gt;(&lt;span style=&#34;color:#66d9ef&#34;&gt;auto&lt;/span&gt; &lt;span style=&#34;color:#66d9ef&#34;&gt;const&lt;/span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;&amp;amp;&lt;/span&gt; value: folders_name) {
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;		string base_name &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; value &lt;span style=&#34;color:#f92672&#34;&gt;+&lt;/span&gt; &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;/&amp;#34;&lt;/span&gt;;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;		cout &lt;span style=&#34;color:#f92672&#34;&gt;&amp;lt;&amp;lt;&lt;/span&gt; &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;Reading from: &amp;#34;&lt;/span&gt; &lt;span style=&#34;color:#f92672&#34;&gt;&amp;lt;&amp;lt;&lt;/span&gt; base_name &lt;span style=&#34;color:#f92672&#34;&gt;&amp;lt;&amp;lt;&lt;/span&gt; endl;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;		DIR&lt;span style=&#34;color:#f92672&#34;&gt;*&lt;/span&gt; dir;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;		&lt;span style=&#34;color:#66d9ef&#34;&gt;struct&lt;/span&gt; &lt;span style=&#34;color:#a6e22e&#34;&gt;dirent&lt;/span&gt; &lt;span style=&#34;color:#f92672&#34;&gt;*&lt;/span&gt;ent;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;		&lt;span style=&#34;color:#66d9ef&#34;&gt;if&lt;/span&gt;((dir &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; opendir(base_name.c_str())) &lt;span style=&#34;color:#f92672&#34;&gt;!=&lt;/span&gt; NULL) {
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;			&lt;span style=&#34;color:#66d9ef&#34;&gt;while&lt;/span&gt;((ent &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; readdir(dir)) &lt;span style=&#34;color:#f92672&#34;&gt;!=&lt;/span&gt; NULL) {
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;				string filename &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; ent&lt;span style=&#34;color:#f92672&#34;&gt;-&amp;gt;&lt;/span&gt;d_name;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;				&lt;span style=&#34;color:#66d9ef&#34;&gt;if&lt;/span&gt;(filename.length() &lt;span style=&#34;color:#f92672&#34;&gt;&amp;gt;&lt;/span&gt; &lt;span style=&#34;color:#ae81ff&#34;&gt;4&lt;/span&gt; &lt;span style=&#34;color:#f92672&#34;&gt;&amp;amp;&amp;amp;&lt;/span&gt; filename.substr(filename.length() &lt;span style=&#34;color:#f92672&#34;&gt;-&lt;/span&gt; &lt;span style=&#34;color:#ae81ff&#34;&gt;3&lt;/span&gt;) &lt;span style=&#34;color:#f92672&#34;&gt;==&lt;/span&gt; &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;jpg&amp;#34;&lt;/span&gt;) {
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;					cout &lt;span style=&#34;color:#f92672&#34;&gt;&amp;lt;&amp;lt;&lt;/span&gt; base_name &lt;span style=&#34;color:#f92672&#34;&gt;+&lt;/span&gt; ent&lt;span style=&#34;color:#f92672&#34;&gt;-&amp;gt;&lt;/span&gt;d_name &lt;span style=&#34;color:#f92672&#34;&gt;&amp;lt;&amp;lt;&lt;/span&gt; endl;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;					&lt;span style=&#34;color:#75715e&#34;&gt;// cv::Mat temp = cv::imread(base_name + &amp;#34;/&amp;#34; + ent-&amp;gt;d_name, 1);
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#75715e&#34;&gt;&lt;/span&gt;					list_images.push_back(base_name &lt;span style=&#34;color:#f92672&#34;&gt;+&lt;/span&gt; ent&lt;span style=&#34;color:#f92672&#34;&gt;-&amp;gt;&lt;/span&gt;d_name);
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;					list_labels.push_back(label);
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;				}
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;			}
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;			closedir(dir);
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;		} &lt;span style=&#34;color:#66d9ef&#34;&gt;else&lt;/span&gt; {
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;			cout &lt;span style=&#34;color:#f92672&#34;&gt;&amp;lt;&amp;lt;&lt;/span&gt; &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;Could not open directory&amp;#34;&lt;/span&gt; &lt;span style=&#34;color:#f92672&#34;&gt;&amp;lt;&amp;lt;&lt;/span&gt; endl;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;			&lt;span style=&#34;color:#75715e&#34;&gt;// return EXIT_FAILURE;
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#75715e&#34;&gt;&lt;/span&gt;		}
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;		label &lt;span style=&#34;color:#f92672&#34;&gt;+=&lt;/span&gt; &lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;	}
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;	&lt;span style=&#34;color:#66d9ef&#34;&gt;return&lt;/span&gt; std&lt;span style=&#34;color:#f92672&#34;&gt;::&lt;/span&gt;make_pair(list_images, list_labels);
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;}
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;The above snippet has the utility functions we need. Here is a quick summary of what they do:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;load_data_from_folder&lt;/strong&gt;: This function returns a tuple of list of image paths (string) and list of labels (int). It takes a vector of folders names (string type) as parameter.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;process_images&lt;/strong&gt;: This function returns a vector of Tensors (images). This function calls &lt;code&gt;read_data&lt;/code&gt; function which reads, resizes and converts an image to a Torch Tensor. It takes a vector of image paths (string) as parameter.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;process_labels&lt;/strong&gt;: Similar to &lt;code&gt;process_images&lt;/code&gt; function, this function returns a vector of Tensors (labels). This function calls &lt;code&gt;read_label&lt;/code&gt; function which takes an &lt;code&gt;int&lt;/code&gt; as a parameter (label: 0 or 1) and returns a Tensor.&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;Let&amp;rsquo;s now go ahead and see how we load the data. For this, we first need to define the &lt;code&gt;Dataset&lt;/code&gt; class. This class should initialize two variables: one for images and one for labels. As discussed &lt;a href=&#34;https://krshrimali.github.io/posts/2019/07/custom-data-loading-using-pytorch-c-api/&#34;&gt;here&lt;/a&gt;, we&amp;rsquo;ll also define &lt;code&gt;get()&lt;/code&gt; and &lt;code&gt;size()&lt;/code&gt; functions.&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-cpp&#34; data-lang=&#34;cpp&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#66d9ef&#34;&gt;class&lt;/span&gt; &lt;span style=&#34;color:#a6e22e&#34;&gt;CustomDataset&lt;/span&gt; &lt;span style=&#34;color:#f92672&#34;&gt;:&lt;/span&gt; &lt;span style=&#34;color:#66d9ef&#34;&gt;public&lt;/span&gt; torch&lt;span style=&#34;color:#f92672&#34;&gt;::&lt;/span&gt;data&lt;span style=&#34;color:#f92672&#34;&gt;::&lt;/span&gt;Dataset&lt;span style=&#34;color:#f92672&#34;&gt;&amp;lt;&lt;/span&gt;CustomDataset&lt;span style=&#34;color:#f92672&#34;&gt;&amp;gt;&lt;/span&gt; {
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#66d9ef&#34;&gt;private&lt;/span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;:&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;	&lt;span style=&#34;color:#75715e&#34;&gt;/* data */&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;	&lt;span style=&#34;color:#75715e&#34;&gt;// Should be 2 tensors
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#75715e&#34;&gt;&lt;/span&gt;	vector&lt;span style=&#34;color:#f92672&#34;&gt;&amp;lt;&lt;/span&gt;torch&lt;span style=&#34;color:#f92672&#34;&gt;::&lt;/span&gt;Tensor&lt;span style=&#34;color:#f92672&#34;&gt;&amp;gt;&lt;/span&gt; states, labels;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#66d9ef&#34;&gt;public&lt;/span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;:&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;	CustomDataset(vector&lt;span style=&#34;color:#f92672&#34;&gt;&amp;lt;&lt;/span&gt;string&lt;span style=&#34;color:#f92672&#34;&gt;&amp;gt;&lt;/span&gt; list_images, vector&lt;span style=&#34;color:#f92672&#34;&gt;&amp;lt;&lt;/span&gt;&lt;span style=&#34;color:#66d9ef&#34;&gt;int&lt;/span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;&amp;gt;&lt;/span&gt; list_labels) {
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;		states &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; process_images(list_images);
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;		labels &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; process_labels(list_labels);
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;	};
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;	torch&lt;span style=&#34;color:#f92672&#34;&gt;::&lt;/span&gt;data&lt;span style=&#34;color:#f92672&#34;&gt;::&lt;/span&gt;Example&lt;span style=&#34;color:#f92672&#34;&gt;&amp;lt;&amp;gt;&lt;/span&gt; get(size_t index) &lt;span style=&#34;color:#66d9ef&#34;&gt;override&lt;/span&gt; {
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;		&lt;span style=&#34;color:#75715e&#34;&gt;/* This should return {torch::Tensor, torch::Tensor} */&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;		torch&lt;span style=&#34;color:#f92672&#34;&gt;::&lt;/span&gt;Tensor sample_img &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; states.at(index);
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;		torch&lt;span style=&#34;color:#f92672&#34;&gt;::&lt;/span&gt;Tensor sample_label &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; labels.at(index);
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;		&lt;span style=&#34;color:#66d9ef&#34;&gt;return&lt;/span&gt; {sample_img.clone(), sample_label.clone()};
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;	};
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;  torch&lt;span style=&#34;color:#f92672&#34;&gt;::&lt;/span&gt;optional&lt;span style=&#34;color:#f92672&#34;&gt;&amp;lt;&lt;/span&gt;size_t&lt;span style=&#34;color:#f92672&#34;&gt;&amp;gt;&lt;/span&gt; size() &lt;span style=&#34;color:#66d9ef&#34;&gt;const&lt;/span&gt; &lt;span style=&#34;color:#66d9ef&#34;&gt;override&lt;/span&gt; {
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;		&lt;span style=&#34;color:#66d9ef&#34;&gt;return&lt;/span&gt; states.size();
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;  };
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;};
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;Once done, we can go ahead and initialize the Dataset in our &lt;code&gt;main()&lt;/code&gt; function.&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-cpp&#34; data-lang=&#34;cpp&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#66d9ef&#34;&gt;int&lt;/span&gt; &lt;span style=&#34;color:#a6e22e&#34;&gt;main&lt;/span&gt;(&lt;span style=&#34;color:#66d9ef&#34;&gt;int&lt;/span&gt; argc, &lt;span style=&#34;color:#66d9ef&#34;&gt;char&lt;/span&gt; &lt;span style=&#34;color:#66d9ef&#34;&gt;const&lt;/span&gt; &lt;span style=&#34;color:#f92672&#34;&gt;*&lt;/span&gt;argv[]) {
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;  &lt;span style=&#34;color:#75715e&#34;&gt;// Load the model.
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#75715e&#34;&gt;&lt;/span&gt;  &lt;span style=&#34;color:#75715e&#34;&gt;// Read Data
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#75715e&#34;&gt;&lt;/span&gt;  vector&lt;span style=&#34;color:#f92672&#34;&gt;&amp;lt;&lt;/span&gt;string&lt;span style=&#34;color:#f92672&#34;&gt;&amp;gt;&lt;/span&gt; folders_name;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;  folders_name.push_back(&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;/home/krshrimali/Documents/data-dogs-cats/train/cat&amp;#34;&lt;/span&gt;);
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;  folders_name.push_back(&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;/home/krshrimali/Documents/data-dogs-cats/train/dog&amp;#34;&lt;/span&gt;);
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;  std&lt;span style=&#34;color:#f92672&#34;&gt;::&lt;/span&gt;pair&lt;span style=&#34;color:#f92672&#34;&gt;&amp;lt;&lt;/span&gt;vector&lt;span style=&#34;color:#f92672&#34;&gt;&amp;lt;&lt;/span&gt;string&lt;span style=&#34;color:#f92672&#34;&gt;&amp;gt;&lt;/span&gt;, vector&lt;span style=&#34;color:#f92672&#34;&gt;&amp;lt;&lt;/span&gt;&lt;span style=&#34;color:#66d9ef&#34;&gt;int&lt;/span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;&amp;gt;&amp;gt;&lt;/span&gt; pair_images_labels &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; load_data_from_folder(folders_name);
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;  vector&lt;span style=&#34;color:#f92672&#34;&gt;&amp;lt;&lt;/span&gt;string&lt;span style=&#34;color:#f92672&#34;&gt;&amp;gt;&lt;/span&gt; list_images &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; pair_images_labels.first;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;  vector&lt;span style=&#34;color:#f92672&#34;&gt;&amp;lt;&lt;/span&gt;&lt;span style=&#34;color:#66d9ef&#34;&gt;int&lt;/span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;&amp;gt;&lt;/span&gt; list_labels &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; pair_images_labels.second;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;  &lt;span style=&#34;color:#66d9ef&#34;&gt;auto&lt;/span&gt; custom_dataset &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; CustomDataset(list_images, list_labels).map(torch&lt;span style=&#34;color:#f92672&#34;&gt;::&lt;/span&gt;data&lt;span style=&#34;color:#f92672&#34;&gt;::&lt;/span&gt;transforms&lt;span style=&#34;color:#f92672&#34;&gt;::&lt;/span&gt;Stack&lt;span style=&#34;color:#f92672&#34;&gt;&amp;lt;&amp;gt;&lt;/span&gt;());
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;}
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;h2 id=&#34;network-overview&#34;&gt;Network Overview&lt;/h2&gt;
&lt;p&gt;To make things easy to read (a programmer&amp;rsquo;s mantra), we define our network in a header file. We will use a CNN network initially for this binary classification task. Since I&amp;rsquo;m not using a GPU, the training is slow and that&amp;rsquo;s why I experimented it only for 10 epochs. The whole objective is to discuss and show how to use PyTorch C++ API for this. You can always run it for more epochs or change the network parameters.&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-cpp&#34; data-lang=&#34;cpp&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#66d9ef&#34;&gt;struct&lt;/span&gt; &lt;span style=&#34;color:#a6e22e&#34;&gt;NetImpl&lt;/span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;:&lt;/span&gt; &lt;span style=&#34;color:#66d9ef&#34;&gt;public&lt;/span&gt; torch&lt;span style=&#34;color:#f92672&#34;&gt;::&lt;/span&gt;nn&lt;span style=&#34;color:#f92672&#34;&gt;::&lt;/span&gt;Module {
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    NetImpl() {
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        &lt;span style=&#34;color:#75715e&#34;&gt;// Initialize the network
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#75715e&#34;&gt;&lt;/span&gt;        &lt;span style=&#34;color:#75715e&#34;&gt;// On how to pass strides and padding: https://github.com/pytorch/pytorch/issues/12649#issuecomment-430156160
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#75715e&#34;&gt;&lt;/span&gt;        conv1_1 &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; register_module(&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;conv1_1&amp;#34;&lt;/span&gt;, torch&lt;span style=&#34;color:#f92672&#34;&gt;::&lt;/span&gt;nn&lt;span style=&#34;color:#f92672&#34;&gt;::&lt;/span&gt;Conv2d(torch&lt;span style=&#34;color:#f92672&#34;&gt;::&lt;/span&gt;nn&lt;span style=&#34;color:#f92672&#34;&gt;::&lt;/span&gt;Conv2dOptions(&lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;10&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;3&lt;/span&gt;).padding(&lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;)));
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        conv1_2 &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; register_module(&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;conv1_2&amp;#34;&lt;/span&gt;, torch&lt;span style=&#34;color:#f92672&#34;&gt;::&lt;/span&gt;nn&lt;span style=&#34;color:#f92672&#34;&gt;::&lt;/span&gt;Conv2d(torch&lt;span style=&#34;color:#f92672&#34;&gt;::&lt;/span&gt;nn&lt;span style=&#34;color:#f92672&#34;&gt;::&lt;/span&gt;Conv2dOptions(&lt;span style=&#34;color:#ae81ff&#34;&gt;10&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;20&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;3&lt;/span&gt;).padding(&lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;)));
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        &lt;span style=&#34;color:#75715e&#34;&gt;// Insert pool layer
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#75715e&#34;&gt;&lt;/span&gt;        conv2_1 &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; register_module(&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;conv2_1&amp;#34;&lt;/span&gt;, torch&lt;span style=&#34;color:#f92672&#34;&gt;::&lt;/span&gt;nn&lt;span style=&#34;color:#f92672&#34;&gt;::&lt;/span&gt;Conv2d(torch&lt;span style=&#34;color:#f92672&#34;&gt;::&lt;/span&gt;nn&lt;span style=&#34;color:#f92672&#34;&gt;::&lt;/span&gt;Conv2dOptions(&lt;span style=&#34;color:#ae81ff&#34;&gt;20&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;30&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;3&lt;/span&gt;).padding(&lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;)));
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        conv2_2 &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; register_module(&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;conv2_2&amp;#34;&lt;/span&gt;, torch&lt;span style=&#34;color:#f92672&#34;&gt;::&lt;/span&gt;nn&lt;span style=&#34;color:#f92672&#34;&gt;::&lt;/span&gt;Conv2d(torch&lt;span style=&#34;color:#f92672&#34;&gt;::&lt;/span&gt;nn&lt;span style=&#34;color:#f92672&#34;&gt;::&lt;/span&gt;Conv2dOptions(&lt;span style=&#34;color:#ae81ff&#34;&gt;30&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;40&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;3&lt;/span&gt;).padding(&lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;)));
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        &lt;span style=&#34;color:#75715e&#34;&gt;// Insert pool layer
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#75715e&#34;&gt;&lt;/span&gt;        conv3_1 &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; register_module(&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;conv3_1&amp;#34;&lt;/span&gt;, torch&lt;span style=&#34;color:#f92672&#34;&gt;::&lt;/span&gt;nn&lt;span style=&#34;color:#f92672&#34;&gt;::&lt;/span&gt;Conv2d(torch&lt;span style=&#34;color:#f92672&#34;&gt;::&lt;/span&gt;nn&lt;span style=&#34;color:#f92672&#34;&gt;::&lt;/span&gt;Conv2dOptions(&lt;span style=&#34;color:#ae81ff&#34;&gt;40&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;50&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;3&lt;/span&gt;).padding(&lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;)));
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        conv3_2 &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; register_module(&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;conv3_2&amp;#34;&lt;/span&gt;, torch&lt;span style=&#34;color:#f92672&#34;&gt;::&lt;/span&gt;nn&lt;span style=&#34;color:#f92672&#34;&gt;::&lt;/span&gt;Conv2d(torch&lt;span style=&#34;color:#f92672&#34;&gt;::&lt;/span&gt;nn&lt;span style=&#34;color:#f92672&#34;&gt;::&lt;/span&gt;Conv2dOptions(&lt;span style=&#34;color:#ae81ff&#34;&gt;50&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;60&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;3&lt;/span&gt;).padding(&lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;)));
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        conv3_3 &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; register_module(&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;conv3_3&amp;#34;&lt;/span&gt;, torch&lt;span style=&#34;color:#f92672&#34;&gt;::&lt;/span&gt;nn&lt;span style=&#34;color:#f92672&#34;&gt;::&lt;/span&gt;Conv2d(torch&lt;span style=&#34;color:#f92672&#34;&gt;::&lt;/span&gt;nn&lt;span style=&#34;color:#f92672&#34;&gt;::&lt;/span&gt;Conv2dOptions(&lt;span style=&#34;color:#ae81ff&#34;&gt;60&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;70&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;3&lt;/span&gt;).padding(&lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;)));
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        &lt;span style=&#34;color:#75715e&#34;&gt;// Insert pool layer
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#75715e&#34;&gt;&lt;/span&gt;        conv4_1 &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; register_module(&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;conv4_1&amp;#34;&lt;/span&gt;, torch&lt;span style=&#34;color:#f92672&#34;&gt;::&lt;/span&gt;nn&lt;span style=&#34;color:#f92672&#34;&gt;::&lt;/span&gt;Conv2d(torch&lt;span style=&#34;color:#f92672&#34;&gt;::&lt;/span&gt;nn&lt;span style=&#34;color:#f92672&#34;&gt;::&lt;/span&gt;Conv2dOptions(&lt;span style=&#34;color:#ae81ff&#34;&gt;70&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;80&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;3&lt;/span&gt;).padding(&lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;)));
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        conv4_2 &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; register_module(&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;conv4_2&amp;#34;&lt;/span&gt;, torch&lt;span style=&#34;color:#f92672&#34;&gt;::&lt;/span&gt;nn&lt;span style=&#34;color:#f92672&#34;&gt;::&lt;/span&gt;Conv2d(torch&lt;span style=&#34;color:#f92672&#34;&gt;::&lt;/span&gt;nn&lt;span style=&#34;color:#f92672&#34;&gt;::&lt;/span&gt;Conv2dOptions(&lt;span style=&#34;color:#ae81ff&#34;&gt;80&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;90&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;3&lt;/span&gt;).padding(&lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;)));
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        conv4_3 &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; register_module(&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;conv4_3&amp;#34;&lt;/span&gt;, torch&lt;span style=&#34;color:#f92672&#34;&gt;::&lt;/span&gt;nn&lt;span style=&#34;color:#f92672&#34;&gt;::&lt;/span&gt;Conv2d(torch&lt;span style=&#34;color:#f92672&#34;&gt;::&lt;/span&gt;nn&lt;span style=&#34;color:#f92672&#34;&gt;::&lt;/span&gt;Conv2dOptions(&lt;span style=&#34;color:#ae81ff&#34;&gt;90&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;100&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;3&lt;/span&gt;).padding(&lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;)));
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        &lt;span style=&#34;color:#75715e&#34;&gt;// Insert pool layer
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#75715e&#34;&gt;&lt;/span&gt;        conv5_1 &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; register_module(&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;conv5_1&amp;#34;&lt;/span&gt;, torch&lt;span style=&#34;color:#f92672&#34;&gt;::&lt;/span&gt;nn&lt;span style=&#34;color:#f92672&#34;&gt;::&lt;/span&gt;Conv2d(torch&lt;span style=&#34;color:#f92672&#34;&gt;::&lt;/span&gt;nn&lt;span style=&#34;color:#f92672&#34;&gt;::&lt;/span&gt;Conv2dOptions(&lt;span style=&#34;color:#ae81ff&#34;&gt;100&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;110&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;3&lt;/span&gt;).padding(&lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;)));
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        conv5_2 &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; register_module(&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;conv5_2&amp;#34;&lt;/span&gt;, torch&lt;span style=&#34;color:#f92672&#34;&gt;::&lt;/span&gt;nn&lt;span style=&#34;color:#f92672&#34;&gt;::&lt;/span&gt;Conv2d(torch&lt;span style=&#34;color:#f92672&#34;&gt;::&lt;/span&gt;nn&lt;span style=&#34;color:#f92672&#34;&gt;::&lt;/span&gt;Conv2dOptions(&lt;span style=&#34;color:#ae81ff&#34;&gt;110&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;120&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;3&lt;/span&gt;).padding(&lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;)));
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        conv5_3 &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; register_module(&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;conv5_3&amp;#34;&lt;/span&gt;, torch&lt;span style=&#34;color:#f92672&#34;&gt;::&lt;/span&gt;nn&lt;span style=&#34;color:#f92672&#34;&gt;::&lt;/span&gt;Conv2d(torch&lt;span style=&#34;color:#f92672&#34;&gt;::&lt;/span&gt;nn&lt;span style=&#34;color:#f92672&#34;&gt;::&lt;/span&gt;Conv2dOptions(&lt;span style=&#34;color:#ae81ff&#34;&gt;120&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;130&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;3&lt;/span&gt;).padding(&lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;)));
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        &lt;span style=&#34;color:#75715e&#34;&gt;// Insert pool layer
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#75715e&#34;&gt;&lt;/span&gt;        fc1 &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; register_module(&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;fc1&amp;#34;&lt;/span&gt;, torch&lt;span style=&#34;color:#f92672&#34;&gt;::&lt;/span&gt;nn&lt;span style=&#34;color:#f92672&#34;&gt;::&lt;/span&gt;Linear(&lt;span style=&#34;color:#ae81ff&#34;&gt;130&lt;/span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;*&lt;/span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;6&lt;/span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;*&lt;/span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;6&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;2000&lt;/span&gt;));
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        fc2 &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; register_module(&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;fc2&amp;#34;&lt;/span&gt;, torch&lt;span style=&#34;color:#f92672&#34;&gt;::&lt;/span&gt;nn&lt;span style=&#34;color:#f92672&#34;&gt;::&lt;/span&gt;Linear(&lt;span style=&#34;color:#ae81ff&#34;&gt;2000&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;1000&lt;/span&gt;));
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        fc3 &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; register_module(&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;fc3&amp;#34;&lt;/span&gt;, torch&lt;span style=&#34;color:#f92672&#34;&gt;::&lt;/span&gt;nn&lt;span style=&#34;color:#f92672&#34;&gt;::&lt;/span&gt;Linear(&lt;span style=&#34;color:#ae81ff&#34;&gt;1000&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;100&lt;/span&gt;));
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        fc4 &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; register_module(&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;fc4&amp;#34;&lt;/span&gt;, torch&lt;span style=&#34;color:#f92672&#34;&gt;::&lt;/span&gt;nn&lt;span style=&#34;color:#f92672&#34;&gt;::&lt;/span&gt;Linear(&lt;span style=&#34;color:#ae81ff&#34;&gt;100&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;2&lt;/span&gt;));
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    }
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#75715e&#34;&gt;// Implement Algorithm
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#75715e&#34;&gt;&lt;/span&gt;    torch&lt;span style=&#34;color:#f92672&#34;&gt;::&lt;/span&gt;Tensor forward(torch&lt;span style=&#34;color:#f92672&#34;&gt;::&lt;/span&gt;Tensor x) {
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        x &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; torch&lt;span style=&#34;color:#f92672&#34;&gt;::&lt;/span&gt;relu(conv1_1&lt;span style=&#34;color:#f92672&#34;&gt;-&amp;gt;&lt;/span&gt;forward(x));
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        x &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; torch&lt;span style=&#34;color:#f92672&#34;&gt;::&lt;/span&gt;relu(conv1_2&lt;span style=&#34;color:#f92672&#34;&gt;-&amp;gt;&lt;/span&gt;forward(x));
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        x &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; torch&lt;span style=&#34;color:#f92672&#34;&gt;::&lt;/span&gt;max_pool2d(x, &lt;span style=&#34;color:#ae81ff&#34;&gt;2&lt;/span&gt;);
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        x &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; torch&lt;span style=&#34;color:#f92672&#34;&gt;::&lt;/span&gt;relu(conv2_1&lt;span style=&#34;color:#f92672&#34;&gt;-&amp;gt;&lt;/span&gt;forward(x));
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        x &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; torch&lt;span style=&#34;color:#f92672&#34;&gt;::&lt;/span&gt;relu(conv2_2&lt;span style=&#34;color:#f92672&#34;&gt;-&amp;gt;&lt;/span&gt;forward(x));
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        x &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; torch&lt;span style=&#34;color:#f92672&#34;&gt;::&lt;/span&gt;max_pool2d(x, &lt;span style=&#34;color:#ae81ff&#34;&gt;2&lt;/span&gt;);
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        x &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; torch&lt;span style=&#34;color:#f92672&#34;&gt;::&lt;/span&gt;relu(conv3_1&lt;span style=&#34;color:#f92672&#34;&gt;-&amp;gt;&lt;/span&gt;forward(x));
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        x &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; torch&lt;span style=&#34;color:#f92672&#34;&gt;::&lt;/span&gt;relu(conv3_2&lt;span style=&#34;color:#f92672&#34;&gt;-&amp;gt;&lt;/span&gt;forward(x));
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        x &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; torch&lt;span style=&#34;color:#f92672&#34;&gt;::&lt;/span&gt;relu(conv3_3&lt;span style=&#34;color:#f92672&#34;&gt;-&amp;gt;&lt;/span&gt;forward(x));
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        x &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; torch&lt;span style=&#34;color:#f92672&#34;&gt;::&lt;/span&gt;max_pool2d(x, &lt;span style=&#34;color:#ae81ff&#34;&gt;2&lt;/span&gt;);
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        x &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; torch&lt;span style=&#34;color:#f92672&#34;&gt;::&lt;/span&gt;relu(conv4_1&lt;span style=&#34;color:#f92672&#34;&gt;-&amp;gt;&lt;/span&gt;forward(x));
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        x &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; torch&lt;span style=&#34;color:#f92672&#34;&gt;::&lt;/span&gt;relu(conv4_2&lt;span style=&#34;color:#f92672&#34;&gt;-&amp;gt;&lt;/span&gt;forward(x));
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        x &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; torch&lt;span style=&#34;color:#f92672&#34;&gt;::&lt;/span&gt;relu(conv4_3&lt;span style=&#34;color:#f92672&#34;&gt;-&amp;gt;&lt;/span&gt;forward(x));
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        x &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; torch&lt;span style=&#34;color:#f92672&#34;&gt;::&lt;/span&gt;max_pool2d(x, &lt;span style=&#34;color:#ae81ff&#34;&gt;2&lt;/span&gt;);
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        x &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; torch&lt;span style=&#34;color:#f92672&#34;&gt;::&lt;/span&gt;relu(conv5_1&lt;span style=&#34;color:#f92672&#34;&gt;-&amp;gt;&lt;/span&gt;forward(x));
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        x &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; torch&lt;span style=&#34;color:#f92672&#34;&gt;::&lt;/span&gt;relu(conv5_2&lt;span style=&#34;color:#f92672&#34;&gt;-&amp;gt;&lt;/span&gt;forward(x));
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        x &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; torch&lt;span style=&#34;color:#f92672&#34;&gt;::&lt;/span&gt;relu(conv5_3&lt;span style=&#34;color:#f92672&#34;&gt;-&amp;gt;&lt;/span&gt;forward(x));
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        x &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; torch&lt;span style=&#34;color:#f92672&#34;&gt;::&lt;/span&gt;max_pool2d(x, &lt;span style=&#34;color:#ae81ff&#34;&gt;2&lt;/span&gt;);
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        x &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; x.view({&lt;span style=&#34;color:#f92672&#34;&gt;-&lt;/span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;130&lt;/span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;*&lt;/span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;6&lt;/span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;*&lt;/span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;6&lt;/span&gt;});
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        x &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; torch&lt;span style=&#34;color:#f92672&#34;&gt;::&lt;/span&gt;relu(fc1&lt;span style=&#34;color:#f92672&#34;&gt;-&amp;gt;&lt;/span&gt;forward(x));
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        x &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; torch&lt;span style=&#34;color:#f92672&#34;&gt;::&lt;/span&gt;relu(fc2&lt;span style=&#34;color:#f92672&#34;&gt;-&amp;gt;&lt;/span&gt;forward(x));
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        x &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; torch&lt;span style=&#34;color:#f92672&#34;&gt;::&lt;/span&gt;relu(fc3&lt;span style=&#34;color:#f92672&#34;&gt;-&amp;gt;&lt;/span&gt;forward(x));
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        x &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; fc4&lt;span style=&#34;color:#f92672&#34;&gt;-&amp;gt;&lt;/span&gt;forward(x);
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        &lt;span style=&#34;color:#66d9ef&#34;&gt;return&lt;/span&gt; torch&lt;span style=&#34;color:#f92672&#34;&gt;::&lt;/span&gt;log_softmax(x, &lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;);
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    }
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#75715e&#34;&gt;// Declare layers
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#75715e&#34;&gt;&lt;/span&gt;    torch&lt;span style=&#34;color:#f92672&#34;&gt;::&lt;/span&gt;nn&lt;span style=&#34;color:#f92672&#34;&gt;::&lt;/span&gt;Conv2d conv1_1{&lt;span style=&#34;color:#66d9ef&#34;&gt;nullptr&lt;/span&gt;};
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    torch&lt;span style=&#34;color:#f92672&#34;&gt;::&lt;/span&gt;nn&lt;span style=&#34;color:#f92672&#34;&gt;::&lt;/span&gt;Conv2d conv1_2{&lt;span style=&#34;color:#66d9ef&#34;&gt;nullptr&lt;/span&gt;};
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    torch&lt;span style=&#34;color:#f92672&#34;&gt;::&lt;/span&gt;nn&lt;span style=&#34;color:#f92672&#34;&gt;::&lt;/span&gt;Conv2d conv2_1{&lt;span style=&#34;color:#66d9ef&#34;&gt;nullptr&lt;/span&gt;};
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    torch&lt;span style=&#34;color:#f92672&#34;&gt;::&lt;/span&gt;nn&lt;span style=&#34;color:#f92672&#34;&gt;::&lt;/span&gt;Conv2d conv2_2{&lt;span style=&#34;color:#66d9ef&#34;&gt;nullptr&lt;/span&gt;};
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    torch&lt;span style=&#34;color:#f92672&#34;&gt;::&lt;/span&gt;nn&lt;span style=&#34;color:#f92672&#34;&gt;::&lt;/span&gt;Conv2d conv3_1{&lt;span style=&#34;color:#66d9ef&#34;&gt;nullptr&lt;/span&gt;};
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    torch&lt;span style=&#34;color:#f92672&#34;&gt;::&lt;/span&gt;nn&lt;span style=&#34;color:#f92672&#34;&gt;::&lt;/span&gt;Conv2d conv3_2{&lt;span style=&#34;color:#66d9ef&#34;&gt;nullptr&lt;/span&gt;};
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    torch&lt;span style=&#34;color:#f92672&#34;&gt;::&lt;/span&gt;nn&lt;span style=&#34;color:#f92672&#34;&gt;::&lt;/span&gt;Conv2d conv3_3{&lt;span style=&#34;color:#66d9ef&#34;&gt;nullptr&lt;/span&gt;};
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    torch&lt;span style=&#34;color:#f92672&#34;&gt;::&lt;/span&gt;nn&lt;span style=&#34;color:#f92672&#34;&gt;::&lt;/span&gt;Conv2d conv4_1{&lt;span style=&#34;color:#66d9ef&#34;&gt;nullptr&lt;/span&gt;};
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    torch&lt;span style=&#34;color:#f92672&#34;&gt;::&lt;/span&gt;nn&lt;span style=&#34;color:#f92672&#34;&gt;::&lt;/span&gt;Conv2d conv4_2{&lt;span style=&#34;color:#66d9ef&#34;&gt;nullptr&lt;/span&gt;};
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    torch&lt;span style=&#34;color:#f92672&#34;&gt;::&lt;/span&gt;nn&lt;span style=&#34;color:#f92672&#34;&gt;::&lt;/span&gt;Conv2d conv4_3{&lt;span style=&#34;color:#66d9ef&#34;&gt;nullptr&lt;/span&gt;};
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    torch&lt;span style=&#34;color:#f92672&#34;&gt;::&lt;/span&gt;nn&lt;span style=&#34;color:#f92672&#34;&gt;::&lt;/span&gt;Conv2d conv5_1{&lt;span style=&#34;color:#66d9ef&#34;&gt;nullptr&lt;/span&gt;};
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    torch&lt;span style=&#34;color:#f92672&#34;&gt;::&lt;/span&gt;nn&lt;span style=&#34;color:#f92672&#34;&gt;::&lt;/span&gt;Conv2d conv5_2{&lt;span style=&#34;color:#66d9ef&#34;&gt;nullptr&lt;/span&gt;};
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    torch&lt;span style=&#34;color:#f92672&#34;&gt;::&lt;/span&gt;nn&lt;span style=&#34;color:#f92672&#34;&gt;::&lt;/span&gt;Conv2d conv5_3{&lt;span style=&#34;color:#66d9ef&#34;&gt;nullptr&lt;/span&gt;};
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    torch&lt;span style=&#34;color:#f92672&#34;&gt;::&lt;/span&gt;nn&lt;span style=&#34;color:#f92672&#34;&gt;::&lt;/span&gt;Linear fc1{&lt;span style=&#34;color:#66d9ef&#34;&gt;nullptr&lt;/span&gt;}, fc2{&lt;span style=&#34;color:#66d9ef&#34;&gt;nullptr&lt;/span&gt;}, fc3{&lt;span style=&#34;color:#66d9ef&#34;&gt;nullptr&lt;/span&gt;}, fc4{&lt;span style=&#34;color:#66d9ef&#34;&gt;nullptr&lt;/span&gt;};
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;};
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;We will initialize this network and pass each batch of our data once in an epoch.&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-cpp&#34; data-lang=&#34;cpp&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#75715e&#34;&gt;// Initialize the Network
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#75715e&#34;&gt;&lt;/span&gt;&lt;span style=&#34;color:#66d9ef&#34;&gt;auto&lt;/span&gt; net &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; std&lt;span style=&#34;color:#f92672&#34;&gt;::&lt;/span&gt;make_shared&lt;span style=&#34;color:#f92672&#34;&gt;&amp;lt;&lt;/span&gt;NetImpl&lt;span style=&#34;color:#f92672&#34;&gt;&amp;gt;&lt;/span&gt;();
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;h2 id=&#34;training-the-network-on-dogs-vs-cats-dataset&#34;&gt;Training the Network on Dogs vs Cats Dataset&lt;/h2&gt;
&lt;p&gt;We had before discussed code for training &lt;a href=&#34;https://krshrimali.github.io/posts/2019/07/training-a-network-on-custom-dataset-using-pytorch-c-api/&#34;&gt;here&lt;/a&gt;. I suggest the reader to go through that blog in order to train the dataset. I&amp;rsquo;ll give more insights on training in the next blog!&lt;/p&gt;
&lt;p&gt;That&amp;rsquo;s it for today. I&amp;rsquo;ll be back with Part-2 of this &amp;ldquo;Dogs vs Cats Classification&amp;rdquo; with training, experimentation and results. We&amp;rsquo;ll also discuss on using different networks, and in the Part-3, we&amp;rsquo;ll discuss using &lt;strong&gt;Transfer Learning&lt;/strong&gt; for this classification task.&lt;/p&gt;
      </description>
    </item>
    
    <item>
      <title>Training a Network on Custom Dataset using PyTorch C&#43;&#43; API</title>
      <link>https://krshrimali.github.io/posts/2019/07/training-a-network-on-custom-dataset-using-pytorch-c-api/</link>
      <pubDate>Fri, 05 Jul 2019 00:00:00 +0000</pubDate>
      
      <guid>https://krshrimali.github.io/posts/2019/07/training-a-network-on-custom-dataset-using-pytorch-c-api/</guid>
      <description>
          
            &lt;h4&gt;Training a Network on Custom Dataset using PyTorch C++ API&lt;/h4&gt;
          
          &lt;h2 id=&#34;recap-of-the-last-blog&#34;&gt;Recap of the last blog&lt;/h2&gt;
&lt;p&gt;Before we move on, it&amp;rsquo;s important what we covered in the last blog. We&amp;rsquo;ll be going forward from loading Custom Dataset to now using the dataset to train our VGG-16 Network. Previously, we were able to load our custom dataset using the following template:&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Note&lt;/strong&gt;: Those who are already aware of loading a custom dataset can skip this section.&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-cpp&#34; data-lang=&#34;cpp&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#66d9ef&#34;&gt;class&lt;/span&gt; &lt;span style=&#34;color:#a6e22e&#34;&gt;CustomDataset&lt;/span&gt; &lt;span style=&#34;color:#f92672&#34;&gt;:&lt;/span&gt; &lt;span style=&#34;color:#66d9ef&#34;&gt;public&lt;/span&gt; torch&lt;span style=&#34;color:#f92672&#34;&gt;::&lt;/span&gt;data&lt;span style=&#34;color:#f92672&#34;&gt;::&lt;/span&gt;dataset&lt;span style=&#34;color:#f92672&#34;&gt;&amp;lt;&lt;/span&gt;CustomDataset&lt;span style=&#34;color:#f92672&#34;&gt;&amp;gt;&lt;/span&gt; {
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#66d9ef&#34;&gt;private&lt;/span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;:&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;  &lt;span style=&#34;color:#75715e&#34;&gt;// Declare 2 vectors of tensors for images and labels
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#75715e&#34;&gt;&lt;/span&gt;  vector&lt;span style=&#34;color:#f92672&#34;&gt;&amp;lt;&lt;/span&gt;torch&lt;span style=&#34;color:#f92672&#34;&gt;::&lt;/span&gt;Tensor&lt;span style=&#34;color:#f92672&#34;&gt;&amp;gt;&lt;/span&gt; images, labels;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#66d9ef&#34;&gt;public&lt;/span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;:&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;  &lt;span style=&#34;color:#75715e&#34;&gt;// Constructor
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#75715e&#34;&gt;&lt;/span&gt;  CustomDataset(vector&lt;span style=&#34;color:#f92672&#34;&gt;&amp;lt;&lt;/span&gt;string&lt;span style=&#34;color:#f92672&#34;&gt;&amp;gt;&lt;/span&gt; list_images, vector&lt;span style=&#34;color:#f92672&#34;&gt;&amp;lt;&lt;/span&gt;string&lt;span style=&#34;color:#f92672&#34;&gt;&amp;gt;&lt;/span&gt; list_labels) {
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    images &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; process_images(list_images);
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    labels &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; process_labels(list_labels);
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;  };
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;  &lt;span style=&#34;color:#75715e&#34;&gt;// Override get() function to return tensor at location index
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#75715e&#34;&gt;&lt;/span&gt;  torch&lt;span style=&#34;color:#f92672&#34;&gt;::&lt;/span&gt;data&lt;span style=&#34;color:#f92672&#34;&gt;::&lt;/span&gt;Example&lt;span style=&#34;color:#f92672&#34;&gt;&amp;lt;&amp;gt;&lt;/span&gt; get(size_t index) &lt;span style=&#34;color:#66d9ef&#34;&gt;override&lt;/span&gt; {
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    torch&lt;span style=&#34;color:#f92672&#34;&gt;::&lt;/span&gt;Tensor sample_img &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; images.at(index);
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    torch&lt;span style=&#34;color:#f92672&#34;&gt;::&lt;/span&gt;Tensor sample_label &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; labels.at(index);
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#66d9ef&#34;&gt;return&lt;/span&gt; {sample_img.clone(), sample_label.clone()};
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;  };
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;  &lt;span style=&#34;color:#75715e&#34;&gt;// Return the length of data
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#75715e&#34;&gt;&lt;/span&gt;  torch&lt;span style=&#34;color:#f92672&#34;&gt;::&lt;/span&gt;optional&lt;span style=&#34;color:#f92672&#34;&gt;&amp;lt;&lt;/span&gt;size_t&lt;span style=&#34;color:#f92672&#34;&gt;&amp;gt;&lt;/span&gt; size() &lt;span style=&#34;color:#66d9ef&#34;&gt;const&lt;/span&gt; &lt;span style=&#34;color:#66d9ef&#34;&gt;override&lt;/span&gt; {
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#66d9ef&#34;&gt;return&lt;/span&gt; labels.size();
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;  };
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;};
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#66d9ef&#34;&gt;int&lt;/span&gt; &lt;span style=&#34;color:#a6e22e&#34;&gt;main&lt;/span&gt;(&lt;span style=&#34;color:#66d9ef&#34;&gt;int&lt;/span&gt; argc, &lt;span style=&#34;color:#66d9ef&#34;&gt;char&lt;/span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;**&lt;/span&gt; argv) {
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;  vector&lt;span style=&#34;color:#f92672&#34;&gt;&amp;lt;&lt;/span&gt;string&lt;span style=&#34;color:#f92672&#34;&gt;&amp;gt;&lt;/span&gt; list_images; &lt;span style=&#34;color:#75715e&#34;&gt;// list of path of images
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#75715e&#34;&gt;&lt;/span&gt;  vector&lt;span style=&#34;color:#f92672&#34;&gt;&amp;lt;&lt;/span&gt;&lt;span style=&#34;color:#66d9ef&#34;&gt;int&lt;/span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;&amp;gt;&lt;/span&gt; list_labels; &lt;span style=&#34;color:#75715e&#34;&gt;// list of integer labels
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#75715e&#34;&gt;&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;  &lt;span style=&#34;color:#75715e&#34;&gt;// Dataset init and apply transforms - None!
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#75715e&#34;&gt;&lt;/span&gt;  &lt;span style=&#34;color:#66d9ef&#34;&gt;auto&lt;/span&gt; custom_dataset &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; CustomDataset(list_images, list_labels).map(torch&lt;span style=&#34;color:#f92672&#34;&gt;::&lt;/span&gt;data&lt;span style=&#34;color:#f92672&#34;&gt;::&lt;/span&gt;transforms&lt;span style=&#34;color:#f92672&#34;&gt;::&lt;/span&gt;Stack&lt;span style=&#34;color:#f92672&#34;&gt;&amp;lt;&amp;gt;&lt;/span&gt;());
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;}
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;These were the steps we followed last time:&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/krshrimali/blog/main/assets/blogs/Steps-Loading-Custom-Data.PNG&#34; alt=&#34;&#34;&gt;&lt;/p&gt;
&lt;h2 id=&#34;overview-how-to-pass-batches-to-our-network&#34;&gt;Overview: How to pass batches to our network?&lt;/h2&gt;
&lt;p&gt;Since we have our dataset loaded, let&amp;rsquo;s see how to pass batches of data to our network. Before we go on and see how PyTorch C++ API does it, let&amp;rsquo;s see how it&amp;rsquo;s done in Python.&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;dataset_loader &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; torch&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;utils&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;data&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;DataLoader(custom_dataset,
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;                                             batch_size&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;4&lt;/span&gt;, shuffle&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#66d9ef&#34;&gt;True&lt;/span&gt;)
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;Just a short review of what &lt;code&gt;DataLoader()&lt;/code&gt; class does: It loads the data and returns single or multiple iterators over the dataset. We pass in our object from &lt;code&gt;Dataset&lt;/code&gt; class (here, &lt;code&gt;custom_dataset&lt;/code&gt;). We will do the same process in C++ using the following template:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-cpp&#34; data-lang=&#34;cpp&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#66d9ef&#34;&gt;auto&lt;/span&gt; data_loader &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; torch&lt;span style=&#34;color:#f92672&#34;&gt;::&lt;/span&gt;data&lt;span style=&#34;color:#f92672&#34;&gt;::&lt;/span&gt;make_data_loader&lt;span style=&#34;color:#f92672&#34;&gt;&amp;lt;&lt;/span&gt;torch&lt;span style=&#34;color:#f92672&#34;&gt;::&lt;/span&gt;data&lt;span style=&#34;color:#f92672&#34;&gt;::&lt;/span&gt;samplers&lt;span style=&#34;color:#f92672&#34;&gt;::&lt;/span&gt;SequentialSampler&lt;span style=&#34;color:#f92672&#34;&gt;&amp;gt;&lt;/span&gt;(
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;  std&lt;span style=&#34;color:#f92672&#34;&gt;::&lt;/span&gt;move(custom_dataset),
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;  batch_size
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;);
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;In brief, we are loading our data using &lt;code&gt;SequentialSampler&lt;/code&gt; class which samples our data in the same order that we provided it with. Have a look at the &lt;code&gt;SequentialSampler&lt;/code&gt; class &lt;a href=&#34;https://pytorch.org/docs/stable/data.html#torch.utils.data.SequentialSampler&#34;&gt;here&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;For the definition of this function: &lt;code&gt;torch::data::make_data_loader&lt;/code&gt; &lt;a href=&#34;https://pytorch.org/cppdocs/api/function_namespacetorch_1_1data_1a0d29ca9900cae66957c5cc5052ecc122.html#exhale-function-namespacetorch-1-1data-1a0d29ca9900cae66957c5cc5052ecc122&#34;&gt;here&lt;/a&gt;. A short screenshot from the documentation is given below.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/krshrimali/blog/main/assets/blogs/Data-Loader-Function.PNG&#34; alt=&#34;&#34;&gt;&lt;/p&gt;
&lt;p&gt;Let&amp;rsquo;s go ahead and learn to iterate through our data loader and pass each batch of data and labels to our network. For once, imagine that we have a &lt;code&gt;struct&lt;/code&gt; named &lt;code&gt;Net&lt;/code&gt; which defines our network and &lt;code&gt;forward()&lt;/code&gt; function which parses the data through each layer and returns the output.&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-cpp&#34; data-lang=&#34;cpp&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#66d9ef&#34;&gt;for&lt;/span&gt;(&lt;span style=&#34;color:#66d9ef&#34;&gt;auto&lt;/span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;&amp;amp;&lt;/span&gt; batch: &lt;span style=&#34;color:#f92672&#34;&gt;*&lt;/span&gt;data_loader) {
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;  &lt;span style=&#34;color:#66d9ef&#34;&gt;auto&lt;/span&gt; data &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; batch.data;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;  &lt;span style=&#34;color:#66d9ef&#34;&gt;auto&lt;/span&gt; target &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; batch.target.squeeze();
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;}
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;So we have retrieved our &lt;code&gt;data&lt;/code&gt; and label (&lt;code&gt;target&lt;/code&gt;) - which also depends on the batch size. If you have &lt;code&gt;batch_size&lt;/code&gt; as 4 in the &lt;code&gt;torch::data::make_data_loader()&lt;/code&gt; function, then size of the target and data will be 4.&lt;/p&gt;
&lt;h2 id=&#34;defining-the-hyperparameters-in-libtorch&#34;&gt;Defining the Hyperparameters in Libtorch&lt;/h2&gt;
&lt;p&gt;Remember the Hyperparameters we need to define for training? Let&amp;rsquo;s take a quick review of what they are:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;&lt;strong&gt;Batch Size&lt;/strong&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Optimizer&lt;/strong&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Loss Function&lt;/strong&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;We have used &lt;code&gt;batch_size&lt;/code&gt; parameter above while making the data loader. For defining optimizer, we&amp;rsquo;ll go for &lt;code&gt;Adam&lt;/code&gt; Optimizer here:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-cpp&#34; data-lang=&#34;cpp&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#75715e&#34;&gt;// We need to define the network first
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#75715e&#34;&gt;&lt;/span&gt;&lt;span style=&#34;color:#66d9ef&#34;&gt;auto&lt;/span&gt; net &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; std&lt;span style=&#34;color:#f92672&#34;&gt;::&lt;/span&gt;make_shared&lt;span style=&#34;color:#f92672&#34;&gt;&amp;lt;&lt;/span&gt;Net&lt;span style=&#34;color:#f92672&#34;&gt;&amp;gt;&lt;/span&gt;();
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;torch&lt;span style=&#34;color:#f92672&#34;&gt;::&lt;/span&gt;optim&lt;span style=&#34;color:#f92672&#34;&gt;::&lt;/span&gt;Adam optimizer(net&lt;span style=&#34;color:#f92672&#34;&gt;-&amp;gt;&lt;/span&gt;parameters(), torch&lt;span style=&#34;color:#f92672&#34;&gt;::&lt;/span&gt;optim&lt;span style=&#34;color:#f92672&#34;&gt;::&lt;/span&gt;AdamOptions(&lt;span style=&#34;color:#ae81ff&#34;&gt;1e-3&lt;/span&gt;));
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;&lt;strong&gt;Note that the PyTorch C++ API supports below listed optimizers:&lt;/strong&gt;&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;&lt;a href=&#34;https://pytorch.org/cppdocs/api/classtorch_1_1optim_1_1_r_m_sprop.html#exhale-class-classtorch-1-1optim-1-1-r-m-sprop&#34;&gt;RMSprop&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://pytorch.org/cppdocs/api/classtorch_1_1optim_1_1_s_g_d.html#exhale-class-classtorch-1-1optim-1-1-s-g-d&#34;&gt;SGD&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://pytorch.org/cppdocs/api/classtorch_1_1optim_1_1_adam.html#exhale-class-classtorch-1-1optim-1-1-adam&#34;&gt;Adam&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://pytorch.org/cppdocs/api/classtorch_1_1optim_1_1_adagrad.html#exhale-class-classtorch-1-1optim-1-1-adagrad&#34;&gt;Adagrad&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://pytorch.org/cppdocs/api/classtorch_1_1optim_1_1_l_b_f_g_s.html#exhale-class-classtorch-1-1optim-1-1-l-b-f-g-s&#34;&gt;LBFGS&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://pytorch.org/cppdocs/api/classtorch_1_1optim_1_1_loss_closure_optimizer.html#exhale-class-classtorch-1-1optim-1-1-loss-closure-optimizer&#34;&gt;LossClosureOptimizer&lt;/a&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;As mentioned in the documentation of &lt;code&gt;torch.optim&lt;/code&gt; package:&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/krshrimali/blog/main/assets/blogs/Use-Optim.PNG&#34; alt=&#34;&#34;&gt;&lt;/p&gt;
&lt;p&gt;The documentation is self explanatory, so all we need to do is pass parameters of our Network which will be optimized using our optimizer, and pass in the learning rate like above. To know about parameters we can pass through &lt;code&gt;AdamOptions&lt;/code&gt;, check out this &lt;a href=&#34;https://pytorch.org/cppdocs/api/structtorch_1_1optim_1_1_adam_options.html#exhale-struct-structtorch-1-1optim-1-1-adam-options&#34;&gt;documentation page&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;Let&amp;rsquo;s go ahead and learn to define &lt;strong&gt;Loss Function&lt;/strong&gt; in PyTorch C++ API. For an example, we&amp;rsquo;ll define &lt;code&gt;nll_loss&lt;/code&gt; (Negative Log Likelihood Loss Function):&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-cpp&#34; data-lang=&#34;cpp&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#66d9ef&#34;&gt;auto&lt;/span&gt; output &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; net&lt;span style=&#34;color:#f92672&#34;&gt;-&amp;gt;&lt;/span&gt;forward(data);
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#66d9ef&#34;&gt;auto&lt;/span&gt; loss &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; torch&lt;span style=&#34;color:#f92672&#34;&gt;::&lt;/span&gt;nll_loss(output, target);
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#75715e&#34;&gt;// To backpropagate loss
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#75715e&#34;&gt;&lt;/span&gt;loss.backward()
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;If you need to output the loss, use: &lt;code&gt;loss.item&amp;lt;float&amp;gt;()&lt;/code&gt;.&lt;/p&gt;
&lt;h2 id=&#34;training-the-network&#34;&gt;Training the Network&lt;/h2&gt;
&lt;p&gt;We are close to our last step! Training the network is almost similar to the way we do in Python. That&amp;rsquo;s why, I&amp;rsquo;ll include the code snippet here which should be self explanatory (since we have already discussed the core parts of it).&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-cpp&#34; data-lang=&#34;cpp&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;dataset_size &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; custom_dataset.size().value();
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#66d9ef&#34;&gt;int&lt;/span&gt; n_epochs &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; &lt;span style=&#34;color:#ae81ff&#34;&gt;10&lt;/span&gt;; &lt;span style=&#34;color:#75715e&#34;&gt;// Number of epochs
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#75715e&#34;&gt;&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#66d9ef&#34;&gt;for&lt;/span&gt;(&lt;span style=&#34;color:#66d9ef&#34;&gt;int&lt;/span&gt; epoch&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;; epoch&lt;span style=&#34;color:#f92672&#34;&gt;&amp;lt;=&lt;/span&gt;n_epochs; epoch&lt;span style=&#34;color:#f92672&#34;&gt;++&lt;/span&gt;) {
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;  &lt;span style=&#34;color:#66d9ef&#34;&gt;for&lt;/span&gt;(&lt;span style=&#34;color:#66d9ef&#34;&gt;auto&lt;/span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;&amp;amp;&lt;/span&gt; batch: &lt;span style=&#34;color:#f92672&#34;&gt;*&lt;/span&gt;data_loader) {
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#66d9ef&#34;&gt;auto&lt;/span&gt; data &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; batch.data;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#66d9ef&#34;&gt;auto&lt;/span&gt; target &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; batch.target.squeeze();
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#75715e&#34;&gt;// Convert data to float32 format and target to Int64 format
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#75715e&#34;&gt;&lt;/span&gt;    &lt;span style=&#34;color:#75715e&#34;&gt;// Assuming you have labels as integers
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#75715e&#34;&gt;&lt;/span&gt;    data &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; data.to(torch&lt;span style=&#34;color:#f92672&#34;&gt;::&lt;/span&gt;kF2);
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    target &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; target.to(torch&lt;span style=&#34;color:#f92672&#34;&gt;::&lt;/span&gt;kInt64);
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#75715e&#34;&gt;// Clear the optimizer parameters
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#75715e&#34;&gt;&lt;/span&gt;    optimizer.zero_grad();
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#66d9ef&#34;&gt;auto&lt;/span&gt; output &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; net&lt;span style=&#34;color:#f92672&#34;&gt;-&amp;gt;&lt;/span&gt;forward(data);
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#66d9ef&#34;&gt;auto&lt;/span&gt; loss &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; torch&lt;span style=&#34;color:#f92672&#34;&gt;::&lt;/span&gt;nll_loss(output, target);
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#75715e&#34;&gt;// Backpropagate the loss
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#75715e&#34;&gt;&lt;/span&gt;    loss.backward();
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#75715e&#34;&gt;// Update the parameters
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#75715e&#34;&gt;&lt;/span&gt;    optimizer.step();
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    cout &lt;span style=&#34;color:#f92672&#34;&gt;&amp;lt;&amp;lt;&lt;/span&gt; &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;Train Epoch: %d/%ld [%5ld/%5d] Loss: %.4f&amp;#34;&lt;/span&gt; &lt;span style=&#34;color:#f92672&#34;&gt;&amp;lt;&amp;lt;&lt;/span&gt; epoch &lt;span style=&#34;color:#f92672&#34;&gt;&amp;lt;&amp;lt;&lt;/span&gt; n_epochs &lt;span style=&#34;color:#f92672&#34;&gt;&amp;lt;&amp;lt;&lt;/span&gt; batch_index &lt;span style=&#34;color:#f92672&#34;&gt;*&lt;/span&gt; batch.data.size(&lt;span style=&#34;color:#ae81ff&#34;&gt;0&lt;/span&gt;) &lt;span style=&#34;color:#f92672&#34;&gt;&amp;lt;&amp;lt;&lt;/span&gt; dataset_size &lt;span style=&#34;color:#f92672&#34;&gt;&amp;lt;&amp;lt;&lt;/span&gt; loss.item&lt;span style=&#34;color:#f92672&#34;&gt;&amp;lt;&lt;/span&gt;&lt;span style=&#34;color:#66d9ef&#34;&gt;float&lt;/span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;&amp;gt;&lt;/span&gt;() &lt;span style=&#34;color:#f92672&#34;&gt;&amp;lt;&amp;lt;&lt;/span&gt; endl;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;  }
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;}
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#75715e&#34;&gt;// Save the model
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#75715e&#34;&gt;&lt;/span&gt;torch&lt;span style=&#34;color:#f92672&#34;&gt;::&lt;/span&gt;save(net, &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;best_model.pt&amp;#34;&lt;/span&gt;);
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;In the next blog (&lt;strong&gt;coming soon&lt;/strong&gt;), we&amp;rsquo;ll be discussing about &lt;strong&gt;Making Predictions&lt;/strong&gt; using our network and will also show an example of training our network on a benchmark dataset.&lt;/p&gt;
      </description>
    </item>
    
    <item>
      <title>Announcing a series of blogs on PyTorch C&#43;&#43; API</title>
      <link>https://krshrimali.github.io/posts/2019/07/announcing-a-series-of-blogs-on-pytorch-c-api/</link>
      <pubDate>Thu, 04 Jul 2019 00:00:00 +0000</pubDate>
      
      <guid>https://krshrimali.github.io/posts/2019/07/announcing-a-series-of-blogs-on-pytorch-c-api/</guid>
      <description>
          
            &lt;h4&gt;Announcing a series of blogs on PyTorch C++ API&lt;/h4&gt;
          
          &lt;p&gt;&lt;strong&gt;I&amp;rsquo;m happy to announce a Series of Blog Posts on PyTorch C++ API&lt;/strong&gt;. Check out the blogs in the series &lt;a href=&#34;https://krshrimali.github.io/categories/pytorch/&#34;&gt;here&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;Happy Reading!&lt;/p&gt;

      </description>
    </item>
    
    <item>
      <title>Custom Data Loading using PyTorch C&#43;&#43; API</title>
      <link>https://krshrimali.github.io/posts/2019/07/custom-data-loading-using-pytorch-c-api/</link>
      <pubDate>Tue, 02 Jul 2019 00:00:00 +0000</pubDate>
      
      <guid>https://krshrimali.github.io/posts/2019/07/custom-data-loading-using-pytorch-c-api/</guid>
      <description>
          
            &lt;h4&gt;Custom Data Loading using PyTorch C++ API&lt;/h4&gt;
          
          &lt;h2 id=&#34;overview-how-c-api-loads-data&#34;&gt;Overview: How C++ API loads data?&lt;/h2&gt;
&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/krshrimali/blog/main/assets/cover-images/Cover-Custom-Data.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt;
&lt;p&gt;In the last blog, we discussed application of a VGG-16 Network on MNIST Data. For those, who are reading this blog for the first time, here is how we had loaded MNIST data:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-cpp&#34; data-lang=&#34;cpp&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#66d9ef&#34;&gt;auto&lt;/span&gt; data_loader &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; torch&lt;span style=&#34;color:#f92672&#34;&gt;::&lt;/span&gt;data&lt;span style=&#34;color:#f92672&#34;&gt;::&lt;/span&gt;make_data_loader&lt;span style=&#34;color:#f92672&#34;&gt;&amp;lt;&lt;/span&gt;torch&lt;span style=&#34;color:#f92672&#34;&gt;::&lt;/span&gt;data&lt;span style=&#34;color:#f92672&#34;&gt;::&lt;/span&gt;samplers&lt;span style=&#34;color:#f92672&#34;&gt;::&lt;/span&gt;SequentialSampler&lt;span style=&#34;color:#f92672&#34;&gt;&amp;gt;&lt;/span&gt;(
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;			std&lt;span style=&#34;color:#f92672&#34;&gt;::&lt;/span&gt;move(torch&lt;span style=&#34;color:#f92672&#34;&gt;::&lt;/span&gt;data&lt;span style=&#34;color:#f92672&#34;&gt;::&lt;/span&gt;datasets&lt;span style=&#34;color:#f92672&#34;&gt;::&lt;/span&gt;MNIST(&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;../../data&amp;#34;&lt;/span&gt;).map(torch&lt;span style=&#34;color:#f92672&#34;&gt;::&lt;/span&gt;data&lt;span style=&#34;color:#f92672&#34;&gt;::&lt;/span&gt;transforms&lt;span style=&#34;color:#f92672&#34;&gt;::&lt;/span&gt;Normalize&lt;span style=&#34;color:#f92672&#34;&gt;&amp;lt;&amp;gt;&lt;/span&gt;(&lt;span style=&#34;color:#ae81ff&#34;&gt;0.13707&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;0.3081&lt;/span&gt;))).map(
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;				torch&lt;span style=&#34;color:#f92672&#34;&gt;::&lt;/span&gt;data&lt;span style=&#34;color:#f92672&#34;&gt;::&lt;/span&gt;transforms&lt;span style=&#34;color:#f92672&#34;&gt;::&lt;/span&gt;Stack&lt;span style=&#34;color:#f92672&#34;&gt;&amp;lt;&amp;gt;&lt;/span&gt;()), &lt;span style=&#34;color:#ae81ff&#34;&gt;64&lt;/span&gt;);
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;Let&amp;rsquo;s break this piece by piece, as for beginners, this may be unclear. First, we ask the C++ API to load data (images and labels) into tensors.&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-cpp&#34; data-lang=&#34;cpp&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#75715e&#34;&gt;// Your data should be at: ../data position
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#75715e&#34;&gt;&lt;/span&gt;&lt;span style=&#34;color:#66d9ef&#34;&gt;auto&lt;/span&gt; data_set &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; torch&lt;span style=&#34;color:#f92672&#34;&gt;::&lt;/span&gt;data&lt;span style=&#34;color:#f92672&#34;&gt;::&lt;/span&gt;datasets&lt;span style=&#34;color:#f92672&#34;&gt;::&lt;/span&gt;MNIST(&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;../data&amp;#34;&lt;/span&gt;);
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;If you have this question on how the API loads the images and labels to tensors - we&amp;rsquo;ll get to that. For now, just take it as a black box, which loads the data. Next, we apply transforms (like normalizing to ImageNet standards):&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-cpp&#34; data-lang=&#34;cpp&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#66d9ef&#34;&gt;auto&lt;/span&gt; data_set &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; data_set.map(torch&lt;span style=&#34;color:#f92672&#34;&gt;::&lt;/span&gt;data&lt;span style=&#34;color:#f92672&#34;&gt;::&lt;/span&gt;transforms&lt;span style=&#34;color:#f92672&#34;&gt;::&lt;/span&gt;Normalize&lt;span style=&#34;color:#f92672&#34;&gt;&amp;lt;&amp;gt;&lt;/span&gt;(&lt;span style=&#34;color:#ae81ff&#34;&gt;0.13707&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;0.3081&lt;/span&gt;)).map(torch&lt;span style=&#34;color:#f92672&#34;&gt;::&lt;/span&gt;data&lt;span style=&#34;color:#f92672&#34;&gt;::&lt;/span&gt;transforms&lt;span style=&#34;color:#f92672&#34;&gt;::&lt;/span&gt;Stack&lt;span style=&#34;color:#f92672&#34;&gt;&amp;lt;&amp;gt;&lt;/span&gt;())
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;For the sake of batch size, let&amp;rsquo;s divide the data for batch size as 64.&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-cpp&#34; data-lang=&#34;cpp&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;std&lt;span style=&#34;color:#f92672&#34;&gt;::&lt;/span&gt;move(data_set, &lt;span style=&#34;color:#ae81ff&#34;&gt;64&lt;/span&gt;);
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;Once this all is done, we can iterate through the data loader and pass each batch to the network. It&amp;rsquo;s time to understand how this all works, let&amp;rsquo;s go ahead and look at the source code of &lt;code&gt;torch::data::datasets::MNIST&lt;/code&gt; class.&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-cpp&#34; data-lang=&#34;cpp&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#66d9ef&#34;&gt;namespace&lt;/span&gt; torch {
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#66d9ef&#34;&gt;namespace&lt;/span&gt; data {
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#66d9ef&#34;&gt;namespace&lt;/span&gt; datasets {
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#75715e&#34;&gt;/// The MNIST dataset.
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#75715e&#34;&gt;&lt;/span&gt;&lt;span style=&#34;color:#66d9ef&#34;&gt;class&lt;/span&gt; &lt;span style=&#34;color:#a6e22e&#34;&gt;TORCH_API&lt;/span&gt; MNIST : &lt;span style=&#34;color:#66d9ef&#34;&gt;public&lt;/span&gt; Dataset&lt;span style=&#34;color:#f92672&#34;&gt;&amp;lt;&lt;/span&gt;MNIST&lt;span style=&#34;color:#f92672&#34;&gt;&amp;gt;&lt;/span&gt; {
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt; &lt;span style=&#34;color:#66d9ef&#34;&gt;public&lt;/span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;:&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;  &lt;span style=&#34;color:#75715e&#34;&gt;/// The mode in which the dataset is loaded.
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#75715e&#34;&gt;&lt;/span&gt;  &lt;span style=&#34;color:#66d9ef&#34;&gt;enum&lt;/span&gt; &lt;span style=&#34;color:#66d9ef&#34;&gt;class&lt;/span&gt; &lt;span style=&#34;color:#a6e22e&#34;&gt;Mode&lt;/span&gt; { kTrain, kTest };
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;  &lt;span style=&#34;color:#75715e&#34;&gt;/// Loads the MNIST dataset from the root path.
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#75715e&#34;&gt;&lt;/span&gt;  &lt;span style=&#34;color:#75715e&#34;&gt;///
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#75715e&#34;&gt;&lt;/span&gt;  &lt;span style=&#34;color:#75715e&#34;&gt;/// The supplied root path should contain the content of the unzipped
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#75715e&#34;&gt;&lt;/span&gt;  &lt;span style=&#34;color:#75715e&#34;&gt;/// MNIST dataset, available from http://yann.lecun.com/exdb/mnist.
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#75715e&#34;&gt;&lt;/span&gt;  &lt;span style=&#34;color:#66d9ef&#34;&gt;explicit&lt;/span&gt; &lt;span style=&#34;color:#a6e22e&#34;&gt;MNIST&lt;/span&gt;(&lt;span style=&#34;color:#66d9ef&#34;&gt;const&lt;/span&gt; std&lt;span style=&#34;color:#f92672&#34;&gt;::&lt;/span&gt;string&lt;span style=&#34;color:#f92672&#34;&gt;&amp;amp;&lt;/span&gt; root, Mode mode &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; Mode&lt;span style=&#34;color:#f92672&#34;&gt;::&lt;/span&gt;kTrain);
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;  &lt;span style=&#34;color:#75715e&#34;&gt;/// Returns the Example at the given index.
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#75715e&#34;&gt;&lt;/span&gt;  Example&lt;span style=&#34;color:#f92672&#34;&gt;&amp;lt;&amp;gt;&lt;/span&gt; get(size_t index) &lt;span style=&#34;color:#66d9ef&#34;&gt;override&lt;/span&gt;;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;  &lt;span style=&#34;color:#75715e&#34;&gt;/// Returns the size of the dataset.
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#75715e&#34;&gt;&lt;/span&gt;  optional&lt;span style=&#34;color:#f92672&#34;&gt;&amp;lt;&lt;/span&gt;size_t&lt;span style=&#34;color:#f92672&#34;&gt;&amp;gt;&lt;/span&gt; size() &lt;span style=&#34;color:#66d9ef&#34;&gt;const&lt;/span&gt; &lt;span style=&#34;color:#66d9ef&#34;&gt;override&lt;/span&gt;;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;  &lt;span style=&#34;color:#75715e&#34;&gt;/// Returns true if this is the training subset of MNIST.
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#75715e&#34;&gt;&lt;/span&gt;  &lt;span style=&#34;color:#66d9ef&#34;&gt;bool&lt;/span&gt; &lt;span style=&#34;color:#a6e22e&#34;&gt;is_train&lt;/span&gt;() &lt;span style=&#34;color:#66d9ef&#34;&gt;const&lt;/span&gt; &lt;span style=&#34;color:#66d9ef&#34;&gt;noexcept&lt;/span&gt;;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;  &lt;span style=&#34;color:#75715e&#34;&gt;/// Returns all images stacked into a single tensor.
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#75715e&#34;&gt;&lt;/span&gt;  &lt;span style=&#34;color:#66d9ef&#34;&gt;const&lt;/span&gt; Tensor&lt;span style=&#34;color:#f92672&#34;&gt;&amp;amp;&lt;/span&gt; images() &lt;span style=&#34;color:#66d9ef&#34;&gt;const&lt;/span&gt;;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;  &lt;span style=&#34;color:#75715e&#34;&gt;/// Returns all targets stacked into a single tensor.
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#75715e&#34;&gt;&lt;/span&gt;  &lt;span style=&#34;color:#66d9ef&#34;&gt;const&lt;/span&gt; Tensor&lt;span style=&#34;color:#f92672&#34;&gt;&amp;amp;&lt;/span&gt; targets() &lt;span style=&#34;color:#66d9ef&#34;&gt;const&lt;/span&gt;;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt; &lt;span style=&#34;color:#66d9ef&#34;&gt;private&lt;/span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;:&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;  Tensor images_, targets_;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;};
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;} &lt;span style=&#34;color:#75715e&#34;&gt;// namespace datasets
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#75715e&#34;&gt;&lt;/span&gt;} &lt;span style=&#34;color:#75715e&#34;&gt;// namespace data
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#75715e&#34;&gt;&lt;/span&gt;} &lt;span style=&#34;color:#75715e&#34;&gt;// namespace torch
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;Reference: &lt;a href=&#34;https://github.com/pytorch/pytorch/blob/master/torch/csrc/api/include/torch/data/datasets/mnist.h&#34;&gt;https://github.com/pytorch/pytorch/blob/master/torch/csrc/api/include/torch/data/datasets/mnist.h&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Assuming the reader has done some basic C++ before reading this, they will be very well aware of how to initialize a C++ Class. Let&amp;rsquo;s go step by step. What happens when we initialize the class? Let&amp;rsquo;s look at the definition of constructor of the class MNIST at &lt;code&gt;mnist.cpp&lt;/code&gt;:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-cpp&#34; data-lang=&#34;cpp&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;MNIST&lt;span style=&#34;color:#f92672&#34;&gt;::&lt;/span&gt;MNIST(&lt;span style=&#34;color:#66d9ef&#34;&gt;const&lt;/span&gt; std&lt;span style=&#34;color:#f92672&#34;&gt;::&lt;/span&gt;string&lt;span style=&#34;color:#f92672&#34;&gt;&amp;amp;&lt;/span&gt; root, Mode mode)
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#f92672&#34;&gt;:&lt;/span&gt; images_(read_images(root, mode &lt;span style=&#34;color:#f92672&#34;&gt;==&lt;/span&gt; Mode&lt;span style=&#34;color:#f92672&#34;&gt;::&lt;/span&gt;kTrain)),
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;      targets_(read_targets(root, mode &lt;span style=&#34;color:#f92672&#34;&gt;==&lt;/span&gt; Mode&lt;span style=&#34;color:#f92672&#34;&gt;::&lt;/span&gt;kTrain)) {}
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;Observing the above snippet, it&amp;rsquo;s clear that the constructor calls &lt;code&gt;read_images(root, mode)&lt;/code&gt; and &lt;code&gt;read_targets&lt;/code&gt; for loading images and labels into tensors. Let&amp;rsquo;s go to the source code of &lt;code&gt;read_images()&lt;/code&gt; and &lt;code&gt;read_targets()&lt;/code&gt;.&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;&lt;strong&gt;read_images()&lt;/strong&gt;:&lt;/li&gt;
&lt;/ol&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-cpp&#34; data-lang=&#34;cpp&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;Tensor &lt;span style=&#34;color:#a6e22e&#34;&gt;read_images&lt;/span&gt;(&lt;span style=&#34;color:#66d9ef&#34;&gt;const&lt;/span&gt; std&lt;span style=&#34;color:#f92672&#34;&gt;::&lt;/span&gt;string&lt;span style=&#34;color:#f92672&#34;&gt;&amp;amp;&lt;/span&gt; root, &lt;span style=&#34;color:#66d9ef&#34;&gt;bool&lt;/span&gt; train) {
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;  &lt;span style=&#34;color:#75715e&#34;&gt;// kTrainImagesFilename and kTestImagesFilename are specific to MNIST dataset here
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#75715e&#34;&gt;&lt;/span&gt;  &lt;span style=&#34;color:#75715e&#34;&gt;// No need for using join_paths here
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#75715e&#34;&gt;&lt;/span&gt;  &lt;span style=&#34;color:#66d9ef&#34;&gt;const&lt;/span&gt; &lt;span style=&#34;color:#66d9ef&#34;&gt;auto&lt;/span&gt; path &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;      join_paths(root, train &lt;span style=&#34;color:#f92672&#34;&gt;?&lt;/span&gt; kTrainImagesFilename : kTestImagesFilename);
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;  &lt;span style=&#34;color:#75715e&#34;&gt;// Load images
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#75715e&#34;&gt;&lt;/span&gt;  std&lt;span style=&#34;color:#f92672&#34;&gt;::&lt;/span&gt;ifstream images(path, std&lt;span style=&#34;color:#f92672&#34;&gt;::&lt;/span&gt;ios&lt;span style=&#34;color:#f92672&#34;&gt;::&lt;/span&gt;binary);
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;  TORCH_CHECK(images, &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;Error opening images file at &amp;#34;&lt;/span&gt;, path);
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;  &lt;span style=&#34;color:#75715e&#34;&gt;// kTrainSize = len(training data)
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#75715e&#34;&gt;&lt;/span&gt;  &lt;span style=&#34;color:#75715e&#34;&gt;// kTestSize = len(testing_data)
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#75715e&#34;&gt;&lt;/span&gt;  &lt;span style=&#34;color:#66d9ef&#34;&gt;const&lt;/span&gt; &lt;span style=&#34;color:#66d9ef&#34;&gt;auto&lt;/span&gt; count &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; train &lt;span style=&#34;color:#f92672&#34;&gt;?&lt;/span&gt; kTrainSize : kTestSize;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;  &lt;span style=&#34;color:#75715e&#34;&gt;// Specific to MNIST data
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#75715e&#34;&gt;&lt;/span&gt;  &lt;span style=&#34;color:#75715e&#34;&gt;// From http://yann.lecun.com/exdb/mnist/
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#75715e&#34;&gt;&lt;/span&gt;  expect_int32(images, kImageMagicNumber);
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;  expect_int32(images, count);
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;  expect_int32(images, kImageRows);
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;  expect_int32(images, kImageColumns);
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;  &lt;span style=&#34;color:#75715e&#34;&gt;// This converts images to tensors
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#75715e&#34;&gt;&lt;/span&gt;  &lt;span style=&#34;color:#75715e&#34;&gt;// Allocate an empty tensor of size of image (count, channels, height, width)
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#75715e&#34;&gt;&lt;/span&gt;  &lt;span style=&#34;color:#66d9ef&#34;&gt;auto&lt;/span&gt; tensor &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;      torch&lt;span style=&#34;color:#f92672&#34;&gt;::&lt;/span&gt;empty({count, &lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;, kImageRows, kImageColumns}, torch&lt;span style=&#34;color:#f92672&#34;&gt;::&lt;/span&gt;kByte);
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;  &lt;span style=&#34;color:#75715e&#34;&gt;// Read image and convert to tensor
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#75715e&#34;&gt;&lt;/span&gt;  images.read(&lt;span style=&#34;color:#66d9ef&#34;&gt;reinterpret_cast&lt;/span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;&amp;lt;&lt;/span&gt;&lt;span style=&#34;color:#66d9ef&#34;&gt;char&lt;/span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;*&amp;gt;&lt;/span&gt;(tensor.data_ptr()), tensor.numel());
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;  &lt;span style=&#34;color:#75715e&#34;&gt;// Normalize the image from 0 to 255 to 0 to 1
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#75715e&#34;&gt;&lt;/span&gt;  &lt;span style=&#34;color:#66d9ef&#34;&gt;return&lt;/span&gt; tensor.to(torch&lt;span style=&#34;color:#f92672&#34;&gt;::&lt;/span&gt;kFloat32).div_(&lt;span style=&#34;color:#ae81ff&#34;&gt;255&lt;/span&gt;);
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;}
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;ol start=&#34;2&#34;&gt;
&lt;li&gt;&lt;strong&gt;read_targets()&lt;/strong&gt;:&lt;/li&gt;
&lt;/ol&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-cpp&#34; data-lang=&#34;cpp&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;Tensor &lt;span style=&#34;color:#a6e22e&#34;&gt;read_targets&lt;/span&gt;(&lt;span style=&#34;color:#66d9ef&#34;&gt;const&lt;/span&gt; std&lt;span style=&#34;color:#f92672&#34;&gt;::&lt;/span&gt;string&lt;span style=&#34;color:#f92672&#34;&gt;&amp;amp;&lt;/span&gt; root, &lt;span style=&#34;color:#66d9ef&#34;&gt;bool&lt;/span&gt; train) {
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;  &lt;span style=&#34;color:#75715e&#34;&gt;// Specific to MNIST dataset (kTrainImagesFilename and kTestTargetsFilename)
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#75715e&#34;&gt;&lt;/span&gt;  &lt;span style=&#34;color:#66d9ef&#34;&gt;const&lt;/span&gt; &lt;span style=&#34;color:#66d9ef&#34;&gt;auto&lt;/span&gt; path &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;      join_paths(root, train &lt;span style=&#34;color:#f92672&#34;&gt;?&lt;/span&gt; kTrainTargetsFilename : kTestTargetsFilename);
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;  &lt;span style=&#34;color:#75715e&#34;&gt;// Read the labels
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#75715e&#34;&gt;&lt;/span&gt;  std&lt;span style=&#34;color:#f92672&#34;&gt;::&lt;/span&gt;ifstream targets(path, std&lt;span style=&#34;color:#f92672&#34;&gt;::&lt;/span&gt;ios&lt;span style=&#34;color:#f92672&#34;&gt;::&lt;/span&gt;binary);
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;  TORCH_CHECK(targets, &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;Error opening targets file at &amp;#34;&lt;/span&gt;, path);
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;  &lt;span style=&#34;color:#75715e&#34;&gt;// kTrainSize = len(training_labels)
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#75715e&#34;&gt;&lt;/span&gt;  &lt;span style=&#34;color:#75715e&#34;&gt;// kTestSize = len(testing_labels)
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#75715e&#34;&gt;&lt;/span&gt;  &lt;span style=&#34;color:#66d9ef&#34;&gt;const&lt;/span&gt; &lt;span style=&#34;color:#66d9ef&#34;&gt;auto&lt;/span&gt; count &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; train &lt;span style=&#34;color:#f92672&#34;&gt;?&lt;/span&gt; kTrainSize : kTestSize;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;  expect_int32(targets, kTargetMagicNumber);
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;  expect_int32(targets, count);
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;  &lt;span style=&#34;color:#75715e&#34;&gt;// Allocate an empty tensor of size of number of labels
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#75715e&#34;&gt;&lt;/span&gt;  &lt;span style=&#34;color:#66d9ef&#34;&gt;auto&lt;/span&gt; tensor &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; torch&lt;span style=&#34;color:#f92672&#34;&gt;::&lt;/span&gt;empty(count, torch&lt;span style=&#34;color:#f92672&#34;&gt;::&lt;/span&gt;kByte);
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;  &lt;span style=&#34;color:#75715e&#34;&gt;// Convert to tensor
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#75715e&#34;&gt;&lt;/span&gt;  targets.read(&lt;span style=&#34;color:#66d9ef&#34;&gt;reinterpret_cast&lt;/span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;&amp;lt;&lt;/span&gt;&lt;span style=&#34;color:#66d9ef&#34;&gt;char&lt;/span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;*&amp;gt;&lt;/span&gt;(tensor.data_ptr()), count);
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;  &lt;span style=&#34;color:#66d9ef&#34;&gt;return&lt;/span&gt; tensor.to(torch&lt;span style=&#34;color:#f92672&#34;&gt;::&lt;/span&gt;kInt64);
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;}
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;Since we are now done with how the constructor works, let&amp;rsquo;s go ahead and see what other functions does the class inherit.&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-cpp&#34; data-lang=&#34;cpp&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;Example&lt;span style=&#34;color:#f92672&#34;&gt;&amp;lt;&amp;gt;&lt;/span&gt; MNIST&lt;span style=&#34;color:#f92672&#34;&gt;::&lt;/span&gt;get(size_t index) {
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;  &lt;span style=&#34;color:#66d9ef&#34;&gt;return&lt;/span&gt; {images_[index], targets_[index]};
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;}
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;optional&lt;span style=&#34;color:#f92672&#34;&gt;&amp;lt;&lt;/span&gt;size_t&lt;span style=&#34;color:#f92672&#34;&gt;&amp;gt;&lt;/span&gt; MNIST&lt;span style=&#34;color:#f92672&#34;&gt;::&lt;/span&gt;size() &lt;span style=&#34;color:#66d9ef&#34;&gt;const&lt;/span&gt; {
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;  &lt;span style=&#34;color:#66d9ef&#34;&gt;return&lt;/span&gt; images_.size(&lt;span style=&#34;color:#ae81ff&#34;&gt;0&lt;/span&gt;);
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;}
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;The above two functions: &lt;code&gt;get(size_t)&lt;/code&gt; and &lt;code&gt;size()&lt;/code&gt; are used to get a sample image and label and length of the data respectively.&lt;/p&gt;
&lt;h2 id=&#34;the-pipeline&#34;&gt;The Pipeline&lt;/h2&gt;
&lt;p&gt;Since we are now clear with the possible pipeline of loading custom data:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Read Images and Labels&lt;/li&gt;
&lt;li&gt;Convert to Tensors&lt;/li&gt;
&lt;li&gt;Write &lt;code&gt;get()&lt;/code&gt; and &lt;code&gt;size()&lt;/code&gt; functions&lt;/li&gt;
&lt;li&gt;Initialize the class with paths of images and labels&lt;/li&gt;
&lt;li&gt;Pass it to the data loader&lt;/li&gt;
&lt;/ol&gt;
&lt;h2 id=&#34;coding-your-own-custom-data-loader&#34;&gt;Coding your own Custom Data Loader&lt;/h2&gt;
&lt;p&gt;Let&amp;rsquo;s first write the template of our custom data loader:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-cpp&#34; data-lang=&#34;cpp&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#75715e&#34;&gt;// Include libraries
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#75715e&#34;&gt;&lt;/span&gt;&lt;span style=&#34;color:#75715e&#34;&gt;#include&lt;/span&gt; &lt;span style=&#34;color:#75715e&#34;&gt;&amp;lt;ATen/ATen.h&amp;gt;&lt;/span&gt;&lt;span style=&#34;color:#75715e&#34;&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#75715e&#34;&gt;#include&lt;/span&gt; &lt;span style=&#34;color:#75715e&#34;&gt;&amp;lt;torch/torch.h&amp;gt;&lt;/span&gt;&lt;span style=&#34;color:#75715e&#34;&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#75715e&#34;&gt;#include&lt;/span&gt; &lt;span style=&#34;color:#75715e&#34;&gt;&amp;lt;iostream&amp;gt;&lt;/span&gt;&lt;span style=&#34;color:#75715e&#34;&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#75715e&#34;&gt;#include&lt;/span&gt; &lt;span style=&#34;color:#75715e&#34;&gt;&amp;lt;vector&amp;gt;&lt;/span&gt;&lt;span style=&#34;color:#75715e&#34;&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#75715e&#34;&gt;#include&lt;/span&gt; &lt;span style=&#34;color:#75715e&#34;&gt;&amp;lt;tuple&amp;gt;&lt;/span&gt;&lt;span style=&#34;color:#75715e&#34;&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#75715e&#34;&gt;#include&lt;/span&gt; &lt;span style=&#34;color:#75715e&#34;&gt;&amp;lt;opencv2/opencv.hpp&amp;gt;&lt;/span&gt;&lt;span style=&#34;color:#75715e&#34;&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#75715e&#34;&gt;#include&lt;/span&gt; &lt;span style=&#34;color:#75715e&#34;&gt;&amp;lt;string&amp;gt;&lt;/span&gt;&lt;span style=&#34;color:#75715e&#34;&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#75715e&#34;&gt;&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#75715e&#34;&gt;/* Convert and Load image to tensor from location argument */&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;torch&lt;span style=&#34;color:#f92672&#34;&gt;::&lt;/span&gt;Tensor read_data(std&lt;span style=&#34;color:#f92672&#34;&gt;::&lt;/span&gt;string location) {
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;  &lt;span style=&#34;color:#75715e&#34;&gt;// Read Data here
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#75715e&#34;&gt;&lt;/span&gt;  &lt;span style=&#34;color:#75715e&#34;&gt;// Return tensor form of the image
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#75715e&#34;&gt;&lt;/span&gt;  &lt;span style=&#34;color:#66d9ef&#34;&gt;return&lt;/span&gt; torch&lt;span style=&#34;color:#f92672&#34;&gt;::&lt;/span&gt;Tensor;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;}
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#75715e&#34;&gt;/* Converts label to tensor type in the integer argument */&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;torch&lt;span style=&#34;color:#f92672&#34;&gt;::&lt;/span&gt;Tensor read_label(&lt;span style=&#34;color:#66d9ef&#34;&gt;int&lt;/span&gt; label) {
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;  &lt;span style=&#34;color:#75715e&#34;&gt;// Read label here
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#75715e&#34;&gt;&lt;/span&gt;  &lt;span style=&#34;color:#75715e&#34;&gt;// Convert to tensor and return
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#75715e&#34;&gt;&lt;/span&gt;  &lt;span style=&#34;color:#66d9ef&#34;&gt;return&lt;/span&gt; torch&lt;span style=&#34;color:#f92672&#34;&gt;::&lt;/span&gt;Tensor;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;}
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#75715e&#34;&gt;/* Loads images to tensor type in the string argument */&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;vector&lt;span style=&#34;color:#f92672&#34;&gt;&amp;lt;&lt;/span&gt;torch&lt;span style=&#34;color:#f92672&#34;&gt;::&lt;/span&gt;Tensor&lt;span style=&#34;color:#f92672&#34;&gt;&amp;gt;&lt;/span&gt; process_images(vector&lt;span style=&#34;color:#f92672&#34;&gt;&amp;lt;&lt;/span&gt;string&lt;span style=&#34;color:#f92672&#34;&gt;&amp;gt;&lt;/span&gt; list_images) {
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;  cout &lt;span style=&#34;color:#f92672&#34;&gt;&amp;lt;&amp;lt;&lt;/span&gt; &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;Reading Images...&amp;#34;&lt;/span&gt; &lt;span style=&#34;color:#f92672&#34;&gt;&amp;lt;&amp;lt;&lt;/span&gt; endl;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;  &lt;span style=&#34;color:#75715e&#34;&gt;// Return vector of Tensor form of all the images
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#75715e&#34;&gt;&lt;/span&gt;  &lt;span style=&#34;color:#66d9ef&#34;&gt;return&lt;/span&gt; vector&lt;span style=&#34;color:#f92672&#34;&gt;&amp;lt;&lt;/span&gt;torch&lt;span style=&#34;color:#f92672&#34;&gt;::&lt;/span&gt;Tensor&lt;span style=&#34;color:#f92672&#34;&gt;&amp;gt;&lt;/span&gt;;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;}
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#75715e&#34;&gt;/* Loads labels to tensor type in the string argument */&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;vector&lt;span style=&#34;color:#f92672&#34;&gt;&amp;lt;&lt;/span&gt;torch&lt;span style=&#34;color:#f92672&#34;&gt;::&lt;/span&gt;Tensor&lt;span style=&#34;color:#f92672&#34;&gt;&amp;gt;&lt;/span&gt; process_labels(vector&lt;span style=&#34;color:#f92672&#34;&gt;&amp;lt;&lt;/span&gt;string&lt;span style=&#34;color:#f92672&#34;&gt;&amp;gt;&lt;/span&gt; list_labels) {
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;  cout &lt;span style=&#34;color:#f92672&#34;&gt;&amp;lt;&amp;lt;&lt;/span&gt; &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;Reading Labels...&amp;#34;&lt;/span&gt; &lt;span style=&#34;color:#f92672&#34;&gt;&amp;lt;&amp;lt;&lt;/span&gt; endl;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;  &lt;span style=&#34;color:#75715e&#34;&gt;// Return vector of Tensor form of all the labels
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#75715e&#34;&gt;&lt;/span&gt;  &lt;span style=&#34;color:#66d9ef&#34;&gt;return&lt;/span&gt; vector&lt;span style=&#34;color:#f92672&#34;&gt;&amp;lt;&lt;/span&gt;torch&lt;span style=&#34;color:#f92672&#34;&gt;::&lt;/span&gt;Tensor&lt;span style=&#34;color:#f92672&#34;&gt;&amp;gt;&lt;/span&gt;;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;}
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#66d9ef&#34;&gt;class&lt;/span&gt; &lt;span style=&#34;color:#a6e22e&#34;&gt;CustomDataset&lt;/span&gt; &lt;span style=&#34;color:#f92672&#34;&gt;:&lt;/span&gt; &lt;span style=&#34;color:#66d9ef&#34;&gt;public&lt;/span&gt; torch&lt;span style=&#34;color:#f92672&#34;&gt;::&lt;/span&gt;data&lt;span style=&#34;color:#f92672&#34;&gt;::&lt;/span&gt;dataset&lt;span style=&#34;color:#f92672&#34;&gt;&amp;lt;&lt;/span&gt;CustomDataset&lt;span style=&#34;color:#f92672&#34;&gt;&amp;gt;&lt;/span&gt; {
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#66d9ef&#34;&gt;private&lt;/span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;:&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;  &lt;span style=&#34;color:#75715e&#34;&gt;// Declare 2 vectors of tensors for images and labels
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#75715e&#34;&gt;&lt;/span&gt;  vector&lt;span style=&#34;color:#f92672&#34;&gt;&amp;lt;&lt;/span&gt;torch&lt;span style=&#34;color:#f92672&#34;&gt;::&lt;/span&gt;Tensor&lt;span style=&#34;color:#f92672&#34;&gt;&amp;gt;&lt;/span&gt; images, labels;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#66d9ef&#34;&gt;public&lt;/span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;:&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;  &lt;span style=&#34;color:#75715e&#34;&gt;// Constructor
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#75715e&#34;&gt;&lt;/span&gt;  CustomDataset(vector&lt;span style=&#34;color:#f92672&#34;&gt;&amp;lt;&lt;/span&gt;string&lt;span style=&#34;color:#f92672&#34;&gt;&amp;gt;&lt;/span&gt; list_images, vector&lt;span style=&#34;color:#f92672&#34;&gt;&amp;lt;&lt;/span&gt;string&lt;span style=&#34;color:#f92672&#34;&gt;&amp;gt;&lt;/span&gt; list_labels) {
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    images &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; process_images(list_images);
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    labels &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; process_labels(list_labels);
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;  };
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;  &lt;span style=&#34;color:#75715e&#34;&gt;// Override get() function to return tensor at location index
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#75715e&#34;&gt;&lt;/span&gt;  torch&lt;span style=&#34;color:#f92672&#34;&gt;::&lt;/span&gt;data&lt;span style=&#34;color:#f92672&#34;&gt;::&lt;/span&gt;Example&lt;span style=&#34;color:#f92672&#34;&gt;&amp;lt;&amp;gt;&lt;/span&gt; get(size_t index) &lt;span style=&#34;color:#66d9ef&#34;&gt;override&lt;/span&gt; {
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    torch&lt;span style=&#34;color:#f92672&#34;&gt;::&lt;/span&gt;Tensor sample_img &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; images.at(index);
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    torch&lt;span style=&#34;color:#f92672&#34;&gt;::&lt;/span&gt;Tensor sample_label &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; labels.at(index);
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#66d9ef&#34;&gt;return&lt;/span&gt; {sample_img.clone(), sample_label.clone()};
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;  };
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;  &lt;span style=&#34;color:#75715e&#34;&gt;// Return the length of data
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#75715e&#34;&gt;&lt;/span&gt;  torch&lt;span style=&#34;color:#f92672&#34;&gt;::&lt;/span&gt;optional&lt;span style=&#34;color:#f92672&#34;&gt;&amp;lt;&lt;/span&gt;size_t&lt;span style=&#34;color:#f92672&#34;&gt;&amp;gt;&lt;/span&gt; size() &lt;span style=&#34;color:#66d9ef&#34;&gt;const&lt;/span&gt; &lt;span style=&#34;color:#66d9ef&#34;&gt;override&lt;/span&gt; {
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#66d9ef&#34;&gt;return&lt;/span&gt; labels.size();
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;  };
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;};
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;We are almost there, all we need to do is - Read Images and Labels to &lt;code&gt;torch::Tensor&lt;/code&gt; type. I&amp;rsquo;ll be using OpenCV to read images, as it also helps later on to visualize results.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Reading Images&lt;/strong&gt;:&lt;/p&gt;
&lt;p&gt;The process to read an image in OpenCV is trivial: &lt;code&gt;cv::imread(std::string location, int)&lt;/code&gt;. We then convert it to a tensor. Note that a tensor is of form (batch size, channels, height, width), so we also permute the tensor to that form.&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-cpp&#34; data-lang=&#34;cpp&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;torch&lt;span style=&#34;color:#f92672&#34;&gt;::&lt;/span&gt;Tensor read_data(std&lt;span style=&#34;color:#f92672&#34;&gt;::&lt;/span&gt;string loc) {
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;	&lt;span style=&#34;color:#75715e&#34;&gt;// Read Image from the location of image
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#75715e&#34;&gt;&lt;/span&gt;	cv&lt;span style=&#34;color:#f92672&#34;&gt;::&lt;/span&gt;Mat img &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; cv&lt;span style=&#34;color:#f92672&#34;&gt;::&lt;/span&gt;imread(loc, &lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;);
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;  &lt;span style=&#34;color:#75715e&#34;&gt;// Convert image to tensor
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#75715e&#34;&gt;&lt;/span&gt;	torch&lt;span style=&#34;color:#f92672&#34;&gt;::&lt;/span&gt;Tensor img_tensor &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; torch&lt;span style=&#34;color:#f92672&#34;&gt;::&lt;/span&gt;from_blob(img.data, {img.rows, img.cols, &lt;span style=&#34;color:#ae81ff&#34;&gt;3&lt;/span&gt;}, torch&lt;span style=&#34;color:#f92672&#34;&gt;::&lt;/span&gt;kByte);
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;	img_tensor &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; img_tensor.permute({&lt;span style=&#34;color:#ae81ff&#34;&gt;2&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;0&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;}); &lt;span style=&#34;color:#75715e&#34;&gt;// Channels x Height x Width
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#75715e&#34;&gt;&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;	&lt;span style=&#34;color:#66d9ef&#34;&gt;return&lt;/span&gt; img_tensor.clone();
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;};
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;&lt;strong&gt;Reading Labels&lt;/strong&gt;:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-cpp&#34; data-lang=&#34;cpp&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#75715e&#34;&gt;// Read Label (int) and convert to torch::Tensor type
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#75715e&#34;&gt;&lt;/span&gt;torch&lt;span style=&#34;color:#f92672&#34;&gt;::&lt;/span&gt;Tensor read_label(&lt;span style=&#34;color:#66d9ef&#34;&gt;int&lt;/span&gt; label) {
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;	torch&lt;span style=&#34;color:#f92672&#34;&gt;::&lt;/span&gt;Tensor label_tensor &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; torch&lt;span style=&#34;color:#f92672&#34;&gt;::&lt;/span&gt;full({&lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;}, label);
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;	&lt;span style=&#34;color:#66d9ef&#34;&gt;return&lt;/span&gt; label_tensor.clone();
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;}
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;h2 id=&#34;final-code&#34;&gt;Final Code&lt;/h2&gt;
&lt;p&gt;Let&amp;rsquo;s wrap up the code!&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-cpp&#34; data-lang=&#34;cpp&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#75715e&#34;&gt;// Include libraries
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#75715e&#34;&gt;&lt;/span&gt;&lt;span style=&#34;color:#75715e&#34;&gt;#include&lt;/span&gt; &lt;span style=&#34;color:#75715e&#34;&gt;&amp;lt;ATen/ATen.h&amp;gt;&lt;/span&gt;&lt;span style=&#34;color:#75715e&#34;&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#75715e&#34;&gt;#include&lt;/span&gt; &lt;span style=&#34;color:#75715e&#34;&gt;&amp;lt;torch/torch.h&amp;gt;&lt;/span&gt;&lt;span style=&#34;color:#75715e&#34;&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#75715e&#34;&gt;#include&lt;/span&gt; &lt;span style=&#34;color:#75715e&#34;&gt;&amp;lt;iostream&amp;gt;&lt;/span&gt;&lt;span style=&#34;color:#75715e&#34;&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#75715e&#34;&gt;#include&lt;/span&gt; &lt;span style=&#34;color:#75715e&#34;&gt;&amp;lt;vector&amp;gt;&lt;/span&gt;&lt;span style=&#34;color:#75715e&#34;&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#75715e&#34;&gt;#include&lt;/span&gt; &lt;span style=&#34;color:#75715e&#34;&gt;&amp;lt;tuple&amp;gt;&lt;/span&gt;&lt;span style=&#34;color:#75715e&#34;&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#75715e&#34;&gt;#include&lt;/span&gt; &lt;span style=&#34;color:#75715e&#34;&gt;&amp;lt;opencv2/opencv.hpp&amp;gt;&lt;/span&gt;&lt;span style=&#34;color:#75715e&#34;&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#75715e&#34;&gt;#include&lt;/span&gt; &lt;span style=&#34;color:#75715e&#34;&gt;&amp;lt;string&amp;gt;&lt;/span&gt;&lt;span style=&#34;color:#75715e&#34;&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#75715e&#34;&gt;&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#75715e&#34;&gt;/* Convert and Load image to tensor from location argument */&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;torch&lt;span style=&#34;color:#f92672&#34;&gt;::&lt;/span&gt;Tensor read_data(std&lt;span style=&#34;color:#f92672&#34;&gt;::&lt;/span&gt;string loc) {
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;  &lt;span style=&#34;color:#75715e&#34;&gt;// Read Data here
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#75715e&#34;&gt;&lt;/span&gt;  &lt;span style=&#34;color:#75715e&#34;&gt;// Return tensor form of the image
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#75715e&#34;&gt;&lt;/span&gt;  cv&lt;span style=&#34;color:#f92672&#34;&gt;::&lt;/span&gt;Mat img &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; cv&lt;span style=&#34;color:#f92672&#34;&gt;::&lt;/span&gt;imread(loc, &lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;);
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;	cv&lt;span style=&#34;color:#f92672&#34;&gt;::&lt;/span&gt;resize(img, img, cv&lt;span style=&#34;color:#f92672&#34;&gt;::&lt;/span&gt;Size(&lt;span style=&#34;color:#ae81ff&#34;&gt;1920&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;1080&lt;/span&gt;), cv&lt;span style=&#34;color:#f92672&#34;&gt;::&lt;/span&gt;INTER_CUBIC);
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;	std&lt;span style=&#34;color:#f92672&#34;&gt;::&lt;/span&gt;cout &lt;span style=&#34;color:#f92672&#34;&gt;&amp;lt;&amp;lt;&lt;/span&gt; &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;Sizes: &amp;#34;&lt;/span&gt; &lt;span style=&#34;color:#f92672&#34;&gt;&amp;lt;&amp;lt;&lt;/span&gt; img.size() &lt;span style=&#34;color:#f92672&#34;&gt;&amp;lt;&amp;lt;&lt;/span&gt; std&lt;span style=&#34;color:#f92672&#34;&gt;::&lt;/span&gt;endl;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;	torch&lt;span style=&#34;color:#f92672&#34;&gt;::&lt;/span&gt;Tensor img_tensor &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; torch&lt;span style=&#34;color:#f92672&#34;&gt;::&lt;/span&gt;from_blob(img.data, {img.rows, img.cols, &lt;span style=&#34;color:#ae81ff&#34;&gt;3&lt;/span&gt;}, torch&lt;span style=&#34;color:#f92672&#34;&gt;::&lt;/span&gt;kByte);
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;	img_tensor &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; img_tensor.permute({&lt;span style=&#34;color:#ae81ff&#34;&gt;2&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;0&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;}); &lt;span style=&#34;color:#75715e&#34;&gt;// Channels x Height x Width
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#75715e&#34;&gt;&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;	&lt;span style=&#34;color:#66d9ef&#34;&gt;return&lt;/span&gt; img_tensor.clone();
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;}
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#75715e&#34;&gt;/* Converts label to tensor type in the integer argument */&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;torch&lt;span style=&#34;color:#f92672&#34;&gt;::&lt;/span&gt;Tensor read_label(&lt;span style=&#34;color:#66d9ef&#34;&gt;int&lt;/span&gt; label) {
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;  &lt;span style=&#34;color:#75715e&#34;&gt;// Read label here
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#75715e&#34;&gt;&lt;/span&gt;  &lt;span style=&#34;color:#75715e&#34;&gt;// Convert to tensor and return
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#75715e&#34;&gt;&lt;/span&gt;  torch&lt;span style=&#34;color:#f92672&#34;&gt;::&lt;/span&gt;Tensor label_tensor &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; torch&lt;span style=&#34;color:#f92672&#34;&gt;::&lt;/span&gt;full({&lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;}, label);
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;	&lt;span style=&#34;color:#66d9ef&#34;&gt;return&lt;/span&gt; label_tensor.clone();
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;}
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#75715e&#34;&gt;/* Loads images to tensor type in the string argument */&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;vector&lt;span style=&#34;color:#f92672&#34;&gt;&amp;lt;&lt;/span&gt;torch&lt;span style=&#34;color:#f92672&#34;&gt;::&lt;/span&gt;Tensor&lt;span style=&#34;color:#f92672&#34;&gt;&amp;gt;&lt;/span&gt; process_images(vector&lt;span style=&#34;color:#f92672&#34;&gt;&amp;lt;&lt;/span&gt;string&lt;span style=&#34;color:#f92672&#34;&gt;&amp;gt;&lt;/span&gt; list_images) {
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;  cout &lt;span style=&#34;color:#f92672&#34;&gt;&amp;lt;&amp;lt;&lt;/span&gt; &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;Reading Images...&amp;#34;&lt;/span&gt; &lt;span style=&#34;color:#f92672&#34;&gt;&amp;lt;&amp;lt;&lt;/span&gt; endl;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;  &lt;span style=&#34;color:#75715e&#34;&gt;// Return vector of Tensor form of all the images
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#75715e&#34;&gt;&lt;/span&gt;  vector&lt;span style=&#34;color:#f92672&#34;&gt;&amp;lt;&lt;/span&gt;torch&lt;span style=&#34;color:#f92672&#34;&gt;::&lt;/span&gt;Tensor&lt;span style=&#34;color:#f92672&#34;&gt;&amp;gt;&lt;/span&gt; states;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;	&lt;span style=&#34;color:#66d9ef&#34;&gt;for&lt;/span&gt; (std&lt;span style=&#34;color:#f92672&#34;&gt;::&lt;/span&gt;vector&lt;span style=&#34;color:#f92672&#34;&gt;&amp;lt;&lt;/span&gt;string&lt;span style=&#34;color:#f92672&#34;&gt;&amp;gt;::&lt;/span&gt;iterator it &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; list_images.begin(); it &lt;span style=&#34;color:#f92672&#34;&gt;!=&lt;/span&gt; list_images.end(); &lt;span style=&#34;color:#f92672&#34;&gt;++&lt;/span&gt;it) {
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;		torch&lt;span style=&#34;color:#f92672&#34;&gt;::&lt;/span&gt;Tensor img &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; read_data(&lt;span style=&#34;color:#f92672&#34;&gt;*&lt;/span&gt;it);
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;		states.push_back(img);
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;	}
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;	&lt;span style=&#34;color:#66d9ef&#34;&gt;return&lt;/span&gt; states;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;}
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#75715e&#34;&gt;/* Loads labels to tensor type in the string argument */&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;vector&lt;span style=&#34;color:#f92672&#34;&gt;&amp;lt;&lt;/span&gt;torch&lt;span style=&#34;color:#f92672&#34;&gt;::&lt;/span&gt;Tensor&lt;span style=&#34;color:#f92672&#34;&gt;&amp;gt;&lt;/span&gt; process_labels(vector&lt;span style=&#34;color:#f92672&#34;&gt;&amp;lt;&lt;/span&gt;string&lt;span style=&#34;color:#f92672&#34;&gt;&amp;gt;&lt;/span&gt; list_labels) {
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;  cout &lt;span style=&#34;color:#f92672&#34;&gt;&amp;lt;&amp;lt;&lt;/span&gt; &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;Reading Labels...&amp;#34;&lt;/span&gt; &lt;span style=&#34;color:#f92672&#34;&gt;&amp;lt;&amp;lt;&lt;/span&gt; endl;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;  &lt;span style=&#34;color:#75715e&#34;&gt;// Return vector of Tensor form of all the labels
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#75715e&#34;&gt;&lt;/span&gt;  vector&lt;span style=&#34;color:#f92672&#34;&gt;&amp;lt;&lt;/span&gt;torch&lt;span style=&#34;color:#f92672&#34;&gt;::&lt;/span&gt;Tensor&lt;span style=&#34;color:#f92672&#34;&gt;&amp;gt;&lt;/span&gt; labels;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;	&lt;span style=&#34;color:#66d9ef&#34;&gt;for&lt;/span&gt; (std&lt;span style=&#34;color:#f92672&#34;&gt;::&lt;/span&gt;vector&lt;span style=&#34;color:#f92672&#34;&gt;&amp;lt;&lt;/span&gt;&lt;span style=&#34;color:#66d9ef&#34;&gt;int&lt;/span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;&amp;gt;::&lt;/span&gt;iterator it &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; list_labels.begin(); it &lt;span style=&#34;color:#f92672&#34;&gt;!=&lt;/span&gt; list_labels.end(); &lt;span style=&#34;color:#f92672&#34;&gt;++&lt;/span&gt;it) {
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;		torch&lt;span style=&#34;color:#f92672&#34;&gt;::&lt;/span&gt;Tensor label &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; read_label(&lt;span style=&#34;color:#f92672&#34;&gt;*&lt;/span&gt;it);
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;		labels.push_back(label);
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;	}
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;	&lt;span style=&#34;color:#66d9ef&#34;&gt;return&lt;/span&gt; labels;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;}
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#66d9ef&#34;&gt;class&lt;/span&gt; &lt;span style=&#34;color:#a6e22e&#34;&gt;CustomDataset&lt;/span&gt; &lt;span style=&#34;color:#f92672&#34;&gt;:&lt;/span&gt; &lt;span style=&#34;color:#66d9ef&#34;&gt;public&lt;/span&gt; torch&lt;span style=&#34;color:#f92672&#34;&gt;::&lt;/span&gt;data&lt;span style=&#34;color:#f92672&#34;&gt;::&lt;/span&gt;dataset&lt;span style=&#34;color:#f92672&#34;&gt;&amp;lt;&lt;/span&gt;CustomDataset&lt;span style=&#34;color:#f92672&#34;&gt;&amp;gt;&lt;/span&gt; {
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#66d9ef&#34;&gt;private&lt;/span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;:&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;  &lt;span style=&#34;color:#75715e&#34;&gt;// Declare 2 vectors of tensors for images and labels
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#75715e&#34;&gt;&lt;/span&gt;  vector&lt;span style=&#34;color:#f92672&#34;&gt;&amp;lt;&lt;/span&gt;torch&lt;span style=&#34;color:#f92672&#34;&gt;::&lt;/span&gt;Tensor&lt;span style=&#34;color:#f92672&#34;&gt;&amp;gt;&lt;/span&gt; images, labels;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#66d9ef&#34;&gt;public&lt;/span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;:&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;  &lt;span style=&#34;color:#75715e&#34;&gt;// Constructor
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#75715e&#34;&gt;&lt;/span&gt;  CustomDataset(vector&lt;span style=&#34;color:#f92672&#34;&gt;&amp;lt;&lt;/span&gt;string&lt;span style=&#34;color:#f92672&#34;&gt;&amp;gt;&lt;/span&gt; list_images, vector&lt;span style=&#34;color:#f92672&#34;&gt;&amp;lt;&lt;/span&gt;string&lt;span style=&#34;color:#f92672&#34;&gt;&amp;gt;&lt;/span&gt; list_labels) {
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    images &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; process_images(list_images);
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    labels &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; process_labels(list_labels);
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;  };
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;  &lt;span style=&#34;color:#75715e&#34;&gt;// Override get() function to return tensor at location index
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#75715e&#34;&gt;&lt;/span&gt;  torch&lt;span style=&#34;color:#f92672&#34;&gt;::&lt;/span&gt;data&lt;span style=&#34;color:#f92672&#34;&gt;::&lt;/span&gt;Example&lt;span style=&#34;color:#f92672&#34;&gt;&amp;lt;&amp;gt;&lt;/span&gt; get(size_t index) &lt;span style=&#34;color:#66d9ef&#34;&gt;override&lt;/span&gt; {
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    torch&lt;span style=&#34;color:#f92672&#34;&gt;::&lt;/span&gt;Tensor sample_img &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; images.at(index);
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    torch&lt;span style=&#34;color:#f92672&#34;&gt;::&lt;/span&gt;Tensor sample_label &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; labels.at(index);
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#66d9ef&#34;&gt;return&lt;/span&gt; {sample_img.clone(), sample_label.clone()};
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;  };
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;  &lt;span style=&#34;color:#75715e&#34;&gt;// Return the length of data
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#75715e&#34;&gt;&lt;/span&gt;  torch&lt;span style=&#34;color:#f92672&#34;&gt;::&lt;/span&gt;optional&lt;span style=&#34;color:#f92672&#34;&gt;&amp;lt;&lt;/span&gt;size_t&lt;span style=&#34;color:#f92672&#34;&gt;&amp;gt;&lt;/span&gt; size() &lt;span style=&#34;color:#66d9ef&#34;&gt;const&lt;/span&gt; &lt;span style=&#34;color:#66d9ef&#34;&gt;override&lt;/span&gt; {
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#66d9ef&#34;&gt;return&lt;/span&gt; labels.size();
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;  };
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;};
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#66d9ef&#34;&gt;int&lt;/span&gt; &lt;span style=&#34;color:#a6e22e&#34;&gt;main&lt;/span&gt;(&lt;span style=&#34;color:#66d9ef&#34;&gt;int&lt;/span&gt; argc, &lt;span style=&#34;color:#66d9ef&#34;&gt;char&lt;/span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;**&lt;/span&gt; argv) {
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;  vector&lt;span style=&#34;color:#f92672&#34;&gt;&amp;lt;&lt;/span&gt;string&lt;span style=&#34;color:#f92672&#34;&gt;&amp;gt;&lt;/span&gt; list_images; &lt;span style=&#34;color:#75715e&#34;&gt;// list of path of images
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#75715e&#34;&gt;&lt;/span&gt;  vector&lt;span style=&#34;color:#f92672&#34;&gt;&amp;lt;&lt;/span&gt;&lt;span style=&#34;color:#66d9ef&#34;&gt;int&lt;/span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;&amp;gt;&lt;/span&gt; list_labels; &lt;span style=&#34;color:#75715e&#34;&gt;// list of integer labels
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#75715e&#34;&gt;&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;  &lt;span style=&#34;color:#75715e&#34;&gt;// Dataset init and apply transforms - None!
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#75715e&#34;&gt;&lt;/span&gt;  &lt;span style=&#34;color:#66d9ef&#34;&gt;auto&lt;/span&gt; custom_dataset &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; CustomDataset(list_images, list_labels).map(torch&lt;span style=&#34;color:#f92672&#34;&gt;::&lt;/span&gt;data&lt;span style=&#34;color:#f92672&#34;&gt;::&lt;/span&gt;transforms&lt;span style=&#34;color:#f92672&#34;&gt;::&lt;/span&gt;Stack&lt;span style=&#34;color:#f92672&#34;&gt;&amp;lt;&amp;gt;&lt;/span&gt;());
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;}
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;That&amp;rsquo;s it for today! In the next blog, we&amp;rsquo;ll use this custom data loader and implement a CNN on our data. By then, happy learning. Hope you liked this blog. :)&lt;/p&gt;

      </description>
    </item>
    
    <item>
      <title>Introduction to PyTorch C&#43;&#43; API: MNIST Digit Recognition using VGG-16 Network</title>
      <link>https://krshrimali.github.io/posts/2019/06/introduction-to-pytorch-c-api-mnist-digit-recognition-using-vgg-16-network/</link>
      <pubDate>Fri, 07 Jun 2019 00:00:00 +0000</pubDate>
      
      <guid>https://krshrimali.github.io/posts/2019/06/introduction-to-pytorch-c-api-mnist-digit-recognition-using-vgg-16-network/</guid>
      <description>
          
            &lt;h4&gt;Introduction to PyTorch C++ API: MNIST Digit Recognition using VGG-16 Network&lt;/h4&gt;
          
          &lt;h1 id=&#34;environment-setup-ubuntu-1604-1804&#34;&gt;Environment Setup [Ubuntu 16.04, 18.04]&lt;/h1&gt;
&lt;p&gt;&lt;em&gt;Note: If you have already finished installing PyTorch C++ API, please skip this section.&lt;/em&gt;&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;Download &lt;code&gt;libtorch&lt;/code&gt;:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;CPU Version: &lt;code&gt;wget https://download.pytorch.org/libtorch/cpu/libtorch-shared-with-deps-latest.zip -O libtorch.zip&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;GPU Version (CUDA 9.0): &lt;code&gt;wget https://download.pytorch.org/libtorch/cu90/libtorch-shared-with-deps-latest.zip -O libtorch.zip&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;GPU Version (CUDA 10.0): &lt;code&gt;wget https://download.pytorch.org/libtorch/cu100/libtorch-shared-with-deps-latest.zip&lt;/code&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Unzip &lt;code&gt;libtorch.zip&lt;/code&gt;:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;code&gt;unzip libtorch.zip&lt;/code&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;We&amp;rsquo;ll use the &lt;code&gt;absolute&lt;/code&gt; path of extracted directory (&lt;code&gt;libtorch&lt;/code&gt;) later on.&lt;/p&gt;
&lt;h1 id=&#34;implementation&#34;&gt;Implementation&lt;/h1&gt;
&lt;p&gt;The VGG-16 Network is shown in the Figure below.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/krshrimali/blog/main/assets/blogs/VGG-16-Architecture-resized.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt;
&lt;p&gt;We&amp;rsquo;ll start of by first including &lt;code&gt;libtorch&lt;/code&gt; header file.&lt;/p&gt;
&lt;p&gt;&lt;code&gt;#include &amp;lt;torch/torch.h&amp;gt;&lt;/code&gt;&lt;/p&gt;
&lt;p&gt;We&amp;rsquo;ll then go ahead and define the network. We&amp;rsquo;ll inherit layers from &lt;code&gt;torch::nn::Module&lt;/code&gt;.&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-cpp&#34; data-lang=&#34;cpp&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#75715e&#34;&gt;/* Sample code for training a FCN on MNIST dataset using PyTorch C++ API */&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#75715e&#34;&gt;/* This code uses VGG-16 Layer Network */&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#66d9ef&#34;&gt;struct&lt;/span&gt; &lt;span style=&#34;color:#a6e22e&#34;&gt;Net&lt;/span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;:&lt;/span&gt; torch&lt;span style=&#34;color:#f92672&#34;&gt;::&lt;/span&gt;nn&lt;span style=&#34;color:#f92672&#34;&gt;::&lt;/span&gt;Module {
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#75715e&#34;&gt;// VGG-16 Layer
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#75715e&#34;&gt;&lt;/span&gt;    &lt;span style=&#34;color:#75715e&#34;&gt;// conv1_1 - conv1_2 - pool 1 - conv2_1 - conv2_2 - pool 2 - conv3_1 - conv3_2 - conv3_3 - pool 3 -
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#75715e&#34;&gt;&lt;/span&gt;    &lt;span style=&#34;color:#75715e&#34;&gt;// conv4_1 - conv4_2 - conv4_3 - pool 4 - conv5_1 - conv5_2 - conv5_3 - pool 5 - fc6 - fc7 - fc8
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#75715e&#34;&gt;&lt;/span&gt;    
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#75715e&#34;&gt;// Note: pool 5 not implemented as no need for MNIST dataset
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#75715e&#34;&gt;&lt;/span&gt;    Net() {
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        &lt;span style=&#34;color:#75715e&#34;&gt;// Initialize VGG-16
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#75715e&#34;&gt;&lt;/span&gt;        &lt;span style=&#34;color:#75715e&#34;&gt;// On how to pass strides and padding: https://github.com/pytorch/pytorch/issues/12649#issuecomment-430156160
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#75715e&#34;&gt;&lt;/span&gt;        conv1_1 &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; register_module(&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;conv1_1&amp;#34;&lt;/span&gt;, torch&lt;span style=&#34;color:#f92672&#34;&gt;::&lt;/span&gt;nn&lt;span style=&#34;color:#f92672&#34;&gt;::&lt;/span&gt;Conv2d(torch&lt;span style=&#34;color:#f92672&#34;&gt;::&lt;/span&gt;nn&lt;span style=&#34;color:#f92672&#34;&gt;::&lt;/span&gt;Conv2dOptions(&lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;10&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;3&lt;/span&gt;).padding(&lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;)));
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        conv1_2 &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; register_module(&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;conv1_2&amp;#34;&lt;/span&gt;, torch&lt;span style=&#34;color:#f92672&#34;&gt;::&lt;/span&gt;nn&lt;span style=&#34;color:#f92672&#34;&gt;::&lt;/span&gt;Conv2d(torch&lt;span style=&#34;color:#f92672&#34;&gt;::&lt;/span&gt;nn&lt;span style=&#34;color:#f92672&#34;&gt;::&lt;/span&gt;Conv2dOptions(&lt;span style=&#34;color:#ae81ff&#34;&gt;10&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;20&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;3&lt;/span&gt;).padding(&lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;)));
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        &lt;span style=&#34;color:#75715e&#34;&gt;// Insert pool layer
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#75715e&#34;&gt;&lt;/span&gt;        conv2_1 &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; register_module(&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;conv2_1&amp;#34;&lt;/span&gt;, torch&lt;span style=&#34;color:#f92672&#34;&gt;::&lt;/span&gt;nn&lt;span style=&#34;color:#f92672&#34;&gt;::&lt;/span&gt;Conv2d(torch&lt;span style=&#34;color:#f92672&#34;&gt;::&lt;/span&gt;nn&lt;span style=&#34;color:#f92672&#34;&gt;::&lt;/span&gt;Conv2dOptions(&lt;span style=&#34;color:#ae81ff&#34;&gt;20&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;30&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;3&lt;/span&gt;).padding(&lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;)));
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        conv2_2 &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; register_module(&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;conv2_2&amp;#34;&lt;/span&gt;, torch&lt;span style=&#34;color:#f92672&#34;&gt;::&lt;/span&gt;nn&lt;span style=&#34;color:#f92672&#34;&gt;::&lt;/span&gt;Conv2d(torch&lt;span style=&#34;color:#f92672&#34;&gt;::&lt;/span&gt;nn&lt;span style=&#34;color:#f92672&#34;&gt;::&lt;/span&gt;Conv2dOptions(&lt;span style=&#34;color:#ae81ff&#34;&gt;30&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;40&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;3&lt;/span&gt;).padding(&lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;)));
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        &lt;span style=&#34;color:#75715e&#34;&gt;// Insert pool layer
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#75715e&#34;&gt;&lt;/span&gt;        conv3_1 &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; register_module(&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;conv3_1&amp;#34;&lt;/span&gt;, torch&lt;span style=&#34;color:#f92672&#34;&gt;::&lt;/span&gt;nn&lt;span style=&#34;color:#f92672&#34;&gt;::&lt;/span&gt;Conv2d(torch&lt;span style=&#34;color:#f92672&#34;&gt;::&lt;/span&gt;nn&lt;span style=&#34;color:#f92672&#34;&gt;::&lt;/span&gt;Conv2dOptions(&lt;span style=&#34;color:#ae81ff&#34;&gt;40&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;50&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;3&lt;/span&gt;).padding(&lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;)));
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        conv3_2 &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; register_module(&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;conv3_2&amp;#34;&lt;/span&gt;, torch&lt;span style=&#34;color:#f92672&#34;&gt;::&lt;/span&gt;nn&lt;span style=&#34;color:#f92672&#34;&gt;::&lt;/span&gt;Conv2d(torch&lt;span style=&#34;color:#f92672&#34;&gt;::&lt;/span&gt;nn&lt;span style=&#34;color:#f92672&#34;&gt;::&lt;/span&gt;Conv2dOptions(&lt;span style=&#34;color:#ae81ff&#34;&gt;50&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;60&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;3&lt;/span&gt;).padding(&lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;)));
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        conv3_3 &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; register_module(&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;conv3_3&amp;#34;&lt;/span&gt;, torch&lt;span style=&#34;color:#f92672&#34;&gt;::&lt;/span&gt;nn&lt;span style=&#34;color:#f92672&#34;&gt;::&lt;/span&gt;Conv2d(torch&lt;span style=&#34;color:#f92672&#34;&gt;::&lt;/span&gt;nn&lt;span style=&#34;color:#f92672&#34;&gt;::&lt;/span&gt;Conv2dOptions(&lt;span style=&#34;color:#ae81ff&#34;&gt;60&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;70&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;3&lt;/span&gt;).padding(&lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;)));
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        &lt;span style=&#34;color:#75715e&#34;&gt;// Insert pool layer
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#75715e&#34;&gt;&lt;/span&gt;        conv4_1 &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; register_module(&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;conv4_1&amp;#34;&lt;/span&gt;, torch&lt;span style=&#34;color:#f92672&#34;&gt;::&lt;/span&gt;nn&lt;span style=&#34;color:#f92672&#34;&gt;::&lt;/span&gt;Conv2d(torch&lt;span style=&#34;color:#f92672&#34;&gt;::&lt;/span&gt;nn&lt;span style=&#34;color:#f92672&#34;&gt;::&lt;/span&gt;Conv2dOptions(&lt;span style=&#34;color:#ae81ff&#34;&gt;70&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;80&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;3&lt;/span&gt;).padding(&lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;)));
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        conv4_2 &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; register_module(&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;conv4_2&amp;#34;&lt;/span&gt;, torch&lt;span style=&#34;color:#f92672&#34;&gt;::&lt;/span&gt;nn&lt;span style=&#34;color:#f92672&#34;&gt;::&lt;/span&gt;Conv2d(torch&lt;span style=&#34;color:#f92672&#34;&gt;::&lt;/span&gt;nn&lt;span style=&#34;color:#f92672&#34;&gt;::&lt;/span&gt;Conv2dOptions(&lt;span style=&#34;color:#ae81ff&#34;&gt;80&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;90&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;3&lt;/span&gt;).padding(&lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;)));
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        conv4_3 &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; register_module(&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;conv4_3&amp;#34;&lt;/span&gt;, torch&lt;span style=&#34;color:#f92672&#34;&gt;::&lt;/span&gt;nn&lt;span style=&#34;color:#f92672&#34;&gt;::&lt;/span&gt;Conv2d(torch&lt;span style=&#34;color:#f92672&#34;&gt;::&lt;/span&gt;nn&lt;span style=&#34;color:#f92672&#34;&gt;::&lt;/span&gt;Conv2dOptions(&lt;span style=&#34;color:#ae81ff&#34;&gt;90&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;100&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;3&lt;/span&gt;).padding(&lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;)));
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        &lt;span style=&#34;color:#75715e&#34;&gt;// Insert pool layer
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#75715e&#34;&gt;&lt;/span&gt;        conv5_1 &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; register_module(&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;conv5_1&amp;#34;&lt;/span&gt;, torch&lt;span style=&#34;color:#f92672&#34;&gt;::&lt;/span&gt;nn&lt;span style=&#34;color:#f92672&#34;&gt;::&lt;/span&gt;Conv2d(torch&lt;span style=&#34;color:#f92672&#34;&gt;::&lt;/span&gt;nn&lt;span style=&#34;color:#f92672&#34;&gt;::&lt;/span&gt;Conv2dOptions(&lt;span style=&#34;color:#ae81ff&#34;&gt;100&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;110&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;3&lt;/span&gt;).padding(&lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;)));
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        conv5_2 &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; register_module(&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;conv5_2&amp;#34;&lt;/span&gt;, torch&lt;span style=&#34;color:#f92672&#34;&gt;::&lt;/span&gt;nn&lt;span style=&#34;color:#f92672&#34;&gt;::&lt;/span&gt;Conv2d(torch&lt;span style=&#34;color:#f92672&#34;&gt;::&lt;/span&gt;nn&lt;span style=&#34;color:#f92672&#34;&gt;::&lt;/span&gt;Conv2dOptions(&lt;span style=&#34;color:#ae81ff&#34;&gt;110&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;120&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;3&lt;/span&gt;).padding(&lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;)));
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        conv5_3 &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; register_module(&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;conv5_3&amp;#34;&lt;/span&gt;, torch&lt;span style=&#34;color:#f92672&#34;&gt;::&lt;/span&gt;nn&lt;span style=&#34;color:#f92672&#34;&gt;::&lt;/span&gt;Conv2d(torch&lt;span style=&#34;color:#f92672&#34;&gt;::&lt;/span&gt;nn&lt;span style=&#34;color:#f92672&#34;&gt;::&lt;/span&gt;Conv2dOptions(&lt;span style=&#34;color:#ae81ff&#34;&gt;120&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;130&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;3&lt;/span&gt;).padding(&lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;)));
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        &lt;span style=&#34;color:#75715e&#34;&gt;// Insert pool layer
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#75715e&#34;&gt;&lt;/span&gt;        fc1 &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; register_module(&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;fc1&amp;#34;&lt;/span&gt;, torch&lt;span style=&#34;color:#f92672&#34;&gt;::&lt;/span&gt;nn&lt;span style=&#34;color:#f92672&#34;&gt;::&lt;/span&gt;Linear(&lt;span style=&#34;color:#ae81ff&#34;&gt;130&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;50&lt;/span&gt;));
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        fc2 &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; register_module(&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;fc2&amp;#34;&lt;/span&gt;, torch&lt;span style=&#34;color:#f92672&#34;&gt;::&lt;/span&gt;nn&lt;span style=&#34;color:#f92672&#34;&gt;::&lt;/span&gt;Linear(&lt;span style=&#34;color:#ae81ff&#34;&gt;50&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;20&lt;/span&gt;));
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        fc3 &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; register_module(&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;fc3&amp;#34;&lt;/span&gt;, torch&lt;span style=&#34;color:#f92672&#34;&gt;::&lt;/span&gt;nn&lt;span style=&#34;color:#f92672&#34;&gt;::&lt;/span&gt;Linear(&lt;span style=&#34;color:#ae81ff&#34;&gt;20&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;10&lt;/span&gt;));
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    }
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#75715e&#34;&gt;// Implement Algorithm
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#75715e&#34;&gt;&lt;/span&gt;    torch&lt;span style=&#34;color:#f92672&#34;&gt;::&lt;/span&gt;Tensor forward(torch&lt;span style=&#34;color:#f92672&#34;&gt;::&lt;/span&gt;Tensor x) {
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        x &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; torch&lt;span style=&#34;color:#f92672&#34;&gt;::&lt;/span&gt;relu(conv1_1&lt;span style=&#34;color:#f92672&#34;&gt;-&amp;gt;&lt;/span&gt;forward(x));
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        x &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; torch&lt;span style=&#34;color:#f92672&#34;&gt;::&lt;/span&gt;relu(conv1_2&lt;span style=&#34;color:#f92672&#34;&gt;-&amp;gt;&lt;/span&gt;forward(x));
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        x &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; torch&lt;span style=&#34;color:#f92672&#34;&gt;::&lt;/span&gt;max_pool2d(x, &lt;span style=&#34;color:#ae81ff&#34;&gt;2&lt;/span&gt;);
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        x &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; torch&lt;span style=&#34;color:#f92672&#34;&gt;::&lt;/span&gt;relu(conv2_1&lt;span style=&#34;color:#f92672&#34;&gt;-&amp;gt;&lt;/span&gt;forward(x));
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        x &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; torch&lt;span style=&#34;color:#f92672&#34;&gt;::&lt;/span&gt;relu(conv2_2&lt;span style=&#34;color:#f92672&#34;&gt;-&amp;gt;&lt;/span&gt;forward(x));
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        x &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; torch&lt;span style=&#34;color:#f92672&#34;&gt;::&lt;/span&gt;max_pool2d(x, &lt;span style=&#34;color:#ae81ff&#34;&gt;2&lt;/span&gt;);
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        x &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; torch&lt;span style=&#34;color:#f92672&#34;&gt;::&lt;/span&gt;relu(conv3_1&lt;span style=&#34;color:#f92672&#34;&gt;-&amp;gt;&lt;/span&gt;forward(x));
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        x &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; torch&lt;span style=&#34;color:#f92672&#34;&gt;::&lt;/span&gt;relu(conv3_2&lt;span style=&#34;color:#f92672&#34;&gt;-&amp;gt;&lt;/span&gt;forward(x));
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        x &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; torch&lt;span style=&#34;color:#f92672&#34;&gt;::&lt;/span&gt;relu(conv3_3&lt;span style=&#34;color:#f92672&#34;&gt;-&amp;gt;&lt;/span&gt;forward(x));
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        x &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; torch&lt;span style=&#34;color:#f92672&#34;&gt;::&lt;/span&gt;max_pool2d(x, &lt;span style=&#34;color:#ae81ff&#34;&gt;2&lt;/span&gt;);
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        x &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; torch&lt;span style=&#34;color:#f92672&#34;&gt;::&lt;/span&gt;relu(conv4_1&lt;span style=&#34;color:#f92672&#34;&gt;-&amp;gt;&lt;/span&gt;forward(x));
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        x &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; torch&lt;span style=&#34;color:#f92672&#34;&gt;::&lt;/span&gt;relu(conv4_2&lt;span style=&#34;color:#f92672&#34;&gt;-&amp;gt;&lt;/span&gt;forward(x));
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        x &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; torch&lt;span style=&#34;color:#f92672&#34;&gt;::&lt;/span&gt;relu(conv4_3&lt;span style=&#34;color:#f92672&#34;&gt;-&amp;gt;&lt;/span&gt;forward(x));
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        x &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; torch&lt;span style=&#34;color:#f92672&#34;&gt;::&lt;/span&gt;max_pool2d(x, &lt;span style=&#34;color:#ae81ff&#34;&gt;2&lt;/span&gt;);
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        x &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; torch&lt;span style=&#34;color:#f92672&#34;&gt;::&lt;/span&gt;relu(conv5_1&lt;span style=&#34;color:#f92672&#34;&gt;-&amp;gt;&lt;/span&gt;forward(x));
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        x &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; torch&lt;span style=&#34;color:#f92672&#34;&gt;::&lt;/span&gt;relu(conv5_2&lt;span style=&#34;color:#f92672&#34;&gt;-&amp;gt;&lt;/span&gt;forward(x));
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        x &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; torch&lt;span style=&#34;color:#f92672&#34;&gt;::&lt;/span&gt;relu(conv5_3&lt;span style=&#34;color:#f92672&#34;&gt;-&amp;gt;&lt;/span&gt;forward(x));
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        x &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; x.view({&lt;span style=&#34;color:#f92672&#34;&gt;-&lt;/span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;130&lt;/span&gt;});
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        x &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; torch&lt;span style=&#34;color:#f92672&#34;&gt;::&lt;/span&gt;relu(fc1&lt;span style=&#34;color:#f92672&#34;&gt;-&amp;gt;&lt;/span&gt;forward(x));
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        x &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; torch&lt;span style=&#34;color:#f92672&#34;&gt;::&lt;/span&gt;relu(fc2&lt;span style=&#34;color:#f92672&#34;&gt;-&amp;gt;&lt;/span&gt;forward(x));
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        x &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; fc3&lt;span style=&#34;color:#f92672&#34;&gt;-&amp;gt;&lt;/span&gt;forward(x);
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        &lt;span style=&#34;color:#66d9ef&#34;&gt;return&lt;/span&gt; torch&lt;span style=&#34;color:#f92672&#34;&gt;::&lt;/span&gt;log_softmax(x, &lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;);
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    }
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#75715e&#34;&gt;// Declare layers
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#75715e&#34;&gt;&lt;/span&gt;    torch&lt;span style=&#34;color:#f92672&#34;&gt;::&lt;/span&gt;nn&lt;span style=&#34;color:#f92672&#34;&gt;::&lt;/span&gt;Conv2d conv1_1{&lt;span style=&#34;color:#66d9ef&#34;&gt;nullptr&lt;/span&gt;};
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    torch&lt;span style=&#34;color:#f92672&#34;&gt;::&lt;/span&gt;nn&lt;span style=&#34;color:#f92672&#34;&gt;::&lt;/span&gt;Conv2d conv1_2{&lt;span style=&#34;color:#66d9ef&#34;&gt;nullptr&lt;/span&gt;};
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    torch&lt;span style=&#34;color:#f92672&#34;&gt;::&lt;/span&gt;nn&lt;span style=&#34;color:#f92672&#34;&gt;::&lt;/span&gt;Conv2d conv2_1{&lt;span style=&#34;color:#66d9ef&#34;&gt;nullptr&lt;/span&gt;};
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    torch&lt;span style=&#34;color:#f92672&#34;&gt;::&lt;/span&gt;nn&lt;span style=&#34;color:#f92672&#34;&gt;::&lt;/span&gt;Conv2d conv2_2{&lt;span style=&#34;color:#66d9ef&#34;&gt;nullptr&lt;/span&gt;};
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    torch&lt;span style=&#34;color:#f92672&#34;&gt;::&lt;/span&gt;nn&lt;span style=&#34;color:#f92672&#34;&gt;::&lt;/span&gt;Conv2d conv3_1{&lt;span style=&#34;color:#66d9ef&#34;&gt;nullptr&lt;/span&gt;};
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    torch&lt;span style=&#34;color:#f92672&#34;&gt;::&lt;/span&gt;nn&lt;span style=&#34;color:#f92672&#34;&gt;::&lt;/span&gt;Conv2d conv3_2{&lt;span style=&#34;color:#66d9ef&#34;&gt;nullptr&lt;/span&gt;};
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    torch&lt;span style=&#34;color:#f92672&#34;&gt;::&lt;/span&gt;nn&lt;span style=&#34;color:#f92672&#34;&gt;::&lt;/span&gt;Conv2d conv3_3{&lt;span style=&#34;color:#66d9ef&#34;&gt;nullptr&lt;/span&gt;};
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    torch&lt;span style=&#34;color:#f92672&#34;&gt;::&lt;/span&gt;nn&lt;span style=&#34;color:#f92672&#34;&gt;::&lt;/span&gt;Conv2d conv4_1{&lt;span style=&#34;color:#66d9ef&#34;&gt;nullptr&lt;/span&gt;};
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    torch&lt;span style=&#34;color:#f92672&#34;&gt;::&lt;/span&gt;nn&lt;span style=&#34;color:#f92672&#34;&gt;::&lt;/span&gt;Conv2d conv4_2{&lt;span style=&#34;color:#66d9ef&#34;&gt;nullptr&lt;/span&gt;};
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    torch&lt;span style=&#34;color:#f92672&#34;&gt;::&lt;/span&gt;nn&lt;span style=&#34;color:#f92672&#34;&gt;::&lt;/span&gt;Conv2d conv4_3{&lt;span style=&#34;color:#66d9ef&#34;&gt;nullptr&lt;/span&gt;};
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    torch&lt;span style=&#34;color:#f92672&#34;&gt;::&lt;/span&gt;nn&lt;span style=&#34;color:#f92672&#34;&gt;::&lt;/span&gt;Conv2d conv5_1{&lt;span style=&#34;color:#66d9ef&#34;&gt;nullptr&lt;/span&gt;};
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    torch&lt;span style=&#34;color:#f92672&#34;&gt;::&lt;/span&gt;nn&lt;span style=&#34;color:#f92672&#34;&gt;::&lt;/span&gt;Conv2d conv5_2{&lt;span style=&#34;color:#66d9ef&#34;&gt;nullptr&lt;/span&gt;};
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    torch&lt;span style=&#34;color:#f92672&#34;&gt;::&lt;/span&gt;nn&lt;span style=&#34;color:#f92672&#34;&gt;::&lt;/span&gt;Conv2d conv5_3{&lt;span style=&#34;color:#66d9ef&#34;&gt;nullptr&lt;/span&gt;};
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    torch&lt;span style=&#34;color:#f92672&#34;&gt;::&lt;/span&gt;nn&lt;span style=&#34;color:#f92672&#34;&gt;::&lt;/span&gt;Linear fc1{&lt;span style=&#34;color:#66d9ef&#34;&gt;nullptr&lt;/span&gt;}, fc2{&lt;span style=&#34;color:#66d9ef&#34;&gt;nullptr&lt;/span&gt;}, fc3{&lt;span style=&#34;color:#66d9ef&#34;&gt;nullptr&lt;/span&gt;};
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;};
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;Once done, we can go ahead and test the network on our sample dataset. Let&amp;rsquo;s go ahead and load data first. We&amp;rsquo;ll be using 10 epochs, learning rate (0.01), and &lt;code&gt;nll_loss&lt;/code&gt; as loss functio.&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-cpp&#34; data-lang=&#34;cpp&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#66d9ef&#34;&gt;int&lt;/span&gt; &lt;span style=&#34;color:#a6e22e&#34;&gt;main&lt;/span&gt;() {
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;	&lt;span style=&#34;color:#75715e&#34;&gt;// Create multi-threaded data loader for MNIST data
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#75715e&#34;&gt;&lt;/span&gt;	&lt;span style=&#34;color:#66d9ef&#34;&gt;auto&lt;/span&gt; data_loader &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; torch&lt;span style=&#34;color:#f92672&#34;&gt;::&lt;/span&gt;data&lt;span style=&#34;color:#f92672&#34;&gt;::&lt;/span&gt;make_data_loader&lt;span style=&#34;color:#f92672&#34;&gt;&amp;lt;&lt;/span&gt;torch&lt;span style=&#34;color:#f92672&#34;&gt;::&lt;/span&gt;data&lt;span style=&#34;color:#f92672&#34;&gt;::&lt;/span&gt;samplers&lt;span style=&#34;color:#f92672&#34;&gt;::&lt;/span&gt;SequentialSampler&lt;span style=&#34;color:#f92672&#34;&gt;&amp;gt;&lt;/span&gt;(
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;			std&lt;span style=&#34;color:#f92672&#34;&gt;::&lt;/span&gt;move(torch&lt;span style=&#34;color:#f92672&#34;&gt;::&lt;/span&gt;data&lt;span style=&#34;color:#f92672&#34;&gt;::&lt;/span&gt;datasets&lt;span style=&#34;color:#f92672&#34;&gt;::&lt;/span&gt;MNIST(&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;/absolute/path/to/data&amp;#34;&lt;/span&gt;).map(torch&lt;span style=&#34;color:#f92672&#34;&gt;::&lt;/span&gt;data&lt;span style=&#34;color:#f92672&#34;&gt;::&lt;/span&gt;transforms&lt;span style=&#34;color:#f92672&#34;&gt;::&lt;/span&gt;Normalize&lt;span style=&#34;color:#f92672&#34;&gt;&amp;lt;&amp;gt;&lt;/span&gt;(&lt;span style=&#34;color:#ae81ff&#34;&gt;0.13707&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;0.3081&lt;/span&gt;)).map(
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;				torch&lt;span style=&#34;color:#f92672&#34;&gt;::&lt;/span&gt;data&lt;span style=&#34;color:#f92672&#34;&gt;::&lt;/span&gt;transforms&lt;span style=&#34;color:#f92672&#34;&gt;::&lt;/span&gt;Stack&lt;span style=&#34;color:#f92672&#34;&gt;&amp;lt;&amp;gt;&lt;/span&gt;())), &lt;span style=&#34;color:#ae81ff&#34;&gt;64&lt;/span&gt;);
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;	
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#75715e&#34;&gt;// Build VGG-16 Network
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#75715e&#34;&gt;&lt;/span&gt;    &lt;span style=&#34;color:#66d9ef&#34;&gt;auto&lt;/span&gt; net &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; std&lt;span style=&#34;color:#f92672&#34;&gt;::&lt;/span&gt;make_shared&lt;span style=&#34;color:#f92672&#34;&gt;&amp;lt;&lt;/span&gt;Net&lt;span style=&#34;color:#f92672&#34;&gt;&amp;gt;&lt;/span&gt;();
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    torch&lt;span style=&#34;color:#f92672&#34;&gt;::&lt;/span&gt;optim&lt;span style=&#34;color:#f92672&#34;&gt;::&lt;/span&gt;SGD optimizer(net&lt;span style=&#34;color:#f92672&#34;&gt;-&amp;gt;&lt;/span&gt;parameters(), &lt;span style=&#34;color:#ae81ff&#34;&gt;0.01&lt;/span&gt;); &lt;span style=&#34;color:#75715e&#34;&gt;// Learning Rate 0.01
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#75715e&#34;&gt;&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;	&lt;span style=&#34;color:#75715e&#34;&gt;// net.train();
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#75715e&#34;&gt;&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;	&lt;span style=&#34;color:#66d9ef&#34;&gt;for&lt;/span&gt;(size_t epoch&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;; epoch&lt;span style=&#34;color:#f92672&#34;&gt;&amp;lt;=&lt;/span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;10&lt;/span&gt;; &lt;span style=&#34;color:#f92672&#34;&gt;++&lt;/span&gt;epoch) {
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;		size_t batch_index &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; &lt;span style=&#34;color:#ae81ff&#34;&gt;0&lt;/span&gt;;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;		&lt;span style=&#34;color:#75715e&#34;&gt;// Iterate data loader to yield batches from the dataset
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#75715e&#34;&gt;&lt;/span&gt;		&lt;span style=&#34;color:#66d9ef&#34;&gt;for&lt;/span&gt; (&lt;span style=&#34;color:#66d9ef&#34;&gt;auto&lt;/span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;&amp;amp;&lt;/span&gt; batch: &lt;span style=&#34;color:#f92672&#34;&gt;*&lt;/span&gt;data_loader) {
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;			&lt;span style=&#34;color:#75715e&#34;&gt;// Reset gradients
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#75715e&#34;&gt;&lt;/span&gt;			optimizer.zero_grad();
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;			&lt;span style=&#34;color:#75715e&#34;&gt;// Execute the model
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#75715e&#34;&gt;&lt;/span&gt;			torch&lt;span style=&#34;color:#f92672&#34;&gt;::&lt;/span&gt;Tensor prediction &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; net&lt;span style=&#34;color:#f92672&#34;&gt;-&amp;gt;&lt;/span&gt;forward(batch.data);
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;			&lt;span style=&#34;color:#75715e&#34;&gt;// Compute loss value
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#75715e&#34;&gt;&lt;/span&gt;			torch&lt;span style=&#34;color:#f92672&#34;&gt;::&lt;/span&gt;Tensor loss &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; torch&lt;span style=&#34;color:#f92672&#34;&gt;::&lt;/span&gt;nll_loss(prediction, batch.target);
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;			&lt;span style=&#34;color:#75715e&#34;&gt;// Compute gradients
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#75715e&#34;&gt;&lt;/span&gt;			loss.backward();
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;			&lt;span style=&#34;color:#75715e&#34;&gt;// Update the parameters
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#75715e&#34;&gt;&lt;/span&gt;			optimizer.step();
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;			&lt;span style=&#34;color:#75715e&#34;&gt;// Output the loss and checkpoint every 100 batches
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#75715e&#34;&gt;&lt;/span&gt;			&lt;span style=&#34;color:#66d9ef&#34;&gt;if&lt;/span&gt; (&lt;span style=&#34;color:#f92672&#34;&gt;++&lt;/span&gt;batch_index &lt;span style=&#34;color:#f92672&#34;&gt;%&lt;/span&gt; &lt;span style=&#34;color:#ae81ff&#34;&gt;2&lt;/span&gt; &lt;span style=&#34;color:#f92672&#34;&gt;==&lt;/span&gt; &lt;span style=&#34;color:#ae81ff&#34;&gt;0&lt;/span&gt;) {
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;				std&lt;span style=&#34;color:#f92672&#34;&gt;::&lt;/span&gt;cout &lt;span style=&#34;color:#f92672&#34;&gt;&amp;lt;&amp;lt;&lt;/span&gt; &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;Epoch: &amp;#34;&lt;/span&gt; &lt;span style=&#34;color:#f92672&#34;&gt;&amp;lt;&amp;lt;&lt;/span&gt; epoch &lt;span style=&#34;color:#f92672&#34;&gt;&amp;lt;&amp;lt;&lt;/span&gt; &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34; | Batch: &amp;#34;&lt;/span&gt; &lt;span style=&#34;color:#f92672&#34;&gt;&amp;lt;&amp;lt;&lt;/span&gt; batch_index 
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;					&lt;span style=&#34;color:#f92672&#34;&gt;&amp;lt;&amp;lt;&lt;/span&gt; &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34; | Loss: &amp;#34;&lt;/span&gt; &lt;span style=&#34;color:#f92672&#34;&gt;&amp;lt;&amp;lt;&lt;/span&gt; loss.item&lt;span style=&#34;color:#f92672&#34;&gt;&amp;lt;&lt;/span&gt;&lt;span style=&#34;color:#66d9ef&#34;&gt;float&lt;/span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;&amp;gt;&lt;/span&gt;() &lt;span style=&#34;color:#f92672&#34;&gt;&amp;lt;&amp;lt;&lt;/span&gt; std&lt;span style=&#34;color:#f92672&#34;&gt;::&lt;/span&gt;endl;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;				torch&lt;span style=&#34;color:#f92672&#34;&gt;::&lt;/span&gt;save(net, &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;net.pt&amp;#34;&lt;/span&gt;);
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;			}
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;		}
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;	}
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;}
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;For code, check out my repo here: &lt;a href=&#34;https://github.com/krshrimali/Digit-Recognition-MNIST-SVHN-PyTorch-CPP&#34;&gt;https://github.com/krshrimali/Digit-Recognition-MNIST-SVHN-PyTorch-CPP&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;In the next blog, we will discuss about another network on MNIST and SVHN Dataset.&lt;/p&gt;
&lt;h1 id=&#34;references&#34;&gt;References&lt;/h1&gt;
&lt;ol&gt;
&lt;li&gt;&lt;a href=&#34;https://pytorch.org/cppdocs/&#34;&gt;https://pytorch.org/cppdocs/&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;http://yann.lecun.com/exdb/mnist/&#34;&gt;http://yann.lecun.com/exdb/mnist/&lt;/a&gt;&lt;/li&gt;
&lt;/ol&gt;

      </description>
    </item>
    
  </channel>
</rss>
