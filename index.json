[{"categories":["weekly updates"],"contents":"Hi everyone, before I go ahead and share my progress, I wanted to quickly talk about what this blog is about.\nI am highly passionate with the idea of high performance computing, optimizing deep learning applications, and solving real world problems using deep learning, computer vision, and speech processing. While I\u0026rsquo;m on this path - I would like to document this somewhere. And while I\u0026rsquo;m doing that, why not share it publicly?\nSo here it is, the first weekly update blog on what I\u0026rsquo;ve been upto:\nPyTorch: (ft. Quansight and Facebook)\n Followed up on batch_norm PR. Merged ðŸŽ‰ Structured Kernel porting for baddbmm, bmm PR. Lots of interesting discussions, and should be approved after final round of review. Will have to follow up on this coming week. Structured Kernel porting (WIP) for nonzero PR. Putting this on hold, as there is a related PR on this - and it makes more sense to let that one go first, and then start working on this. Finished the max_poolNd PR, the PR is approved and will be merged after some time. While working on structured kernel for baddbmm, bmm PR, we needed to differentiate between in-place and out calls, interestingly - the autogenerated set_output function was the answer. A good discussion on this is here. Took a look at implementation of ArrayRef class, find the source code here.  Extras:\n The concept behind JIT is very interesting, and I wanted to read more about it. Started with this article on \u0026ldquo;A Brief History of Just-In-Time\u0026rdquo;. I plan to write a blog on this once I\u0026rsquo;m done reading. Effective Modern C++: I continued reading through Item 12 and Item 13 of Chapter 3:  Declaring Overriding Functions override Prefer const_iterators to iterators   A very interesting blog on vmap and pmap in Jax is here. Read half of it, will continue reading in the next week. An interesting watch on When do I use a union in C or C++, instead of a struct?. I like watching such videos when I\u0026rsquo;m a little tired from work. Blog theme:  Worked on a new theme inspired by this theme. Re-organized the blog, fixed all the links (wherever it\u0026rsquo;s shared on the internet), in-blog images and cover images as well. Check the milestone here. With this blog, the milestone for the blog will be closed. ðŸŽ‰   Started reading tutorial on PyTorch\u0026rsquo;s official docs about Returning a Dispatched Operator in C++. Half way through, will continue reading in the next week.  Yes, and apart from all of this, I did won a few games of CSGO with my friends, had a few health issues to catch up with (ðŸ˜¢), updated my discord channel with blog announcements, helped a few friends with their doubts, and well, yeah that\u0026rsquo;s it.\nI hope to perform better once I feel good (w.r.t health). Thank you for reading.\n","permalink":"https://krshrimali.github.io/posts/2021/09/weekly-progress-report-26-09-2021-1/","tags":["weekly"],"title":"Weekly Progress Report: 26-09-2021, 1"},{"categories":["cpp","book reading"],"contents":" NOTE\nMy notes on Chapter 3, Item 13 of Effective Modern C++ written by Scott Meyers.\nSome (or even all) of the text can be similar to what you see in the book, as these are notes: I\u0026rsquo;ve tried not to be unnecessarily creative with my words. :)\n In C++, iterators come handy to point at memory addresses of STL containers. For example,\n// C++11  std::vector\u0026lt;int\u0026gt; x {11, 9, 23, 6}; // begin() member function returns an iterator, which points to the first // memory address of the container x std::vector\u0026lt;int\u0026gt;::iterator it = x.begin(); While the general practice is to use const whenever possible, but programmers tend to use whenever it\u0026rsquo;s practical. const_iterators is particularly suggested when you want to use iterators, but you don\u0026rsquo;t need to modify what it points to.\nTake an example of the code snippet above, you have the iterator it pointing to the beginning position of the container, and if you want to insert a value before this, you just need to pass this iterator to x.insert() along with the value, right? You don\u0026rsquo;t need to modify the iterator. In such cases, const_iterator will be a better choice.\nBut, this is all for C++-11 and above. In C++-98, const_iterator wasn\u0026rsquo;t just ready:\n// C++-98 // Without using const_iterator  // C++-98, initializing a vector wasn\u0026#39;t that trivial as it is now const int values[] = {11, 23, 6}; std::vector\u0026lt;int\u0026gt; x (values, values+3); // Want to insert 9 before 23 here std::vector\u0026lt;int\u0026gt;::iterator it = std::find(x.begin(), x.end(), 23); x.insert(it, 9); While this will work in C++-98, but since we don\u0026rsquo;t modify the iterator it, using const_iterators is a better choice:\n// C++-98: this won\u0026#39;t compile  // Not using aliases here, C++-98 didn\u0026#39;t have the support for aliases // (was introduced in C++-11) // Using these typedefs makes it easier to reuse them typedef std::vector\u0026lt;int\u0026gt;::iterator it; typedef std::vector\u0026lt;int\u0026gt;::const_iterator c_it; // C++-98, initializing a vector wasn\u0026#39;t that trivial as it is now const int values[] = {11, 23, 6}; std::vector\u0026lt;int\u0026gt; x (values, values+3); c_it ci = std::find(static_cast\u0026lt;c_it\u0026gt;(x.begin()), static_cast\u0026lt;c_it\u0026gt;(x.end()), 23); x.insert(static_cast\u0026lt;it\u0026gt;(ci), 9); If you compile the code snippet above using C++-98 (if you are using g++, use: g++ \u0026lt;filename\u0026gt;.cpp -std=c++-98), you will hopefully get an error similar to:\nmain.cpp:10:14: error: no matching conversion for static_cast from \u0026#39;c_it\u0026#39; (aka \u0026#39;__wrap_iter\u0026lt;const int *\u0026gt;\u0026#39;) to \u0026#39;it\u0026#39; (aka \u0026#39;__wrap_iter\u0026lt;int *\u0026gt;\u0026#39;) x.insert(static_cast\u0026lt;it\u0026gt;(ci), 9); ^~~~~~~~~~~~~~~~~~~ As you can probably notice, it says \u0026ldquo;no matching coversion for static_cast from c_it to it\u0026rdquo;, that means in C++-98 there was no straight forward way to cast const_iterator to iterator. The author does point that there are some reall non-trivial ways to get around, but those are out of the scope of this blog and the book as well.\nA few observations in the snippet above:\n In std::find we cast x.begin() iterators to const_iterator (c_it) because in C++-98, there was no simple way to get a const_iterator from a non-const container. The reason we cast our const_iterator (ci) back to iterator in the call x.insert() is: in C++-98, locations for insertions (and erasures) could only be specified by iterators, not const iterators.  In fact, in C++-11, there is no simple way to convert const_iterator to iterator.\nHere comes C++-11 now, they introduced cbegin() which returns a const_iterator instead, similarly cend() member function exists.\n// In C++-11, initializing of a vector got easier std::vector\u0026lt;int\u0026gt; x {11, 23, 6}; // could have used auto here, but just typing the type for understanding // Notice the use of cbegin(), cend() instead of static_cast\u0026lt;c_it\u0026gt;(x.begin()), ... std::vector\u0026lt;int\u0026gt;::const_iterator ci = std::find(x.cbegin(), x.cend(), 23); x.insert(ci, 11); This will compile successfully, make sure to pass -std=c++11 flag while compiling if you are using g++ to make sure you are using C++-11.\nIt\u0026rsquo;s near this time, isn\u0026rsquo;t it?\nOnly place where C++-11 comes up a bit short is, when you want to use non-member functions (in case you are writing a generic code for your library, see example below):\n// This is a generic function which will find targetVal in a container // Use of decltype discussed before template\u0026lt;typename C, typename V\u0026gt; auto find_value(C\u0026amp; container, const V\u0026amp; targetVal) -\u0026gt; decltype(std::begin(container)) { // Notice the use of non-member cbegin and cend functions here  // Note: member functions would have looked like: container.cbegin(), container.cend()  auto it = std::find(std::cbegin(container), std::cend(container), targetVal); return it; } If you try to compile this using C++-11 -std=c++11 flag, this will fail with an error like:\nmain.cpp:6:25: error: no matching function for call to \u0026#39;begin\u0026#39; auto it = std::find(std::cbegin(container), std::cend(container), targetVal); main.cpp:6:49: error: no matching function for call to \u0026#39;end\u0026#39; auto it = std::find(std::cbegin(container), std::cend(container), targetVal); In C++-11, they failed to add cbegin, cend, rbegin, rend, crbegin, crend as member functions (but begin, end were added). This was later rectified in C++-14, so if you compile the code above using -std=c++14, this won\u0026rsquo;t fail anymore.\nIf you are using C++-11, you can get around by defining a cbegin() function:\ntemplate\u0026lt;class C\u0026gt; auto cbegin(const C\u0026amp; container) -\u0026gt; decltype(std::begin(container)) { return std::begin(container); } The reason using std::begin works here, because in C++-11, if you pass a reference to const version of the container to std::begin, it will return a const_iterator (notice that we pass const C\u0026amp; container in the arguments).\nThe whole gist is to use const_iterator whenever possible. C++-14 tidied up the usage, however C++-11 added required support and features.\nThank you for reading!\n","permalink":"https://krshrimali.github.io/posts/2021/09/prefer-const_iterators-to-iterators-notes/","tags":["development","coding","cpp","notes","iterators"],"title":"Prefer const_iterators to iterators (Notes)"},{"categories":["cpp","book reading"],"contents":" NOTE\nMy notes on Chapter 3, Item 12 of Effective Modern C++ written by Scott Meyers.\nSome (or even all) of the text can be similar to what you see in the book, as these are notes: I\u0026rsquo;ve tried not to be unnecessarily creative with my words. :)\n Overriding != Overloading\nExample of virtual function overriding:\n// Base class class Base { public: virtual void doWork(); // ... }; // Derived class from Base class Derived: public Base { public: // virtual is optional  // this will \u0026#34;override\u0026#34; Base::doWork  virtual void doWork(); // ... }; // This creates a \u0026#34;Base\u0026#34; class pointer to \u0026#34;Derived\u0026#34; class object std::unique_ptr\u0026lt;Base\u0026gt; upb = std::make_unique\u0026lt;Derived\u0026gt;(); // Derived doWork() function is invoked upb-\u0026gt;doWork(); This is how virtual function overriding allows to invoke a \u0026ldquo;derived class function\u0026rdquo; from a base class interface.\nRequirements for overriding:\n Base class function must be virtual. Base and derived function names must be identical (except for destructors). Parameter types of base and derived functions must be identical. Constness of both base and derived functions must be identical. The return types and exception specifications of the base and derived functions must be compatible. [from C++11] Both functions should have identical reference qualifiers (see below)  Reference qualifiers were new to me, but the book gives a really good example:\n// Useful when you want to limit the calls to lvalues or rvalues only  class Widget { public: // ...  void doWork() \u0026amp;; // Only when *this is an lvalue  void doWork() \u0026amp;\u0026amp;; // Only when *this is an rvalue  // ... }; // Suppose there is a makeWidget() function that returns an instance of Widget class, this will be an rvalue here Widget makeWidget(); // lvalue Widget w; // this will call void doWork() \u0026amp;; w.doWork(); // this will call void doWork() \u0026amp;\u0026amp;; makeWidget().doWork(); In case the functions don\u0026rsquo;t have identical reference qualifiers, the derived class will still have the function, but it won\u0026rsquo;t override the base class function.\nBecause of a lot of conditions (see 6 requirements above), it\u0026rsquo;s easy to make mistakes or forget a few things while overriding a function. And the book states that it\u0026rsquo;s not worth expecting from compiler to report an error, because the code can still be valid - it\u0026rsquo;s just you expected it to override, but it didn\u0026rsquo;t.\nSee an example below:\nclass Base { public: virtual void mf1() const; virtual void mf2(int x); virtual void mf3() \u0026amp;; void mf4() const; }; class Derived: public Base { public: virtual void mf1(); virtual void mf2(unsigned int x); virtual void mf3() \u0026amp;\u0026amp;; void mf4() const; }; None of the functions in Derived class will override functions in the Base class. Why?\n mf1(): declared const in Base but not in Derived. mf2(): arg int in Base but unsigned int in Derived. mf3(): lvalue-qualified in Base but rvalue-qualified in Derived. mf4(): isn\u0026rsquo;t declared virtual in Base.  To avoid worrying about human mistakes, it\u0026rsquo;s better to let the compiler know explicitly that these functions are expected to override. For that, declare the functions in Derived class override.\nclass Derived: public Base { public: virtual void mf1() override; virtual void mf2(unsigned int x) override; virtual void mf3() \u0026amp;\u0026amp; override; virtual void mf4() const override; }; Now, this won\u0026rsquo;t compile for sure no matter what compiler you are using.\nC++-11 introduced 2 contextual keywords: final (see note below), override. These keywords are reserved, but only in certain contexts.\nNote: In case you want to prevent a function in a base class to be overridden in derived classes, you can declare them final. final can also be declared for a class, in which case it won\u0026rsquo;t be allowed to be used a base class.\nFor override: In case your old (legacy) code uses override somewhere else, it\u0026rsquo;s fine - keep it! Only when it\u0026rsquo;s used at the end of a member function declaration, then it\u0026rsquo;s reserved.\nclass Sample { public: // legal  void override(); } The chapter (item) ends with a brief about lvalue and rvalue reference member functions, which I would like to cover in another blog. For this blog, I think this should be good! :)\nThank you for reading.\n","permalink":"https://krshrimali.github.io/posts/2021/09/declaring-overriding-functions-override-notes/","tags":["development","coding","cpp","notes","override"],"title":"Declaring Overriding Functions override (Notes)"},{"categories":["cpp","book reading"],"contents":"Prefer deleted functions to private undefined ones This item (11) in the chapter 3 focuses on:\n Why and How to prevent users calling particular functions? C++-98 and C++-11 approach What\u0026rsquo;s the difference between deleting a function vs declaring a member function private (and not defining them)?   NOTE\nThese are my notes on Chapter 3, Item 11 of Effective Modern C++ written by Scott Meyers.\nSome (or even all) of the text can be similar to what you see in the book, as these are notes: I\u0026rsquo;ve tried not to be unnecessarily creative with my words. :)\n  When is it required to delete a function/not define a private member function?  Problem:\n Cases when you don\u0026rsquo;t want the client to call a particular function.  Solution:\n Just don\u0026rsquo;t declare the function  But\u0026hellip;doesn\u0026rsquo;t work always:\nCase: Special member functions generated by C++ automatically, discussed later). Examples considered in this blog:\n Copy Constructor Copy Assignment Operator  C++-98 Approach:\n  Declare these functions private and don\u0026rsquo;t define them.\n  Example:\n All istream and ostream objects inherit (possibly) from basic_ios class in the C++ Standard Library. Copying these objects is undersirable.    Why is copying objects of istream and ostream undesirable? [Also see this question on stackoverflow]\n istream object: represents stream of input values.  some might have been read before. and some may be read later.   If you copy istream object:  Will that copy those values which have been read before? Or will also copy values which are to be read later?      Hence, it\u0026rsquo;s just better to not allow copying istream or ostream objects.\nIn C++-98: Reminder (from above): All istream and ostream objects inherit from basic_ios class (possibly) in the C++ standard, and the basic_ios class in C++-98 looks something like this:\ntemplate \u0026lt;class charT, class traits = char_traits\u0026lt;T\u0026gt; \u0026gt; class basic_ios : public ios_base { public: // ...  // Declaring the copy constructor and copy assignment operator private prohibits clients from calling them private: basic_ios(const basic_ios\u0026amp;); // not defined  basic_ios\u0026amp; operator=(const basic_ios\u0026amp;); // not defined }; Note that:\n basic_ios(const basic_ios\u0026amp;) is the copy constructor basic_ios\u0026amp; operator=(const basic_ios\u0026amp;) is the copy assignment operator  (and both are private).\nHow does not defining these functions help?\n Consider a case where a friend class or member functions try accessing these functions, then linking will fail because of missing function definitions.  In C++-11 In C++-11, the above can be done using = delete to mark the copy constructor and the copy assignment operator as deleted functions.\ntemplate \u0026lt;class charT, class traits = char_traits\u0026lt;charT\u0026gt; \u0026gt; class basic_ios : public ios_base { public: // ...  basic_ios(const basic_ios\u0026amp; ) = delete; // deleted function  basic_ios\u0026amp; operator=(const basic_ios\u0026amp;) = delete; // deleted function  // ... }; It\u0026rsquo;s a convention to declare deleted functions public, but why? Better error messages.\nIn case you declare your deleted functions private, some compilers will probably complain about the function being private and can hide the error message of it not being usable (because it being deleted). Hence, it\u0026rsquo;s a good practice to make them public:\nFrom my experience though, Apple\u0026rsquo;s clang compiler (v 12.0.5) doesn\u0026rsquo;t complain about it being private, but then - it can vary from compiler to compiler, so better to play safe. Here is an example of how the error message looks like:\n#include \u0026lt;iostream\u0026gt; class Sample { public: Sample(int x) : x(x) { }; // Copy constructor has been deleted, so should not be callable  Sample(const Sample\u0026amp;) = delete; void trying_copy_construct(Sample s) { // This function tries to use a copy constructor  // This should fail  Sample new_object(s); } private: int x; }; Compiling the above code fails with the following error:\nmain.cpp:10:16: error: call to deleted constructor of \u0026#39;Sample\u0026#39; Sample new_object(s); ^ ~ main.cpp:6:5: note: \u0026#39;Sample\u0026#39; has been explicitly marked deleted here Sample(const Sample\u0026amp;) = delete; Difference b/w using delete vs declaring private Note: There is more to it except the good practice reasoning.\n Using a deleted function in a member function or by a friend class won\u0026rsquo;t even compile the code if it tries to copy basic_ios objects while declaring private will compile successfully but fail during link-time. Conventionally deleted functions are declared public, not private while the C++-98 way requires the functions to be declared private (see the section above for reasoning). Only member functions can be private, while any function can be deleted (so you can delete some overloads for your function, in case you don\u0026rsquo;t want it to accept certain type inputs).  Let\u0026rsquo;s discuss the last point in detail:\nConsider the case where you have:\ntemplate\u0026lt;typename T\u0026gt; void processPointer(T* ptr); And:\n You need a template that works with built-in pointers. You want to reject calls with void* and char* pointers (more on this later, these deserve special handling at times). Ideal way? Delete these instantiations (with void* or char* input pointers).  template\u0026lt;\u0026gt; void processPointer\u0026lt;void\u0026gt;(void*) = delete; template\u0026lt;\u0026gt; void processPointer\u0026lt;char\u0026gt;(char*) = delete; But, with C++-98 way, it\u0026rsquo;s not possible within the class scope. That is, you can not give template specialization to your member function within the class scope (it should be done in the namespace scope):\nclass Sample { public: // ...  template \u0026lt;typename T\u0026gt; void processPointer(T* ptr) { ... } private: // ...  // This is template specialization inside the scope of the class - not allowed  template \u0026lt;\u0026gt; void processPointer\u0026lt;void\u0026gt;(void*); // error }; While with the C++-11 way, you can delete function outside the class scope:\nclass Sample { public: // ...  template \u0026lt;typename T\u0026gt; void processPointer(T* ptr) { ... } // ... }; template \u0026lt;\u0026gt; void Sample::processPointer\u0026lt;void\u0026gt;(void*) = delete; // in public scope, and deleted! I hope you liked this blog, thank you for reading! :)\n","permalink":"https://krshrimali.github.io/posts/2021/08/prefer-deleted-functions-to-private-undefined-ones-notes/","tags":["development","coding","cpp","notes","deleted functions"],"title":"Prefer Deleted Functions to Private Undefined Ones (Notes)"},{"categories":["cpp","book reading"],"contents":"Scoped vs Unscoped Enums  General rule: declaring a name inside curly braces is limited to that scope. Exception: C++-98 style Enums   NOTE\nMy notes on Chapter 3, Item 10 of Effective Modern C++ written by Scott Meyers.\nSome (or even all) of the text can be similar to what you see in the book, as these are notes: I\u0026rsquo;ve tried not to be unnecessarily creative with my words. :)\n // You can\u0026#39;t declare black, white, red in the scope containing the enum Color enum Color { black, white, red; }; auto white = false; // error: white already declared in this scope  Unscoped Enums have implicit type conversions for their enumerators. Enumerators can implicitly convert to integral types, and then to floating-point types.  // Assume Color is declared like above Color c = red; // valid since Enumerator white is leaked to the scope Color is in if (c \u0026lt; 10) { // valid, implicit conversion  // ... do something } else if (c \u0026lt; 10.5) { // also valid, implicit conversion  // ... do something } The C++-98 Style Enums are termed as Unscoped Enums (because of leaking names).\nC++-11 Scoped Enums:\n// black, white, red are now scoped to Color Enum enum class Color { black, white, red; }; // This is now valid auto white = false; Separately, if you do: (consider Color Enum has already been declared)\nColor c = white; // error: no enumerator named \u0026#34;white\u0026#34; is in this scope Color c = Color::white; // valid auto c = Color::white; // valid  Also referred as enum classes (because declared using enum class). Enumerators in scoped Enums are strongly typed (no implicit type conversion)  // Assume Color is declared as above using enum class Color c = Color::red; if (c \u0026lt; 10.5) { // Error! can\u0026#39;t compare Color and double  // do something... } Note: you can do explicit casting using cast. Note about enums in C++: * Every enum in C++ has an integral underlying type that is determined by compilers. * Compilers need to know the size of enum before using it.\nC++98 vs C++11 on Enums C++98:\n Unscoped enums can not be forward-declared.  Hence only enums with definitions are supported. Allows compilers to choose underlying type for each enum prior to the enum being used.   Drawbacks?  Increase in compilation dependencies: wherever the enum is used, even if not affected by any addition in the enum, it will be recompiled (generally speaking, that is without any tweaks/optimizations).    C++11:\n Both unscoped and scoped enums can be forward-declared. Unscoped enums will need a few efforts though:  /* For Scoped Enums */ // Default underlying type is int enum class Status; // Override it enum class Status: std::uint32_t; /* For Unscoped Enums */ // There is no underlying type for unscoped enum // You can manually specify though enum Status: sd::uint32_t; These specifications for underlying types can also go on enum\u0026rsquo;s definitions.\nUnscoped Enums over Scoped Enums? Imagine when you have a code like this:\n// Ordered as: name, email, reputation using UserInfo = std::tuple\u0026lt;std::string, std::string, std::size_t\u0026gt;; UserInfo uInfo; // This is not clear to the reader, you can\u0026#39;t always remember what 1st indexed field in UserInfo is auto val = std::get\u0026lt;1\u0026gt;(uInfo); Using the property of intrinsic conversion in unscoped enums, you can solve this:\nenum InfoFields { uName, uEmail, uReputation }; // UserInfo defined as above UserInfo uInfo; // Implicit conversion of int (default underlying type of enums) to std::size_t (that\u0026#39;s what std::get takes) auto val = std::get\u0026lt;uEmail\u0026gt;(uInfo); For scoped enums though, you\u0026rsquo;ll have to use static_cast\u0026lt;std::size_t\u0026gt;(InfoFields::uEmail) instead of just uEmail (for unscoped enums) passed to std::get, which is less readable. But\u0026hellip;\nThis can be redued by using a custom function which: * takes: enum * returns: corresponding std::size_t value\nstd::get is a template, and the value needs to be understood during compilation only, so the function should be a constexpr (more on this later in the series).\nTo generalize, let\u0026rsquo;s keep the enum\u0026rsquo;s underlying type (std::underlying_type type trait)\n// Using noexcept because we know there\u0026#39;ll be no exceptions raised template \u0026lt;typename E\u0026gt; constexpr typename std::underlying_type\u0026lt;E\u0026gt;::type toUType(E enumerator) noexcept { return static_cast\u0026lt;typename std::underlying_type\u0026lt;E\u0026gt;::type\u0026gt;(enumerator); } From the previous blog, we know that in C++14, we could have simplified by writing:\n// Using noexcept because we know there\u0026#39;ll be no exceptions raised template \u0026lt;typename E\u0026gt; constexpr std::underlying_type_t\u0026lt;E\u0026gt; toUType(E enumerator) noexcept { return static_cast\u0026lt;std::underlying_type_t\u0026lt;E\u0026gt;\u0026gt;(enumerator); } Could have used auto for return type in C++14:\n// Using noexcept because we know there\u0026#39;ll be no exceptions raised template \u0026lt;typename E\u0026gt; constexpr auto toUType(E enumerator) noexcept { return static_cast\u0026lt;std::underlying_type_t\u0026lt;E\u0026gt;\u0026gt;(enumerator); } Now this can be used as:\n// Reminder, InfoFields was defined as: enum InfoFields { uName, uEmail, uReputation }; // toUType is defined above auto val = std::get\u0026lt;toUType(InfoFields::uEmail)\u0026gt;(uInfo); Good Reads  Forward Declaration:  Stackoverflow: https://stackoverflow.com/questions/4757565/what-are-forward-declarations-in-c   Are Unscoped Enums still helpful?  Stackoverflow: https://stackoverflow.com/questions/27320603/are-unscoped-enumerations-still-useful   Proposal for forward declaration to enums (accepted), dated 2008: http://www.open-std.org/jtc1/sc22/wg21/docs/papers/2008/n2764.pdf Forward Declaring an Enum in C++?  Stackoverflow: https://stackoverflow.com/questions/71416/forward-declaring-an-enum-in-c    Acknowledgement (Reviews) Thanks to Kshitij Kalambarkar for helping in reviewing the blog. It\u0026rsquo;s always helpful to get another set of eyes to what you write. :)\nThat\u0026rsquo;s it for this blog, thank you for reading everyone!\n","permalink":"https://krshrimali.github.io/posts/2021/08/prefer-scoped-enums-over-unscoped-enums-notes/","tags":["development","coding","cpp","notes","scoped enums","unscoped enums"],"title":"Prefer Scoped Enums over Unscoped Enums (Notes)"},{"categories":["cpp","algorithms"],"contents":"Hi Everyone, today I want to talk about Union Find Problem. This is going to be a series covering:\n Union Find Problem (this blog) Solutions to Union Find (1): Quick Find Solutions to Union Find (2): Quick Union Solutions to Union Find (3): Weighted Quick Union Applications of Union Find (perculation and more) Cool project using Union Find Solving some competitive programming questions using Union Find  Each blog will try to cover very basic concepts behind the topic, and also what it\u0026rsquo;s all about.\nUnion Find Problem: Definition Let\u0026rsquo;s define the problem first. It\u0026rsquo;s a problem where you need to find whether two points/objects are in a connected relationship (defined below) or not in a defined environment (where you know the relationships).\nConnection Relationship is an equivalence relation, which means:\n It\u0026rsquo;s reflexive: a ~ a (a is connected to itself) It\u0026rsquo;s symmetric: a ~ b iff b ~ a (a is connected to b iff b is connected to a OR if a is connected to b, b is also connected to a) It\u0026rsquo;s transitive: if a ~ b and b ~c then a ~ c (if a is connected to b, and b is connected to c, then a is connected to c)  And by connected, we just mean that there is a path between the two objects. My thinking around this problem is mostly surrounded by the plot of dynamic connectivity, where you want to find if there is a connection between 2 objects in a graph. These objects can be friends (whether A and B are friends or not in a circle - here circle is the environment).\nUnion Find: Problem, why study it? It\u0026rsquo;s a name to a problem, but you must have encountered this in real life. Whether you are a friend to your ex, oh definitely not ;) (even if Union Find solution finds a connection, trust me - move on :P). Okay, on a serious note now:\nUnion Find Problem is seen in lots of applications:\n Perculation (example: if you pour water on the top of a tank having lots of cells/blocks - some are open, some are closed - will it reach the bottom?).  I also see this as an application where you want to find if the leakage in a whole network of oil pipes will exit or if it will be blocked.   Dynamic Connectivity: A very simple definition would be, whether there is a connection between two objects?  You can see it\u0026rsquo;s application in social media, whether two objects (I know I should use humans but the whole internet objectifies you ;), hence objects ;)). Whether there is a connection between two places in a nation or not?   Games (will be discussed later) and more\u0026hellip;  Now it\u0026rsquo;s indeed a very interesting problem, and in this blog, I\u0026rsquo;ll show you a very basic implementation which I wrote before studying the algorithms which attempt to solve this problem.\nUnion Find breakdown: Union and Find  NOTE\nAll codes are written here in C++ and code is available here: https://github.com/krshrimali/Algorithms-All-In-One/.\n Breaking it down to two functions, is really helpful:\n// Object is an arbitrary type for now, can be an int, can be a user defined type as well void union(Object a, Object b) { // This function will connect two objects, if:  // * they exist  // * there isn\u0026#39;t a connection already  // ... } Similarly, the find function will try to find whether there is a connection between two objects:\n// Object is an arbitrary type for now, can be an int, can be a user defined type as well bool find(Object a, Object b) { // returns true if:  // * both objects exist  // * and they are connected  // else returns false  // ... } Let\u0026rsquo;s try to setup the environment first, and we need to answer these two questions first:\n What should be the objects? Where are these objects stored?  I like thinking of this as a graph (environment) and points as objects. So let\u0026rsquo;s start implementing.\nImplementation: Modelling  NOTE\nThis is a very basic implementation and first try presenting a naive solution to the problem, we\u0026rsquo;ll discuss better algorithms in next blogs.\n The very first question you should ask yourself is, what data structures should be used for Graph and Point(s)? The way I\u0026rsquo;m thinking of solving this is:\n Each Point will have (x, y) coordinates. (so coordinates will be it\u0026rsquo;s property) Whenever two points are merged (union is called), the first point will append the second point in it\u0026rsquo;s list of connections.  So each Point object will have a connection list. (std::vector?)   Whenever find is called, that is - there is an attempt to find if there is a connection between two points?  We just need to search if second point is there in the first point\u0026rsquo;s connection list. If it is, then there is a connection. And if not, then no connection. In python, I would have used a dict, so I went ahead with std::map in C++, will help me not duplicating points.    So the Graph will be a std::map of Point, std::vector\u0026lt;Point\u0026gt;, which will look something like this:\n// This is how graph will look like, in imagination // Example: // a is connected to b, c, d // b is connected to a (Point a, {Point b, Point c, Point d}), (Point b, {Point a}), ...so on As you can see, there will be a list mapped to each Point, we call that list: connection list.\nNow, the Point can simply be a struct having int x, y as coordinates.\nImplementation: Skeleton Let\u0026rsquo;s create the skeleton now:\nstruct Point { // x and y are the coordinates for each point  int x, y; }; // Graph will contain Points, helper functions: union and merge class Graph { private: std::map\u0026lt;Point, std::vector\u0026lt;Point\u0026gt;\u0026gt; graph; public: // We take references to avoid internal copies, const is used since we don\u0026#39;t want these functions  // to modify these points in any way  void union_(const Point\u0026amp; a, const Point\u0026amp; b) { // Use find utility function of std::map  if (this-\u0026gt;graph.find(a) == this-\u0026gt;graph.end()) { // Not found  // Means create an entry in the graph, and add b to the connection list of a  this-\u0026gt;graph[a] = {b}; } else { // Found  // Append b to the connection list of a  this-\u0026gt;graph.at(a).push_back(b); } } // Are a and b connected? OR Is there a path b/w a and b?  bool find_(const Point\u0026amp; a, const Point\u0026amp; b) { // First check if there is an Point a in the graph  if (this-\u0026gt;graph.find(a) == this-\u0026gt;graph.end()) { std::cout \u0026lt;\u0026lt; \u0026#34;Not found\\n\u0026#34;; return false; } else { // Object found  std::vector\u0026lt;Point\u0026gt; connection_list = this-\u0026gt;graph.at(a); // Now find if b exists in the connection list, if yes then there is a connection  if (std::find(connection_list.begin(), connection_list.end(), b) != connection_list.end()) { // b found  return true; } return false; } } }; // Usage int main() { Graph g_sample; struct Point p(2, 3); struct Point q(3, 3); struct Point r(4, 4); // Add a connection for (p, q) and (p, r), for testing  g_sample.union_(p, q); g_sample.union_(p, r); std::cout \u0026lt;\u0026lt; \u0026#34;Are p and q connected? Answer: \u0026#34; \u0026lt;\u0026lt; sampleGraph.find_(p, q) \u0026lt;\u0026lt; \u0026#39;\\n\u0026#39;; // Expected: true  std::cout \u0026lt;\u0026lt; \u0026#34;Are p and r connected? Answer: \u0026#34; \u0026lt;\u0026lt; sampleGraph.find_(p, r) \u0026lt;\u0026lt; \u0026#39;\\n\u0026#39;; // Expected; true  std::cout \u0026lt;\u0026lt; \u0026#34;Are q and r connected? Answer: \u0026#34; \u0026lt;\u0026lt; sampleGraph.find_(q, r) \u0026lt;\u0026lt; \u0026#39;\\n\u0026#39;; // Expected: false } Now this is a great start, I won\u0026rsquo;t spend time explaining the code as the comments should help. In case you have queries, please feel free to open an issue here.\nBut this won\u0026rsquo;t compile. And the reason is, that when you are using std::find with user-defined types like Point, you need to define \u0026lt; operator or give it a comparator because it does some comparisons. Think of this like:\nThe compiler isn\u0026rsquo;t aware of how to do: Point(2, 3) \u0026lt; Point(3, 3)\nBecause for the compiler, both of these are an object. So we need to tell it explicitly, that hey! when you do \u0026lt; operation on Point objects, check their coordinates.\nFinal Implementation The final code can be found here. There are a few TODOs in the code mentioned, and in case you want to pick them up, please create a PR for the same. :)\nThe code will change with time, so I\u0026rsquo;ll refrain copy-pasting it here.\nHomework? Let\u0026rsquo;s do this before I release the next blog:\n Analyze the algorithm used here, it\u0026rsquo;s time and space complexity. Address the TODOs in the code.  In case you are able to do this before my next blog, kudos to you! You might as well help creating a PR, that will be great.\nThank you for reading this blog. I hope you liked it! :)\n","permalink":"https://krshrimali.github.io/posts/2021/08/union-find-problem-and-a-naive-implementation-c-/","tags":["development","coding","cpp","union find","algorithms"],"title":"Union Find Problem, and a naive implementation (C++)"},{"categories":["cpp","book reading"],"contents":"One solution to avoiding using long type names:\n// So C++98 like typedef std::unique_ptr\u0026lt;std::unordered_map\u0026lt;std::string, std::string\u0026gt;\u0026gt; UPtrMapSS;  NOTE\nMy notes on Chapter 3, Item 9 of Effective Modern C++ written by Scott Meyers.\nSome (or even all) of the text can be similar to what you see in the book, as these are notes: I\u0026rsquo;ve tried not to be unnecessarily creative with my words. :)\n C++11 also offers alias declarations:\nusing UPtrMapSS = typedef std::unique_ptr\u0026lt;std::unordered_map\u0026lt;std::string, std::string\u0026gt;\u0026gt;; Advantages of alias declarations over typedefs:\n  For types involving function pointers, aliases are easier to read (for some people):\n// typedef typedef void (*FP)(int, const std::string\u0026amp;); // alias declaration using FP = void (*)(int, const std::string\u0026amp;);   Alias declarations can be templatized, but typedefs can not.\n// MyAlloc is a custom allocator template \u0026lt;typename T\u0026gt; using MyAllocList = std::list\u0026lt;T, MyAlloc\u0026lt;T\u0026gt;\u0026gt;; MyAllocList\u0026lt;Widget\u0026gt; lw; // will create std::list\u0026lt;Widget, MyAlloc\u0026lt;Widget\u0026gt;\u0026gt; vs: typedef, a hack way:\n// templatized struct here template \u0026lt;typename T\u0026gt; struct MyAllocList { typedef std::list\u0026lt;T, MyAlloc\u0026lt;T\u0026gt;\u0026gt; type; }; MyAllocList\u0026lt;Widget\u0026gt;::type lw;   In case you want to use a type specified by template parameter, with typedefs it gets complex:\n// MyAllocList is defined as mentioned in 2nd point template \u0026lt;typename T\u0026gt; class Widget { private: typename MyAllocList\u0026lt;T\u0026gt;::type list; // ... };  Here MyAllocList\u0026lt;T\u0026gt;::type is now a dependent type (dependent on type T from template paramater). C++ Rule: need to use typename before name of a dependent type.  With alias declaration of MyAllocList, no need to use typename and ::type:\ntemplate \u0026lt;typename T\u0026gt; class Widget { private: // no typename and ::type  MyAllocList\u0026lt;T\u0026gt; list; // ... }   Explanation on the 3rd point:\n Compiler understands the alias declared MyAllocList when used inside a template class Widget as MyAllocList\u0026lt;T\u0026gt; is not a dependent type. A user can have type as a data member, and thus it\u0026rsquo;s important to mention typename when using MyAllocList\u0026lt;T\u0026gt;::type (as a type), so that compiler knows it\u0026rsquo;s a type.  Creating revised types from template type paramaeters is a common practice in Template Meta Programming (TMP). A few important points to note:\nIn C++11 (in header: \u0026lt;type_traits\u0026gt;):\nstd::remove_const\u0026lt;T\u0026gt;::type // yields T from const T std::remove_reference\u0026lt;T\u0026gt;::type // yields T from T\u0026amp; and T\u0026amp;\u0026amp; std::add_lvalue_reference\u0026lt;T\u0026gt;::type // yields T\u0026amp; from T In case you are applying the above transformations inside a template to a type parameter, you\u0026rsquo;ll have to use typename. This is because they have been implemented as typedefs inside templatized structs.\nIn C++14, their alias equivalent were added which do not require you to prefix typename:\n// equivalents to the above 3 transformations std::remove_const_t\u0026lt;T\u0026gt; std::remove_reference_t\u0026lt;T\u0026gt; std::add_lvalue_reference\u0026lt;T\u0026gt; Acknowledgement\n Thanks to Kshitij Kalambarkar for reviewing this blog.  Thanks for reading!\n{{ template \u0026ldquo;_internal/disqus.html\u0026rdquo; . }}\n","permalink":"https://krshrimali.github.io/posts/2021/08/prefer-alias-declarations-to-typedefs-notes/","tags":["development","coding","cpp","notes","aliases","typedefs"],"title":"Prefer Alias Declarations to Typedefs (Notes)"},{"categories":["cpp"],"contents":"In today\u0026rsquo;s blog, we\u0026rsquo;ll talk about two important concepts in C++: Function Pointers and Function Objects.\nPlease note that, function objects are commonly referred as functors but we have failed to notice any official alias to the name. Hence, we\u0026rsquo;ll restrict ourselves to using Function Objects in this blog.\nFunction Pointers As the name sounds, a function pointer is simply a pointer to the memory address of a function. Consider a following function:\n// A function which returns true if a \u0026gt; b else false bool isGreater(int a, int b) { return a \u0026gt; b; } As we would expect, the function is stored in the memory starting with an address. You can print the memory address of a function by doing (we do this using printf, see: https://stackoverflow.com/a/2064722)\nauto fn_addresss = isGreater; // get the address of the function printf(\u0026#34;Function address: %p\\n\u0026#34;, fn_address); And you\u0026rsquo;ll notice a hex value as the output: 0x5649d675c139 (in my case). The syntax for creating a function pointer looks like this:\nreturn_type (*ptr_name)(arg1_type, arg2_type, ...); So in our case for isGreater, it will be:\nbool (*justApointer)(int, int); What this means is, the pointer justApointer will point to a function taking 2 integer arguments and returning a boolean value. But note that this doesn\u0026rsquo;t point to any function yet. If you will do:\nbool (*justApointer)(int, int); // De-reference the pointer and call (*justApointer)(3, 4); This will cause a segmentation fault (core dumped) error because it points to no valid address of an executable code. So let\u0026rsquo;s go ahead and point our pointer to the memory address of isGreater:\njustApointer = \u0026amp;isGreater; Here we have given the address of the function to the pointer, you can also do:\n// Note: It\u0026#39;s a good practice to avoid writing the type of function pointers again if it\u0026#39;s too verbose using FnType = bool (*)(int, int); FnType justApointer{ \u0026amp;isGreater }; // OK FnType yetAnotherPointer = \u0026amp;isGreater; // OK FnType yetAnotherFnPointer = isGreater; // OK, implicit conversion happens in C++ from function to function pointer, so you don\u0026#39;t need to use \u0026amp; operator You could also have declared the pointer first, and then initialized:\n// Declararation bool (*justApointer)(int, int); // Initialization justApointer { \u0026amp;isGreater }; justApointer { isGreater }; justApointer = \u0026amp;isGreater; justApointer = isGreater; All of this is valid and works in C++. If you\u0026rsquo;re coming from Modern C++, you might have realized that it\u0026rsquo;s OK to skip the syntax of a function pointer and use:\nauto justApointer = isGreater; Calling a function pointer is fairly straight forward, you can just do:\n// Will return 0 since 3 is not greater than 4 std::cout \u0026lt;\u0026lt; (*justApointer)(3, 4); Since it\u0026rsquo;s a pointer, so you have to de-reference it to get to the function address (executable code in the memory) and then call it using the () call operator. C++ does the implicit conversion, and you can skip de-referencing:\n// Will also return 0 since 3 is not greater than 4 std::cout \u0026lt;\u0026lt; justApointer(3, 4); If you are familiar with concepts of const pointers, you can also create a const function pointer, so that once initialized - it can not be pointed to a different function.\nbool (*const justApointer)(int, int) = \u0026amp;isGreater; // You can not re-initialize (aka assign) it to point to any other address justApointer = \u0026amp;isGreater; // NOT OK, ERROR: assignment of read-only variable \u0026#39;justApointer\u0026#39; If you have noticed, we used *const justApointer - since we wanted to indicate to the compiler - that the pointer is supposed to be const, not the output (const bool (*justApointer)(int, int)). You can play around with different specifiers and see how they work though.\nOne of the use-cases of function pointers is to be able to pass a function as an argument (often referred as Callback Functions). But well, you might have a question - you can use a global function in another function, right? Yes, that\u0026rsquo;s possible, but consider a case where you want to pass different callback functions depending on your requirement to a more generic function (like sorting).\nbool isGreater(int a, int b) { return a \u0026gt; b; } bool isLesser(int a, int b) { return a \u0026lt; b; } bool isEqual(int a, int b) { return a == b; } We have these 3 functions but we don\u0026rsquo;t know yet which one we want to use to sort an array, let\u0026rsquo;s say you want to sort an array in descending order, another array in ascending order.\nstd::vector\u0026lt;int\u0026gt; sample_vec = {0, 5, -3, 4, 9, 2}; void a_generic_sorting_function(std::vector\u0026lt;int\u0026gt; input_vec, bool (*comparisonFunction)(int, int)) { // ... sorting algorithm \t// use comparisonFunction for comparisons } a_generic_sorting_function(sample_vec, isGreater); // sorts in descending order a_generic_sorting_function(sample_vec, isLesser); // sorts in ascending order Observe that even though we passed a function as an argument, but eventually - that\u0026rsquo;s interpreted as a pointer (since that\u0026rsquo;s what the 3rd argument type is, in a_generic_sorting_function).\nFunction Objects  Function Objects are types that implement call operator ().\n Function Objects provide us 2 advantages over function pointers, which are mainly:\n Can be optimized by the compiler, if possible. Allows to store a state.  How can compiler optimize function objects? You\u0026rsquo;ll see this definition almost everywhere, and hence the quote. There is no better and simpler way to define a function object. But we\u0026rsquo;ll also focus on how can they make things easier + faster. A struct or a class in C++ which defines/implements call operator () can be referred as function object. Interestingly, in C++:\n std::plus is a function object implementing x + y. std::minus is a function object implementing x - y.  and many more arithmetic operators like /, *, % and negation (-x). See Operator Function Objects section in https://en.cppreference.com/w/cpp/utility/functional.\nThere are other comparison, logical and bitwise operations as well which are provided as function objects in C++. Let\u0026rsquo;s take a look at std::greater and std::lesser function objects to maintain the consistency b/w function pointers and objects sections. Going by the documentation (https://en.cppreference.com/w/cpp/utility/functional/greater), the struct std::greater implements the call operator ():\nbool operator() (const T\u0026amp; lhs, const T\u0026amp; rhs) const; // (until C++14) (source: https://en.cppreference.com/w/cpp/utility/functional/greater)\nIf we had to define our own function object, something similar to std::greater but only for integer inputs. As mentioned earlier, it\u0026rsquo;s a type with the call operator defined, so let\u0026rsquo;s go ahead and define our own struct:\nstruct greater { bool operator()(int a, int b) { return a \u0026gt; b; } }; greater comparison; std::cout \u0026lt;\u0026lt; comparison(3, 4); // Returns 0 Now you can ask: this could have been accomplished with a function pointer as well, so why a function object? Well, so the answer boils down to optimization (in this case). A compiler can inline the function if it\u0026rsquo;s possible to optimize the execution - and that\u0026rsquo;s only possible with function objects, while for function pointers - you need to de-reference it to know the address which happens during the runtime (unless there is some real complex optimization - which I\u0026rsquo;m not aware of right now).\nA real example can be to see the compiled code on https://godbolt.org/ (an amazing compiler explorer). If I compile the following code (on x86-64, gcc 11.1 with no optimization flags):\n#include \u0026lt;iostream\u0026gt; struct greater { bool operator()(int a, int b) { return a \u0026gt; b; } }; int main() { struct greater obj; std::cout \u0026lt;\u0026lt; obj(3, 4); } The relevant assembly code of the main function looks like this:\nmain: push rbp mov rbp, rsp sub rsp, 16 lea rax, [rbp-1] mov edx, 4 mov esi, 3 mov rdi, rax call greater::operator()(int, int) movzx eax, al mov esi, eax mov edi, OFFSET FLAT:_ZSt4cout call std::basic_ostream\u0026lt;char, std::char_traits\u0026lt;char\u0026gt; \u0026gt;::operator\u0026lt;\u0026lt;(bool) mov eax, 0 leave ret The above assembly code may look overwhelming to some, but the most relevant instruction is: (link to the code: https://godbolt.org/z/WqYozn7qv)\ncall greater::operator(int, int) Now if I add the optimization flag, you\u0026rsquo;ll see the operator being inlined:\nmain: sub rsp, 8 xor esi, esi mov edi, OFFSET FLAT:_ZSt4cout call std::basic_ostream\u0026lt;char, std::char_traits\u0026lt;char\u0026gt; \u0026gt;\u0026amp; std::basic_ostream\u0026lt;char, std::char_traits\u0026lt;char\u0026gt; \u0026gt;::_M_insert\u0026lt;bool\u0026gt;(bool) xor eax, eax add rsp, 8 ret _GLOBAL__sub_I_main: sub rsp, 8 mov edi, OFFSET FLAT:_ZStL8__ioinit call std::ios_base::Init::Init() [complete object constructor] mov edx, OFFSET FLAT:__dso_handle mov esi, OFFSET FLAT:_ZStL8__ioinit mov edi, OFFSET FLAT:_ZNSt8ios_base4InitD1Ev add rsp, 8 jmp __cxa_atexit Though, it may not be visible on the first look, but a closer look to the following instruction:\ncall std::basic_ostream\u0026lt;char, std::char_traits\u0026lt;char\u0026gt; \u0026gt;\u0026amp; std::basic_ostream\u0026lt;char, std::char_traits\u0026lt;char\u0026gt; \u0026gt;::_M_insert\u0026lt;bool\u0026gt;(bool) tells us that it doesn\u0026rsquo;t call the operator of the object of type greater anymore! Instead, the compiler knows that the value is false and hence it inlines the value to the std::cout call. While this is possible for function objects, it\u0026rsquo;s not possible for function pointers (with the -O3 flag at least).\nStoring a state It\u0026rsquo;s more like a property of a class/struct in C++ that you can take arguments in the constructor and have different objects with different values for a member variable. Take an example:\n// Usage: // GreaterThan obj(10); // obj(11); // is 11 \u0026gt; 10? // // GreaterThan obj_(-10); // obj_(-9); // is -9 \u0026gt; -10? class GreaterThan { int compare_with; public: void greaterThan(int inp) : compare_with(inp) { } bool operator()(int another_number) { return another_number \u0026gt; compare_with; } }; int main() { GreaterThan obj(10); std::cout \u0026lt;\u0026lt; obj(11) \u0026lt;\u0026lt; \u0026#39;\\n\u0026#39;; GreaterThan obj_(-10); std::cout \u0026lt;\u0026lt; obj_(-9) \u0026lt;\u0026lt; \u0026#39;\\n\u0026#39;; } Here you have a member variable compare_with and you can have different values for each object instantiated. While it\u0026rsquo;s also possible for a function by using a static variable but you can\u0026rsquo;t have multiple values for it on a single run.\nFunction Objects and Function Pointers in the Standard Library Function Objects and Function Pointers, just like any other type/value can be passed as a type to a template:\ntemplate \u0026lt;typename T, typename ComparatorFunc\u0026gt; void sort(T vector_input, ComparatorFunc func) { // ... sorting logic using given comparator function: func } This allows you to use sort as a generic function with different types of comparators. Let\u0026rsquo;s take a look here, you have 2 function object types: isGreater and isLesser: (the same can be done for function pointers as well)\nstruct isGreater { bool operator()(int a, int b) { return a \u0026gt; b; } } struct isLesser { bool operator()(int a, int b) { return a \u0026lt; b; } } template \u0026lt;typename T, typename ComparatorFunc\u0026gt; T sort(T input, ComparatorFunc func) { // use func to decide sorting strategy (descending/ascending) } This is valid in C++, though I\u0026rsquo;ll like to add a disclaimer here, you could have just used std::sort instead of implementing your own sorting strategy (unless you know what you are doing ;)):\nstd::sort(input.begin(), input.end(), isGreater); std::sort(input.begin(), input.end(), isLesser); This is mostly it for this blog, there is a lot to discuss about function pointers and objects, but I guess this should be enough for you to get started and follow us along in future blogs.\nReferences and Good Reads  Learn CPP\u0026rsquo;s Blog on Function Pointer. Function Objects in the STL (Microsoft Docs) CppReference: Function Objects. ","permalink":"https://krshrimali.github.io/posts/2021/07/function-pointers-and-function-objects-in-c-/","tags":["development","coding","cpp","function pointers","function objects"],"title":"Function Pointers and Function Objects in C++"},{"categories":["opencv"],"contents":"Hi everyone! In the previous blog we implemented Portrait Bokeh using Face Detection in OpenCV. While the results were good for a start, we definitely want to be closer to the output we expect. The end goal is to blur everything except the face. The main problem we noticed was:\n The face cropped was a rectangle, and it was clearly visible in the output result.  To overcome this, we will be talking about cropping a circle in OpenCV today. This will enable us to get rid of \u0026ldquo;some\u0026rdquo; of the background noise we got earlier, for Portrait Bokeh. Let\u0026rsquo;s take this step by step, and first talk about the intuition.\nIntuition behind cropping a circle while pixels are just the brightness values for each channel at a particular coordinate, so you can\u0026rsquo;t really get half of the pixel and crop an exact circle. But the closest we can get to cropping a circle, is to imagine a circle circumscribed in a rectangle (face detection algorithm in OpenCV - CascadeClassifier returns a rectangle - can be a square as well). So if we are able to get a circle from our output of face detection (a rectangle), we will be closer to what we want.\nBut how do we get started? Clearly, since the circle is circumscribing the rectangle, the closest we can get to finding radius is: max(width, height)/2. While center will be: (top_left_x + width/2, top_left_y + height/2). Once we know these two properties of the circle, we will now have the circle equation.\nMethodology Let\u0026rsquo;s divide this problem statement into steps:\n Get face from Face Detection. Get circle circumscribing the face (rectangle). Crop the circle and store it in different array. Blur the whole image except the face.  Essentially, the main goal is to get the face cropped as a circle. Once we have that, we can simply overlay this on the blurred image. The trick is to figure out on how we can crop the circle once we know it\u0026rsquo;s coordinates. Let\u0026rsquo;s talk about it\u0026rsquo;s solution in the next section.\nCropping a circle Usually, our images will have 3 channels (colored image): Blue, Green, Red (BGR). How about we add a transparency channel to our image? The idea behind this is to make all pixels transparent which are NOT in the face, and all the pixels opaque which are within/on the face (circle) boundary. The pseudo code for this should look something like this:\n# Assuming you got a circle equation representing the face face = circle_equation # Now iterate through all the pixel values in the imagge # Check if the pixel is outside the face, if yes - then make it transparent # Else - opaque for pixel_value in image: if pixel_value is outside the face: # Make this pixel transparent else: # Make this pixel opaque # This will be visible To have an option to add transparency, you need to convert the BGR input image to BGRA (4 channel image: Blue, Green, Red, Alpha) - here Alpha channel denotes transparency channel. When the transparency is set to 0, that represents opaque and when it\u0026rsquo;s set to 255, it represents transparent value. Let\u0026rsquo;s go ahead and use this for our application.\nVideo Tutorial I started a YouTube channel where I go live on the weekends, and upload videos on the week days (not so regularly) about Computer Vision, deploying models into production and more. If you haven\u0026rsquo;t seen it before, please check it out here. For this blog, I have already uploaded a detailed tutorial. Check it out here.\n  Step 1: Get face from face detection We have discussed this before, so we won\u0026rsquo;t go in details but for the sake of continuity, I\u0026rsquo;ll add the code for Face Detection.\nimport cv2, sys # Get image path and read image img_path = sys.argv[1] if len(sys.argv) \u0026gt; 1 else \u0026#34;sample.png\u0026#34; img = cv2.imread(img_path, 1) # Convert to grayscale, since Face Detection takes gray scale image as input gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY) # Initialize face detector from the model file face_detector = cv2.CascadeClassifier(\u0026#34;haarcascade_frontalface_default.xml\u0026#34;) # Detect faces from gray-scaled image, using default parameters (scaleFactor) faces = face_detector.detectMultiScale(gray, scaleFactor = 1.1, minNeighbors = 5) # Note: Format of faces will be: [ [top_left_x, top_left_y, w, h (for face 1)], [... (for face 2)], ... ] Once you have ROI (Region Of Interest) of the faces in the image, we can go ahead and start cropping a circle (yay!).\nStep 2: Get circle circumscribing the face From Step-1, we got the faces. Let\u0026rsquo;s iterate through each face, and get the equation of the circle circumscribing that face. As we discussed before in the Intuition section, we\u0026rsquo;ll have to calculate the radius and center of the face.\n# Make sure to convert BGR to BGRA image first # To be used later imgTransp = cv2.cvtColor(img, cv2.COLOR_BGR2BGRA) # Iterate through each face one by one for face in faces: top_left_x, top_left_y, width, height = face[0], face[1], face[2], face[3] radius = max(width, height)/2 center = (top_left_x + width/2, top_left_y + height/2) # Cropping circle code will come here Here, we find the circle contained in the rectangle (closest) for each face. Let\u0026rsquo;s go ahead and crop this circle one by one, and see how we can use this for Portrait Bokeh!\nStep 3: Crop the circle and store it in different array We discussed the pseudo code of this in Cropping a Circle section of this blog. But before this, we have to figure out: How to find a point is within that circle? Think of this as a simple maths problem where you have to find a given coordinate is inside a circle or not. What would you do?\n Find distance between point and center of the circle. If distance is greater than radius, it\u0026rsquo;s outside. If distance is equal to radius, it\u0026rsquo;s on the boundary. If distance is less than radius, it\u0026rsquo;s inside.  We can simplify this for circle as we know it\u0026rsquo;s equation: (point_x - center_x)^2 + (point_y - center_y)^2 - radius^2, which will be:\n 0 if the point is on the boundary. greater than 0 if the point is outside the circle. less than 0 if the point is inside the circle.  Let\u0026rsquo;s use this concept here:\ndef is_inside(point, center, radius): \u0026#34;\u0026#34;\u0026#34; This function returns True if point is inside/on the boundary, False otherwise \u0026#34;\u0026#34;\u0026#34; eqn = (point[0] - center[0]) ** 2 + (point[1] - center[1])**2 - radius**2 return eqn \u0026lt;= 0 # Iterate through all the pixels in the image for row in range(img.shape[0]): for col in range(img.shape[1]): if is_inside(img[row][col], center, radius): # Means the point is inside/on the face # Make it opaque img[row][col][3] = 0 else: # Means the point is outside the face # Make it transparent img[row][col][3] = 255 We will have to execute the for loop once for each face, which means the code becomes:\n# Make sure to convert BGR to BGRA image first # To be used later imgTransp = cv2.cvtColor(img, cv2.COLOR_BGR2BGRA) # Make the whole image transparent, by default imgTransp[..., 3] = 255 def is_inside(point, center, radius): \u0026#34;\u0026#34;\u0026#34; This function returns True if point is inside/on the boundary, False otherwise \u0026#34;\u0026#34;\u0026#34; eqn = (point[0] - center[0]) ** 2 + (point[1] - center[1])**2 - radius**2 return eqn \u0026lt;= 0 # Iterate through each face one by one for face in faces: top_left_x, top_left_y, width, height = face[0], face[1], face[2], face[3] radius = max(width, height)/2 center = (top_left_x + width/2, top_left_y + height/2) # Cropping circle code will come here # Iterate through all the pixels in the image for row in range(img.shape[0]): for col in range(img.shape[1]): if is_inside(img[row][col], center, radius): # Means the point is inside/on the face # Make it opaque imgTransp[row][col][3] = 255 else: # Means the point is outside the face # Make it transparent imgTransp[row][col][3] = 0 If you notice, we have 3 nested for loops, which will result into really non-efficient code for HD Images (1920x1080 images? Think of the number of computations happening in this case). Suppose we have 3 faces detected and our input image is 1920 x 1080 (width x height). Total number of times the function is_inside called will be: 3 x 1920 x 1080, which is 6220800 (approx. 6.2 Million or 62 Lacs). It\u0026rsquo;s a lot!\nWe can not avoid these loops though, but why iterate through the whole image when you know the circle is anyways gonna be within that rectangle (face)! Imagine the face is 200 x 200 now, and everything remains same (3 faces, HD input Image: 1920 x 1080). If we only iterate through the face everytime, the computations will be: 3 * 200 * 200, which is 120000 (120 thousand or 1.2 lacs). Much better. All we have to do is, pick the face ROI, and iterate through that region. Everything else remains same:\nfor row in range(tly, tly + height): for col in range(tlx, tlx + width): if is_inside(img[row][col], center, radius): # Means the point is inside/on the face # Make it opaque imgTransp[row][col][3] = 255 else: # Means the point is outside the face # Make it transparent imgTransp[row][col][3] = 0 So, the code should look like this:\n# Make sure to convert BGR to BGRA image first # To be used later imgTransp = cv2.cvtColor(img, cv2.COLOR_BGR2BGRA) # Make the whole image transparent by default imgTransp[..., 3] = 255 def is_inside(point, center, radius): \u0026#34;\u0026#34;\u0026#34; This function returns True if point is inside/on the boundary, False otherwise \u0026#34;\u0026#34;\u0026#34; eqn = (point[0] - center[0]) ** 2 + (point[1] - center[1])**2 - radius**2 return eqn \u0026lt;= 0 # Iterate through each face one by one for face in faces: top_left_x, top_left_y, width, height = face[0], face[1], face[2], face[3] radius = max(width, height)/2 center = (top_left_x + width/2, top_left_y + height/2) # Cropping circle code will come here # Iterate through all the pixels in the image for row in range(tly, tly + height): for col in range(tlx, tlx + width): if is_inside((col, row), center, radius): # Means the point is inside/on the face # Make it opaque imgTransp[row][col][3] = 255 else: # Means the point is outside the face # Make it transparent imgTransp[row][col][3] = 0 Let\u0026rsquo;s try to visualize imgTransp here and see how this looks like:\nThis looks good so far! We have cropped the circle (face), and all we need to do now is - overlay this image on a blurred image. Let\u0026rsquo;s head straight to Step 4.\nStep 4: Blurring and overlaying In Step 3, we were able to crop the circle. But think about this, whenever we know the pixel is inside the face, let\u0026rsquo;s just replace the blurred pixel with original image.\n# Blur the whole image first img_blurred = cv2.GaussianBlur(img, (11, 11), 0) # Iterate through the faces we were doing before # Whenever the pixel is inside, replace the point at img_blurred with original img # Make sure to convert BGR to BGRA image first # To be used later imgTransp = cv2.cvtColor(img, cv2.COLOR_BGR2BGRA) # Make the whole image transparent by default imgTransp[..., 3] = 255 def is_inside(point, center, radius): \u0026#34;\u0026#34;\u0026#34; This function returns True if point is inside/on the boundary, False otherwise \u0026#34;\u0026#34;\u0026#34; eqn = (point[0] - center[0]) ** 2 + (point[1] - center[1])**2 - radius**2 return eqn \u0026lt;= 0 # Iterate through each face one by one for face in faces: top_left_x, top_left_y, width, height = face[0], face[1], face[2], face[3] radius = max(width, height)/2 center = (top_left_x + width/2, top_left_y + height/2) # Cropping circle code will come here # Iterate through all the pixels in the image for row in range(tly, tly + height): for col in range(tlx, tlx + width): if is_inside((col, row), center, radius): # Means the point is inside/on the face # Make it opaque imgTransp[row][col][3] = 255 # Replace pixel of blurred image with original image imgBlurred[row][col] = img[row][col] else: # Means the point is outside the face # Make it transparent imgTransp[row][col][3] = 0 Note: The only reason we used imgTransp here, is to show how to crop a circle. For portrait bokeh, you don\u0026rsquo;t need to have imgTransp and transparency channels.\nHere is how the output looks like. While I understand that there is still some background, but we can definitely be better than this - and this will be our topic for the next blog!\nThis should be it for this blog, and I hope you learnt something new today. If you liked the content, please leave a comment below. I would love to read your feedbacks, suggestions and if this helped you out in any way. I also go live on weekends, and upload videos on weekdays on my YouTube Channel, so make sure to subscribe there and join me in if you find it interesting! Thank you for reading this blog.\n","permalink":"https://krshrimali.github.io/posts/2020/12/how-to-crop-a-circle-in-opencv-implementing-portrait-bokeh-part-2/","tags":["computer vision","python","portrait bokeh","image processing"],"title":"How to crop a circle in OpenCV? Implementing Portrait Bokeh - Part 2"},{"categories":["opencv"],"contents":"OpenCV: Using face detection for Portrait Bokeh (Background Blur) (Part - 1) This blog discusses using Face Detection in OpenCV for Portrait Bokeh. We\u0026rsquo;ll be implementing Portrait Bokeh (blurring everything but faces) using 3 different methods in this series:\n Using Face Detection (cropping a rectangle) Using Face Detection (cropping a circle) Using Facial Landmark Detection and Convex Hull  Don\u0026rsquo;t lose hopes if you are confused. We will be going through each method one by one, and hopefully the road will be crearer from here.\nPortrait Bokeh: Discussing Problem Statement Before moving ahead, let\u0026rsquo;s talk about \u0026ldquo;What is Portrait Bokeh?\u0026rdquo;. It\u0026rsquo;s important to talk about the problem before discussing solutions. Take a quick look at the two images below:\nAs you might have spotted the difference already, the image on the left is our input (/original) image while the image on the right is our output image. If you haven\u0026rsquo;t spotted the difference, everything except the face in the image on the right is blurred! This feature now comes in almost all smart phones, and is also termed as just Portrait mode. Whenever you want to highlight the people near to the camera (mostly you, your friends or anyone) and blur the background, this is the mode you will usually choose. While some blur everything except faces, others might choose to keep the body instead of just faces. Our problem statement will be limited to faces here.\nMethodology opted Let\u0026rsquo;s discuss on how we can go ahead to solve this problem. We surely need to know where the face is to avoid blurring it, so the first step has to be of face detection. And since we need to blur the background, so at some stage, we need to do blurring as well. Since this part is about the simplest step, we can just combine them and say:\n Detect face(s) from the given input image. Crop the faces and store them as separate objects. Blur the whole image. Overlay the cropped faces from step-2 on the output from step-3.  Video Tutorial I started a YouTube channel where I go live on the weekends, and upload videos on the week days (not so regularly) about Computer Vision, deploying models into production and more. If you haven\u0026rsquo;t seen it before, please check it out here. For this blog, I have already uploaded a detailed tutorial. Check it out here.\n  Step 1: Detecting Faces using Haarcascade We\u0026rsquo;ll be using haarcascade model files to detect face in the image. To ease the computation and satisfy the input to the model, we need to first convert the image to GrayScale (if it\u0026rsquo;s not already) - that is the image will now have only one channel instead of 3 (Blue, Green, Red). Download the model file to your directory from here. Let\u0026rsquo;s go ahead and initialize our Face Detector.\nmodel_path = \u0026#34;haarcascade_frontalface_default.xml\u0026#34; # Assuming this is in our current directory face_detector = cv2.CascadeClassifier(model_path) Once we have the model loaded, let\u0026rsquo;s go ahead and detect faces from the given image. Remember, that we will also convert the image to grayscale.\n# Read input image (get image path first from command line, else take sample.png - default) img_path = self.argv[1] if len(sys.argv) \u0026gt; 1 else \u0026#34;sample.png\u0026#34; img = cv2.imread(img_path, 1) # Convert the image to grayscale gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY) # Get faces # Use default arguments, scaleFactor can be tweaked depending on the image # The output will be in format: [ [\u0026lt;top left x coord\u0026gt;, \u0026lt;top left y\u0026gt;, \u0026lt;width\u0026gt;, \u0026lt;height\u0026gt; : for face 1], [ ... : for face 2], ... ] faces = face_detector.detectMultiScale(gray, scaleFactor=1.1, minNeighbors=5) Once we have the faces, we can crop them and use in the Step-4 again. The output from face detection should look like this:\nStep 2: Crop faces To crop them and store in another object:\ncropped_faces = [] for face in faces: # Get points: tlx (top left x), tly (top left y), w (width), h (height) tlx, tly, w, h = face[0], face[1], face[2], face[3] cropped_faces.append( face[tly:tly+h, tlx:tlx+w] ) The list cropped_faces will now contain only faces. We can use this list again in Step-4!\nStep 3 and Step 4: Blur the image and overlay faces Let\u0026rsquo;s blur the whole image, and then overlay the images on the top of it. To blur, we will be using Gaussian Blur which works just fine.\nblur = cv2.GaussianBlur(img, (11, 11)) # Here, (11, 11) is the kernel size Once the whole image has been blurred, let\u0026rsquo;s overlay the cropped faces from Step 2.\nfor face_index, cropped_face in enumerate(cropped_faces): # Get face coordinates, to get ROI face_coords = faces[face_index] tlx, tly, w, h = face_coords[0], face_coords[1], face_coords[2], face_coords[3] # Overlay the ROI of face to the cropped face blur[tly:tly+h, tlx:tlx+w] = cropped_face Following image explains the procedure in details with visualization.\nAnd this is how the output (on the right) will look like (see below).\nWhile I know many of you will be thinking that it\u0026rsquo;s not accurate at all (since we can see the rectangle there), and that will be a topic for the next blog where we will attempt to crop a circle. Make sure to leave a comment if you have any suggestions, feedback or if this blog helped you in any way - I would love to hear that!\n","permalink":"https://krshrimali.github.io/posts/2020/12/implementing-portrait-bokeh-in-opencv-using-face-detection-part-1/","tags":["computer vision","python","portrait bokeh","image processing"],"title":"Implementing Portrait Bokeh in OpenCV using Face Detection (Part-1)"},{"categories":["pytorch","deep learning"],"contents":"Today, I am elated to share Docker image for OpenCV, Libtorch and Xeus-Cling. We\u0026rsquo;ll discuss how to use the dockerfile and binder.\nBefore I move on, the credits for creating and maintaining Docker image goes to Vishwesh Ravi Shrimali. He has been working on some cool stuff, please do get in touch with him if you\u0026rsquo;re interested to know.\nFirst question in your mind would be, Why use Docker or Binder? The answer to it lies in the frequency of queries on the discussion forum of PyTorch and Stackoverflow on Installation of Libtorch with OpenCV in Windows/Linux/OSX. I\u0026rsquo;ve had nightmares setting up the Windows system myself for Libtorch and nothing could be better than using Docker. Read on, to know why.\nInstalling Docker on Mac OS To install docker (community edition - CE) desktop in Mac OS system, simply navigate to the Stable Channel section here. Once setup, you can use docker (command line and desktop). Once done, navigate to Install and run Docker for Mac section and get used to the commands.\nInstalling Docker on Ubuntu Before moving on, please consider reading the requirements to install Docker Community Edition](https://docs.docker.com/v17.12/install/linux/docker-ce/ubuntu/). For the steps to install Docker CE, refer this.\nInstalling Docker on Windows To install Docker on Windows, download docker (stable channel) from here. The installation steps to install Docker Desktop on Windows can be found here.\nUsing Docker Image  Fetch the docker image: docker pull vishwesh5/libtorch-opencv:opencv-4-1-0. This shall take a lot of time, so sit back and relax. Run: docker run -p 5000:5000 -p 8888:8888 -it vishwesh5/libtorch-opencv:opencv-4-1-0 /bin/bash.  To know more about these commands, check out the references section.\nOnce done, you\u0026rsquo;ll see your terminal showing another username: jovyan. You\u0026rsquo;ve entered the docker image, congratulations! No need to setup OpenCV or Libtorch. Vishwesh has done it for you!\nNow since you have entered the docker container successfully, it should look something similar to this:\nTime to test Libtorch. Let\u0026rsquo;s go ahead and test a simple VGG-Net on MNIST dataset using Libtorch.\nTesting Docker Image  Clone the repository containing code for Digit Classification using Libtorch on MNIST dataset: git clone https://github.com/krshrimali/Digit-Recognition-MNIST-SVHN-PyTorch-CPP.git. Change directory to the cloned repository. Download the MNIST data from http://yann.lecun.com/exdb/mnist/. Download train-images-idx3-ubyte.gz and train-labels-idx1-ubyte.gz files for training the VGG-Net. You can skip downloading the test data for now. Use gunzip \u0026lt;file_path\u0026gt; to extract the training images and labels, and put them in the data/ folder inside the clones repository. Create a build folder: mkdir build Run the CMake Configuration using: cmake -DCMAKE_PREFIX_PATH=/opt/libtorch ... The result should be similar to something in the figure below. Build the code using make command: make. Execute the code, and that\u0026rsquo;s it. Have fun learning.  Testing Docker Image with Xeus-Cling Let\u0026rsquo;s test the Docker Image with Xeus-Cling.\n Run jupyter notebook command in the console and copy the token from the url provided. Open http://localhost:8888 in your browser. Note that the port address (8888) comes from -p 8888:8888 in the docker run command. You can change that if you want. Enter the token when asked. Start a new notebook using C++XX kernel. Include and load libraries in the first cell using: #include \u0026quot;includeLibraries.h\u0026quot;. This should do all the stuff for you. Start doing experiments using Xeus-Cling now.  Using Binder And! What if you just want to try Libtorch or show it to the students? What if you are on a remote PC, and can\u0026rsquo;t install Docker? Well, here is the Binder: https://mybinder.org/v2/gh/vishwesh5/torch-binder/master.\nGo to the above link and a notebook shall open.\nCreate a new notebook and start with: #include \u0026quot;includeLibraries.h\u0026quot; first and then start testing.\nAcknowledgements Thanks to Vishwesh Ravi Shrimali, for creating the docker container and binder for this post.\nReferences  Install OpenCV Docker Image on Ubuntu, MacOS or Windows by Vishwesh Ravi Shrimali.  ","permalink":"https://krshrimali.github.io/posts/2020/09/releasing-docker-container-and-binder-for-using-xeus-cling-libtorch-and-opencv-in-c-/","tags":["development","coding","cpp","notes","pytorch-cpp","libtorch"],"title":"Releasing Docker Container and Binder for using Xeus-Cling, Libtorch and OpenCV in C++"},{"categories":["cpp"],"contents":"In the last blog post, I realized there were a lot of methods inherited from the base struct _Vector_base_ and _Vector_impl_data. Instead of directly going to the source code of these structs, I\u0026rsquo;ll go through their methods and objects by explaining what happens when we initialize a vector.\nThat is, we will start from calling a vector constructor and then see how memory is allocated. If you haven\u0026rsquo;t looked at the previous blog post, please take a look here. I want to be thorough with the blog post, so I\u0026rsquo;ll divide this into multiple posts. By the end of this post, you\u0026rsquo;ll go through the following structs:\n _Vector_impl_data struct which contains pointers to memory locations (start, finish and end of storage). _Vector_impl struct (inherits _Vector_impl_data as well)).  I usually opt for the bottom-up approach. Vectors can be initialized in many ways, three of them will be discussed in today\u0026rsquo;s blog. We\u0026rsquo;ll start from the very basic constructor of a vector using an initializer list and slowly reach to memory allocation and how the above 2 structs are used. Let\u0026rsquo;s start!\nUsing Initializer Lists So what happens when we initialize a vector with an initializer list?\nstd::vector\u0026lt;int\u0026gt; vec {1, 2, 3}; The vector class has many constructors in GCC depending on the type of inputs you give. Let\u0026rsquo;s take a look at the constructor when the input is an initializer list:\nvector(initializer_list\u0026lt;value_type\u0026gt; __l, const allocator_type\u0026amp; __a = allocator_type()) : _Base(__a) { _M_range_initialize(__l.begin(), __l.end(), random_access_iterator_tag()); } If you are curious what _Base is, _Base is declared as: typedef _Vector_base\u0026lt;_Tp, _Alloc\u0026gt; _Base;. Just so you know, where and how is _Vector_base used. When the constructor is called, it calls the constructor of _Vector_base with __a (allocator type). As you might have noticed, we are calling _M_range_initialize and passing 2 iterators (__l.begin(), __l.end()) and 1 forward iterator tag.\nNote that the iterators are Forward Iterators, that is: we can use these iterators to access elements from begin (accessed using .begin()) till the end (accessed using .end()).\nWe are using random_access_iterator_tag as forward_iterator_tag. This tag helps us to categorize the iterator as random-access iterator. Random-access iterators allow accessing elements by passing arbitrary offset position (see: documentation for more details).\nLet\u0026rsquo;s go ahead and see what _M_range_initialize does.\ntemplate \u0026lt;typename _ForwardIterator\u0026gt; void _M_range_initialize(_ForwardIterator __first, _ForwardIterator __last, std::forward_iterator_tag) { const size_type __n = std::distance(__first, __last); this-\u0026gt;_M_impl._M_start = this-\u0026gt;_M_allocate(_S_check_init_len(__n, _M_get_Tp_allocator())); this-\u0026gt;_M_impl._M_end_of_storage = this-\u0026gt;_M_impl._M_start + __n; this-\u0026gt;_M_impl._M_finish = std::__uninitialized_copy_a(__first, __last, this-\u0026gt;_M_impl._M_start, _M_get_Tp_allocator()); } Let\u0026rsquo;s go line by line.\n First we find the distance using std::distance which takes first and last iterators, and returns size such as: __last = __first + size. Next, we allocate memory for __n objects. The function this-\u0026gt;_M_allocate returns pointer to the starting location of the memory allocated. static size_type _S_check_init_len(size_type __n, const allocator_type\u0026amp; __a) { if (__n \u0026gt; _S_max_size(_Tp_alloc_type(__a))) __throw_length_error( __N(\u0026#34;cannot create std::vector larger than max_size()\u0026#34;)); return __n; }  The function _S_check_init_len is called by constructors to check size. If the requested size is greater than the maximum size for the allocator type, it throws length error (\u0026quot;cannot create std::vector larger than max_size()\u0026quot;). Else, it returns __n. Once we have validated the size, this-\u0026gt;_M_allocate call allocates the memory. Note that, _M_allocate is a part of _Vector_base struct. _M_allocate allocates memory for __n number of objects. This returns a pointer to the memory location (starting), to _M_start. The end of storage pointer stores the end of memory location for the memory allocated for __n objects. The function std::__uninitialized_copy_a copies the range [__first, __last) into the this-\u0026gt;_M_impl._M_start. This returns a pointer to memory location starting at this-\u0026gt;_M_impl._M_start with length of __first - __last.    To summarize, when we initialized vector with initializer list:\n It first calculates the number of objects to allocate memory for. This is assigned to __n. Then, memory is allocated for __n objects (including a check if this much memory can be allocated based on the allocator type, if not then it returns a length error). The pointer _M_start points to the starting memory location. The end of storage is the end location of the storage. Since we have passed the initializer list, so it knows the end of storage is starting location + len(initializer_list). The elements are then copied the range [__first, __last) into the memory allocated.  Depending on how you initialize your vectors, the process may change but overall, the intention is the same: to allocate memory (if valid) and set pointers (start, end of storage and finish).\nUsing similar value and specified number of elements (fill) Let\u0026rsquo;s take a look at an example of using\nstd::vector\u0026lt;int\u0026gt; vec(10, 0); The above constructor call will give you a vector of 10 elements with all zeros. You can print the elements using:\n// Instead of using auto, we can use // for (std::vector\u0026lt;int\u0026gt;::iterator it = vec.begin(); it != vec.end(); it++) { // std::cout \u0026lt;\u0026lt; *it \u0026lt;\u0026lt; \u0026#34; \u0026#34;; // } for (auto it = vec.begin(); it != vec.end(); it++) { std::cout \u0026lt;\u0026lt; *it \u0026lt;\u0026lt; \u0026#34; \u0026#34;; } std::cout \u0026lt;\u0026lt; std::endl; Let\u0026rsquo;s see what changes when the vector is constructed in the above mentioned way. Let\u0026rsquo;s take a look at the constructor which is called:\nvector(size_type __n, const value_type\u0026amp; __value, const allocator_type\u0026amp; __a = allocator_type()) : _Base(_S_check_init_len(__n, __a), __a { _M_fill_initialize(__n, __value); } As the documentation of the above constructor explains, this constructor fills the vector with __n copies of __a value. Note the use of _S_check_init_len here (we discussed this before). Instead of calling _M_range_initialize, _M_fill_initialize is called here. For our example, this function is passed with values: 10 (__n) and 0 (__value). Let\u0026rsquo;s take a look at the definition of _M_fill_initialize:\nvoid _M_fill_initialize(size_type __n, const value_type\u0026amp; __value) { this-\u0026gt;_M_impl._M_finish = std::__uninitialized_fill_n_a(this-\u0026gt;_M_impl._M_start, __n, __value, _M_get_Tp_allocator()); } The call __uninitialized_fill_n copies the value (__value, here 0) into the range [this-\u0026gt;_M_impl._M_start, this-\u0026gt;_M_impl._M_start + __n) and returns the end of it\u0026rsquo;s range. As per the documentation, it is similar to fill_n() but does not require an initialized output range. Wait, you might be wondering, we didn\u0026rsquo;t initialize this-\u0026gt;_M_impl._M_start! We did! Note that we called _Base(_S_check_init_len(__n, __a) when the constructor is called. _Base is nothing but a typedef of _Vector_base. Let\u0026rsquo;s take a look at this call:\n_Vector_base(size_t __n) : _M_impl() { _M_create_storage(__n); }  _M_impl is an object of type _Vector_impl declared in _Vector_base struct. _M_create_storage(__n) is defined as: void _M_create_storage(size_t __n) { this-\u0026gt;_M_impl._M_start = this-\u0026gt;_M_allocate(__n); this-\u0026gt;_M_impl._M_finish = this-\u0026gt;_M_impl._M_start; this-\u0026gt;_M_impl._M_end_of_storage = this-\u0026gt;_M_impl._M_start + __n; } This will answer most of your queries. Let\u0026rsquo;s start line by line.\n this-\u0026gt;_M_allocate(__n) was discussed before, which allocates memory for __n objects. Please note that the constructor call _M_impl() had initialized these pointers for us. Here, the pointer is set to the starting memory location. Since the function _M_create_storage creates storage, and doesn\u0026rsquo;t copy elements to the memory location. So this-\u0026gt;_M_impl._M_finish is set to this-\u0026gt;_M_impl._M_start. The end of storage is, as before, set to this-\u0026gt;_M_impl._M_start + __n.    So, eventually, it\u0026rsquo;s quite similar to what we saw when we initialized our vector with initializer list.\nUsing another vector (copy) Let\u0026rsquo;s take a look at another way to another initalize a vector:\nstd::vector\u0026lt;int\u0026gt; vec_copy {1, 2, 3}; std::vector\u0026lt;int\u0026gt; vec(vec_copy); // Try printing the elements of vec for (auto it = vec.begin(); it != vec.end(); it++) { std::cout \u0026lt;\u0026lt; *it \u0026lt;\u0026lt; std::endl; } When you call vec(vec_copy), the copy constructor is called. Let\u0026rsquo;s take a look at it\u0026rsquo;s definition:\nvector(const vector\u0026amp; __x) : _Base(__x.size(), _Alloc_traits::_S_select_on_copy(__x._M_get_Tp_allocator()) { this-\u0026gt;_M_impl._M_finish = std::__uninitialized_copy_a(__x.begin(), __x.end(), this-\u0026gt;_M_impl._M_start, _M_get_Tp_allocator()); } The function body is similar to what we saw in the constructor definition when we initialized vector using size_type __n, value_type value. Notice how we initialize the base struct here. Let\u0026rsquo;s take a look at _S_select_on_copy(__x._M_get_Tp_allocator()) first. _M_get_Tp_allocator() returns _M_impl object.\nconst _Tp_alloc_type\u0026amp; _M_get_Tp_allocator() { return this-\u0026gt;_M_impl; } Note that, here, this-\u0026gt;_M_impl will already have the pointers set to the memory locations for start, finish and end of storage (as we use the allocator of __x). The objective is to use the copy of allocator object used by __x. Let\u0026rsquo;s take a look at the constructor of Base struct:\n_Vector_base(size_t __n, const allocator_type\u0026amp; __a) : _M_impl(__a) { _M_create_storage(__n); } Overall, it\u0026rsquo;s the same to what we saw before except that we use the copy of the alloactor of vector __x. The call _M_create_storage(__n) does the same task of setting pointers _M_start, M_end_of_storage, _M_finish as we observed before.\nFor today\u0026rsquo;s blog, we discussed 3 popular ways to initialize a vector in C++ and went through how memory is allocated when the constructors are called. As we move forward, we will slowly get familiar with the design patterns and methods used in GCC.\nAs always, I would love to hear your feedback on my blogs. Correct me if I was wrong anywhere, no one is perfect afterall. If this helped you, please let me know - it keeps me going! See you all in the next blog!\n","permalink":"https://krshrimali.github.io/posts/2020/04/understanding-how-vectors-work-in-c-part-2-what-happens-when-you-initialize-a-vector/","tags":["development","coding","cpp","notes","vectors"],"title":"Understanding how Vectors work in C++ (Part-2): What happens when you initialize a vector?"},{"categories":["cpp"],"contents":"In this blog, we\u0026rsquo;ll continue diving deep into the source code of Vector Containers in GCC compiler. Today, we will be discussing some of the most commonly used methods of vectors, and how they are implemented.\nBefore we start, if you haven\u0026rsquo;t looked at the previous blogs in the C++ series, please take a look here. If you are already familiar with memory allocation in vector containers and vector\u0026rsquo;s base structs, then you can skip reading the previous blogs and continue here. If not, I suggest you reading them.\nLet\u0026rsquo;s start off with pop_back member function, which essentially deletes the last element from the vector and reduces the size by one. Let\u0026rsquo;s take a look how it is used:\n# Initialize a vector using initializer list std::vector\u0026lt;int\u0026gt; X {1, 2, 3}; X.pop_back(); for (auto const\u0026amp; element: X) { std::cout \u0026lt;\u0026lt; element \u0026lt;\u0026lt; \u0026#34; \u0026#34;; } You will see the output as: 1 2. If you are wondering how this works in the case of a 2D vector, let\u0026rsquo;s take a look:\n# Initialize a 2D vector using initializer list std::vector\u0026lt;std::vector\u0026lt;int\u0026gt;\u0026gt; X { {1, 2, 3}, {4, 5, 6} }; X.pop_back(); for (auto const\u0026amp; element: X) { for (auto const\u0026amp; _element: element) { std::cout \u0026lt;\u0026lt; _element \u0026lt;\u0026lt; \u0026#34; \u0026#34;; } std::cout \u0026lt;\u0026lt; \u0026#34;\\n\u0026#34;; } You will see the output as: 1 2 3. As you can notice, it popped back the last element which was indeed a vector. Let\u0026rsquo;s start diving deep in the source code now, starting with declaration:\nvoid pop_back() { __glibcxx_required_nonempty(); __this-\u0026gt;_M_impl._M_finish; _Alloc_traits::destroy(this-\u0026gt;_M_impl, this-\u0026gt;_M_impl._M_finish); _GLIBCXX_ASAN_ANNOTATE_SHRINK(1); } A short note on _GLIBCXX_NOEXCEPT operator (noexcept since C++11): It returns true if the expression or member function is required to not throw any exceptions. _GLIBCXX_NOEXCEPT is defined as noexcept for C++ versions \u0026gt;= 2011:\nif __cplusplus \u0026gt;= 201103L # define _GLIBCXX_NOEXCEPT noexcept You can use a condition by using _GLIBCXX_NOEXCEPT_IF(condition) which essentially calls noexcept(condition). One use of this is when you want to access a particular index in a vector, you can avoid check if the location exists or not by using noexcept.\nWhen you call pop_back the design rule first checks if the vector is empty or not. If it\u0026rsquo;s nonempty, only then it makes sense to pop the last element, right? This is done by using __glibcxx_required_nonempty() call. The definition of this macro is:\n# define __glibcxx_requires_nonempty() __glibcxx_check_nonempty() As you can see, it\u0026rsquo;s calling __glibcxx_check_nonempty() macro which checks using this-\u0026gt;empty() call:\n# define __glibcxx_check_nonempty() \\ _GLIBCXX_DEBUG_VERIFY(! this-\u0026gt;empty(), _M_message(::__gnu_debug::__msg_empty)._M_sequence(*this, \u0026#34;this)) These are typical GCC macros for assertions. If we the vector is nonempty, we now move forward in fetching the last location in the memory of our vector container (using _M_impl._M_finish pointer), please take a look at the previous blogs if you aren\u0026rsquo;t aware of _M_impl struct. As the term suggests, we attempt to destroy the memory location using _Alloc_traits::destroy(this-\u0026gt;_M_impl, this-\u0026gt;_M_impl._M_finish). _Alloc_traits allows us to access various properties of the allocator used.\n// This function destroys an object of type _Tp template \u0026lt;typename _Tp\u0026gt; static void destroy(_Alloc\u0026amp; __a, _Tp\u0026amp; __p) noexcept(noexcept(_S_destroy(__a, __p, 0)) { _S_destroy(__a, __p, 0); } According to the official documentation of destroy static function: It calls __a.destroy(__p) if that expression is well-formed, other wise calls __p-\u0026gt;~_Tp(). If we take a look at the existing overloads of _S_destroy:\ntemplate \u0026lt;typename _Alloc2, typename _Tp\u0026gt; static auto _S_destroy(_Alloc2\u0026amp; __a, _Tp* __p, int) noexcept(noexcept(__a.destroy(__p))) -\u0026gt; decltype(__a.destroy(__p)) { __a.destroy(__p); } template \u0026lt;typename _Alloc2, typename _Tp\u0026gt; static void _S-destroy(_Alloc2\u0026amp; __a, _Tp* __p, ...) noexcept(noexcept(__p-\u0026gt;~_Tp())) { __p-\u0026gt;~_Tp(); } So clearly, if the expression is well-formed, it will call our allocator\u0026rsquo;s destroy method and pass the pointer location in that call. Otherwise, it calls the destructor of the pointer itself (__p-\u0026gt;~_Tp()). Once successfully done, we reduce the size by 1 using:\n# define _GLIBCXX_ASAN_ANNOTATE_SHRINK(n) \\ _Base::_Vector_impl::template _Asan\u0026lt;\u0026gt;::_S_shrink(this-\u0026gt;_M_impl, n) As you would see, the macro calls _S_shrink function to sanitize the vector container (i.e. reduce the size by n, here 1):\ntemplate \u0026lt;typename _Up\u0026gt; struct _Asan\u0026lt;allocator\u0026lt;_Up\u0026gt;\u0026gt; { static void _S_adjust(_Vector_impl\u0026amp; __impl, pointer __prev, pointer _curr) { __sanitizer_annotate_contiguous_container(__impl._M_start, __impl._M_end_of_storage, __prev, __curr); } static void _S_shrink(_Vector_impl\u0026amp; __impl, size_type __n) { _S_adjust(__impl, __impl._M_finish + __n, __impl._M_finish); } } We don\u0026rsquo;t need to go deeper into these calls, but (as per official documentation), the call _S_adjust adjusts ASan annotation for [_M_start, _M_end_of_storage) to mark end of valid region as __curr instead of __prev (note that we already had deleted the last element, so __impl.__M_finish + __n (here __n is 1) will be the old pointer).\nA good useful note here is, that pop_back function isn\u0026rsquo;t marked noexcept as we already have conditions to check the container being non-empty. In case there is any failure, the debug macros are called and throw necessary exceptions.\nLet\u0026rsquo;s go ahead and take a look at a few other member functions (there are many, take a look here: https://en.cppreference.com/w/cpp/container/vector, I only discuss those which are commonly used)\n  back(): Let\u0026rsquo;s take a look at back call. As the name suggests (and as we saw before), this returns the last element in the vector container. It can be used as X.back() where X is a valid vector container. Let\u0026rsquo;s take a look at how it is implemented in GCC:\nreference back() { _glibcxx_requires_nonempty(); return *(end() - 1); } // definition of end() iterator end() { return iterator(this-\u0026gt;_M_impl._M_finish); } Note that end() points to one past the last element in the vector. That\u0026rsquo;s why we do end()-1 in the definition of back function. This should now be pretty obvious, that why use assertion _glibcxx_requires_nonempty() as we want to make sure that we are returning valid memory location.\n  front(): It should be very similar to what we saw with back(). This returns reference to the first element of the vector.\nreference front() { _glibcxx_requires_nonempty(); return *begin(); } // definition of begin() iterator begin() { return iterator(this-\u0026gt;_M_impl._M_start); } Note how we use the pointers _M_start and _M_finish to access first and the last elements of the vector container respectively.\n  reserve(): Some times we want to pre-allocate memory to a vector container. You can do that using X.reserve(10) to reserve enough size for 10 elements (integers if X is std::vector\u0026lt;int\u0026gt; type).\nvoid reserve(size_type __n) { if (__n \u0026gt; max_size()) _throw_length_error(__N(\u0026#34;vector::reserve\u0026#34;)); if (capacity() \u0026lt; __n) _M_reallocate(__n); } So when you want to pre-allocate memory, there are 3 possibilities:\n There is already enough memory allocated. No need to allocate. (Case of capacity() \u0026gt; __n) There is not enough memory allocated. Need to reallocate memory. (Case of capacity() \u0026lt; __n) The required size is greater than maximum size possible, then lenght error is thrown. (Case of __n \u0026gt; max_size())    size(): This will return the size of the vector container:\nsize_type size() const { return size_type(end() - begin()); } So, let\u0026rsquo;s say you have reserved memory for 10 elements, then size() will return 10.\n  capacity(): This returns the size the container can store currently.\nsize_type capacity() const { return size_type(const_iterator(this-\u0026gt;_M_impl._M_end_addr(), 0) - begin()); } Here, _M_end_addr() returns address of (end of storage + 1) location (if the pointer to this-\u0026gt;_M_impl._M_end_of_storage exists).\n  There maybe a few member functions that I missed, but I\u0026rsquo;m sure the tutorials so far in the Vectors series are (hopefully) enough to help you out with understanding the source code.\nWith this blog post, we are also done with the vector series in C++, and coming up next, we will take a look on using all of this knowledge to implement useful utilities for vectors while implementing libraries and projects, and also other design patterns in C++.\nAcknowledgement I have received a lot of love and support for these blogs, and I am grateful to each and everyone of you! I write these blogs to share what I know with others and in a hope to motivate people to not fear when looking at the source code of any library. I think, reading codes is a good practice.\nI am thankful to Martin York (aka Loki Astari on stackoverflow) for his constructive feedback on my blogs. Special thanks to Ujval Kapasi for taking time to read through my blogs and giving valuable feedback.\nI was, am and will always be grateful to my elder brother Vishwesh Ravi Shrimali (also my all time mentor) who helped me getting started with C++, AI and whatever I have been doing recently. He inspires me.\n","permalink":"https://krshrimali.github.io/posts/2020/04/understanding-how-vectors-work-in-c-part-3-diving-deep-into-member-functions-of-vectors/","tags":["development","coding","cpp","notes","vectors"],"title":"Understanding how Vectors work in C++ (Part-3): Diving deep into member functions of vectors"},{"categories":["cpp"],"contents":"This blog is focused to explain how vectors work in the backend, and we\u0026rsquo;ll specially look at push_back method of the vector container. Looking at the source code helps to understand the implementation, and how vectors can be used efficiently.\nVector Containers are type of sequenced containers in C++ commonly uses as a better alternative of arrays. They are also known as dynamic arrays, and as the term suggests - it\u0026rsquo;s one of the advantages they hold over native arrays in C++. You might have heard of Standard Library containers like vector, set, queue, priority_queue before. They all implement methods defined by the Container Concept.\nA few important notes before we start:\n I\u0026rsquo;m using GCC 10.0.1 which is in the development stage. I\u0026rsquo;ve built GCC 10.0.1 from source on my local system. But everything I discuss here, should be same with GCC 8.4 or GCC 9.3 releases. I assume you are at least using C++11. If for any reason you are using C++98, there might be a few differences (for example, variadic arguments were not present in C++98). To not include lots of macros to check C++ versions, I\u0026rsquo;ve at times assumed the reader is using C++11 or greater. This blog uses lots of C++ Design Patterns that many would not be aware of. I understand it might just be a good idea to explain them first in a blog, but for now - I assume you have at least heard of them and know a thing or two about C++. I\u0026rsquo;ll cover these in future.  Let\u0026rsquo;s start with a basic comparison of using arrays and vectors in C++:\n// Create an array of fixed size: 10 int* InputArray = new int[10]; for (int i = 0; i \u0026lt; 10; i++) { // Let\u0026#39;s assign values to the array  // Values are same as indices  InputArray[i] = i; } We can do the same (from what you see above) using vector:\n// Include this to be able to use vector container #include \u0026lt;vector\u0026gt; std::vector\u0026lt;int\u0026gt; InputVector {}; for (int i = 0; i \u0026lt; 10; i++) { InputVector.push_back(i); } While both do the same, but there are many important differences that happen in the backend. Let\u0026rsquo;s start with performance.\n The piece of code using vector containers in C++ took 23.834 microseconds. The piece of code using arrays in C++ took 3.26 microseconds.  If we had to do this for 10k numbers, the performance might be significant:\n The piece of code using vector containers in C++ (for 10k numbers) took 713 microseconds. The piece of code using arrays in C++ took 173 microseconds.  As in software development, there is always a tradeoff. Since vectors aim to provide dynamic memory allocation, they lose some performance while trying to push_back elements in the vectors since the memory is not allocated before. This can be constant if memory is allocated before.\nLet\u0026rsquo;s try to infer this from the source code of vector container. The signature of a vector container looks like this:\ntemplate\u0026lt;typename _Tp, typename _Alloc = std::allocator\u0026lt;_Tp\u0026gt; \u0026gt; class vector : protected _Vector_base\u0026lt;_Tp, _Alloc\u0026gt; Where _Tp is the type of element, and _Alloc is the allocator type (defaults to std::allocator\u0026lt;_Tp\u0026gt;). Let\u0026rsquo;s start from the constructor of vector (when no parameter is passed):\n#if __cplusplus \u0026gt;= 201103L  vector() = default; #else  vector() { } #endif The constructor when called with no params, creates a vector with no elements. As always, there are various ways to initialize a vector object.\nI want to focus more on push_back today, so let\u0026rsquo;s take a look at it\u0026rsquo;s signature. It\u0026rsquo;s located in stl_vector.h file.\n// Note that value_type is defined as: typedef _Tp value_type as a public type void push_back(const value_type\u0026amp; __x) { if (this-\u0026gt;_M_impl._M_finish != this-\u0026gt;_M_impl._M_end_of_storage) { _GLIBCXX_ASAN_ANNOTATE_GROW(1); _Alloc_traits::construct(this-\u0026gt;_M_impl, this-\u0026gt;_M_impl._M_finish, __x); ++this-\u0026gt;_M_impl._M_finish; _GLIBCXX_ASAN_ANNOTATE_GREW(1); } else _M_realloc_insert(end(), __x); } A few notes to take:\n  value_type: This is the type of the elements in the vector container. That is, if the vector is std::vector\u0026lt;std::vector\u0026lt;int\u0026gt; \u0026gt;, then value_type of the given vector will be std::vector\u0026lt;int\u0026gt;. This comes handy later for type checking and more.\n  _GLIBCXX_ASAN_ANNOTATE_GROW(1): The definition of this macro is:\n#define _GLIBCXX_ASAN_ANNOTATE_GROW(n) \\ typename _Base::_Vector_impl::template _Asan\u0026lt;\u0026gt;::_Grow \\ __attribute__((__unused__)) __grow_guard(this-\u0026gt;_M_impl, (n))  The base struct _Vector_base defines these functions and structs. Let\u0026rsquo;s take a look at struct _Asan. Essentially, all we want to do with the above macro is to grow the vector container memory by n. Since when we insert an element, we only need to grow by 1, so we pass 1 to the macro call.  template\u0026lt;typename = _Tp_alloc_type\u0026gt; struct _Asan { typedef typename __gnu_cxx::__alloc_traits\u0026lt;_Tp_alloc_type\u0026gt;::size_type size_type; struct _Grow { _Grow(_Vector_impl\u0026amp;, size_type) { } void _M_grew(size_type) { } }; // ...  }; If usage of Macros is new to you, please leave it for now as we\u0026rsquo;ll discuss more about these design patterns in future.\n  A note on usage of _M_impl. It is declared as: _Vector_impl\u0026amp; _M_impl in the header file. _Vector_impl is a struct defined as:\nstruct _Vector_impl : public _Tp_alloc_type, public _Vector_impl_data { _Vector_impl() _GLIBCXX_NOEXCEPT_IF(is_nothrow_default_constructible\u0026lt;_Tp_alloc_type\u0026gt;::value) : _Tp_alloc_type() { } } // more overloads for the constructor The base struct _Vector_impl_data gives you helpful pointers to access later on:\nstruct _Vector_impl_data { pointer _M_start; pointer _M_finish; pointer _M_end_of_storage; // overloads of constructors } To go deep into the details is not useful here, but as you would have sensed, this helps us to access pointer to the start, finish and end of storage of the vector.\n  You would have guessed by now, that push_back call will add the element to the end (observe _Alloc_traits::construct(this-\u0026gt;_M_impl, this-\u0026gt;_M_impl._M_finish, __x);) and will then increment the variable _M_finish by 1.\nNote how push_back first checks if there is memory available. Of course we have limited memory available with us, and it checks if the end location of the current vector container equals the end storage capacity:\nif (this-\u0026gt;_M_impl._M_finish != this-\u0026gt;_M_impl._M_end_of_storage) { // ... } else { _M_realloc_insert(end(), __x); } So if we have reached the end of storage, it calls _M_realloc_insert(end(), __x). Now what is this? Let\u0026rsquo;s take a look at it\u0026rsquo;s definition:\ntemplate \u0026lt;typename _Tp, typename _Alloc\u0026gt; template\u0026lt;typename... _Args\u0026gt; void vector\u0026lt;_Tp, _Alloc\u0026gt;::_M_realloc_insert(iterator __position, _Args\u0026amp;\u0026amp;... __args) { // ...  pointer __old_start = this-\u0026gt;_M_impl._M_start; pointer __old_finish = this-\u0026gt;_M_impl._M_finish; // Here we have passed __position as end()  // So __elems_before will be total number of elements in our original vector  const size_type __elems_before = __position - begin(); // Declare new starting and finishing pointers  pointer __new_start(this-\u0026gt;_M_allocate(__len)); pointer __new_finish(__new_start); __try { // Allocate memory and copy original vector to the new memory locations  } __catch(...) // Destroy the original memory location  std::_Destroy(__old_start, __old_finish, _M_get_Tp_allocator()); // Change starting, finishing and end of storage pointers to new pointers  this-\u0026gt;_M_impl._M_start = __new_start; this-\u0026gt;_M_impl._M_finish = __new_finish; // here __len is 1  this-\u0026gt;_M_impl._M_end_of_storage = __new_start + __len; } Even though the above piece of code might scare a few (it did scare me when I looked at it for the first time), but just saying - this is just 10% of the definition of _M_realloc_insert.\nIf you haven\u0026rsquo;t noticed so far, there is something very puzzling in the code: template\u0026lt;typename... _Args\u0026gt; \u0026ndash; these are variadic arguments introduced in C++11. We\u0026rsquo;ll talk about them later in the series of blogs.\nIntuitively, by calling _M_realloc_insert(end(), __x) all we are trying to do is reallocate memory (end_of_storage + 1), copy the original vector data to the new memory locations, add __x and deallocate (or destroy) the original memory in the heap. This also allows to keep vector to have contiguous memory allocation.\nFor today, I think we discussed a lot about vectors and their implementation in GCC. We\u0026rsquo;ll continue to cover rest of the details in the next part of the blog. I\u0026rsquo;m sure, the next time you plan to use push_back - you\u0026rsquo;ll know how things are happening in the backend. Till then, have fun and take care! :)\nA request For the past year, I\u0026rsquo;ve been writing blogs on PyTorch C++ API. I\u0026rsquo;ve been overwhelmed with your feedback, help and comments. Thank you! This series of blogs on C++, is experimental for now. I love reading source codes, and explaining it to readers. I hope this helps. Please leave your comment and feedback here, or reach out to me at kushashwaravishrimali@gmail.com if you wish. Even if you don\u0026rsquo;t like this, say it! I promise, I\u0026rsquo;ll be better next time.\n","permalink":"https://krshrimali.github.io/posts/2020/04/understanding-how-vectors-work-in-c-part-1-how-does-push_back-work/","tags":["development","coding","cpp","notes","vectors"],"title":"Understanding how Vectors work in C++ (Part-1): How does push_back work?"},{"categories":["pytorch","deep learning"],"contents":"It\u0026rsquo;s been around 5 months since I released my last blog on DCGAN Review and Implementation using PyTorch C++ API and I\u0026rsquo;ve missed writing blogs badly! Straight the to the point, I\u0026rsquo;m back!\nBut before we start, the PyTorch C++ Frontend has gone through several changes and thanks to the awesome contributors around the world, it resembles the Python API more than it ever did! Since a lot of things have changed, I have also updated my previous blogs (tested on 1.4 Stable build).\nWhat has changed? There have been major changes in the PyTorch C++ Frontend API (Libtorch) and we\u0026rsquo;ll be discussing some of them which were related to our implementation on DCGAN. Let\u0026rsquo;s see, what parts of our code have changed in the recent Libtorch version. Well, the frontend API of PyTorch in C++ resembles closely to Python now:\nFor what concerns our code on DCGAN, quoting the author (Will Feng) of PR #28917.\n In Conv{1,2,3}dOptions: - with_bias is renamed to bias. - input_channels is renamed to in_channels. - output_channels is renamed to out_channels. - The value of transposed doesn\u0026rsquo;t affect the behavior of Conv{1,2,3}d layers anymore. Users should migrate their code to use ConvTranspose{1,2,3}d layers instead.\n So, starting first, we need to change with_bias to bias in our model definitions. The generator class in DCGAN uses Transposed Convolutions, and that\u0026rsquo;s why we need to migrate from torch::nn::Conv2dOptions class to torch::nn::ConvTranspose2dOptions (this is because using .transposed(true/false) does not work anymore on torch::nn::Conv2dOptions).\nThat is all for the changes we needed to make. To make it easy to track changes and use the code I wrote, I\u0026rsquo;ve made the project public on GitHub. Feel free to file an issue in case you hit a bug/error.\nTime to talk about results!\nResults The aim of this blog is to get DCGAN running on our celebA dataset using PyTorch C++ Frontend API. I\u0026rsquo;m in no way aiming to produce the best possible results. I trained the DCGAN network on celebA dataset for 10 epochs. In order to visualize results, for every checkpoint (where we save our models), we pass a sample noise image (64 images here) to the generator and save the output:\n// equivalent to using torch.no_grad() in Python auto options = torch::TensorOptions().device(device).requires_grad(false); // netG is our sequential generator network // args.nz = 100 in my case torch::Tensor samples = netG-\u0026gt;forward(torch::randn({64, args.nz, 1, 1}, options)); // save the output torch::save(samples, torch::str(\u0026#34;dcgan-sample-\u0026#34;, ++checkpoint_counter, \u0026#34;.pt\u0026#34;)); Once we have the saved output, we can load the file and produce output (find the display_samples.py file in the GitHub repo for this blog). Here is how the output looks like, after 10 epochs of training:\nAnd how about an animation?\nIsn\u0026rsquo;t this amazing?\nThat\u0026rsquo;s it for this blog. See you around! :)\n","permalink":"https://krshrimali.github.io/posts/2020/02/training-and-results-deep-convolutional-generative-adversarial-networks-on-celeba-dataset-using-pytorch-c-api/","tags":["development","coding","cpp","notes","pytorch-cpp","libtorch"],"title":"[Training and Results] Deep Convolutional Generative Adversarial Networks on CelebA Dataset using PyTorch C++ API"},{"categories":["pytorch","deep learning"],"contents":"I\u0026rsquo;m pleased to start a series of blogs on GANs and their implementation with PyTorch C++ API. We\u0026rsquo;ll be starting with one of the initial GANs - DCGANs (Deep Convolutional Generative Adversarial Networks).\nThe authors (Soumith Chintala, Radford and Luke Metz) in this Seminal Paper on DCGANs introduced DCGANs to the world like this:\n We introduce a class of CNNs called deep convolutional generative adversarial networks (DCGANs), that have certain architectural constraints, and demonstrate that they are a strong candidate for unsupervised learning. Training on various image datasets, we show convincing evidence that our deep convolutional adversarial pair learns a hierarchy of representations from object parts to scenes in both the generator and discriminator. Additionally, we use the learned features for novel tasks - demonstrating their applicability as general image representations.\n Even though, the introduction to DCGANs is quite lucid, but here are some points to note:\n DCGANs are a class of Convolutional Neural Networks. They are a strong candidate for Unsupervised Learning. They are applicable as general image representations as well.  Let\u0026rsquo;s go ahead and see what exactly is DCGAN?\nIntroduction to DCGAN At the time when this paper was released, there was quite a focus on Supervised Learning. The paper aimed at bridging the gap between Unsupervised Learning and Supervised Learning. DCGANs are a way to understand and extract important feature representations from a dataset and generate good image representations by training.\nAny Generative Adversarial Network has 2 major components: a Generator and a Discriminator. The tasks for both of them are simple.\n Generator: Generates Images similar to the data distribution such that Discriminator can not distinguish it with the original data. Discriminator: Discriminator has a task on accurately distinguishing between the image from the generator and from the data distribution. It basically has to recognize an image as fake or real, correctly.  Both Generator and Discriminator tasks can be represented beautifully with the following equation:\nThe above equation, shows how the Generator and Discriminator plays min-max game.\n The Generator tries to minimize the loss function. It follows up with two cases:  When the data is from the data distribution: Generator has a task of forcing the Discriminator to predict the data as fake. When data is from the Generator: Generator has a task of forcing the Discriminator to predict the data as real.   The Discriminator tries to maximize the loss function. It follows up with two cases:  When the data is from the data distribution: Discriminator tries to predict the data as real. When the data is from the Generator: Discriminator tries to predict the data as fake.    Fundamentally, the Generator is trying to fool the Discriminator and the Discriminator is trying not to get fooled with. Because of it\u0026rsquo;s analogy, it\u0026rsquo;s also called a police-thief game. (Police is the Discriminator and thief is the Generator).\nWe have good enough discussion on GANs, to kickstart discussion on DCGANs. Let\u0026rsquo;s go ahead and see what changes they proposed on common CNNs:\nChanges in the Generator:\n Spatial Pooling Layers such as MaxPool Layers were replaced with Fractional-Strided Convolutions (a.k.a Transposed Convolutions). This allows the network to learn it\u0026rsquo;s own spatial downsampling, instead of explicitly mentioning the downsampling parameters by Max Pooling. Use BatchNorm in the Generator. Remove Fully Connected layers for deeper architectures. Use ReLU activation function for all the layers except the output layer (which uses Tanh activation function).  Changes in the Discriminator:\n Spatial Pooling Layers such as MaxPool layers were replaced with Strided Convolutions. Use BatchNorm in the Discriminator. Remove FC layers for deeper architectures. Use LeakyReLU activation function for all the layers in the Discriminator.  Generator of the DCGAN used for LSUN scene modeling. Source: https://arxiv.org/pdf/1511.06434.pdfAs you would note in the above architecture, there is absence of spatial pooling layers and fully connected layers.\nDiscriminator of the DCGAN used for LSUN scene modeling. Source: https://github.com/ChengBinJin/DCGAN-TensorFlowNotably again, there are no pooling and fully connected layers (except the last layer).\nLet\u0026rsquo;s start with defining the architectures of both Generators and Discriminators using PyTorch C++ API. I used the Object Oriented approach by making class, each for Generator and Discriminator. Note that each of them are a type of CNNs, and also inherit functions (or methods) from torch::nn::Module class.\nAs mentioned before, Generator uses Transposed Convolutional Layers and has no pooling and FC layers. It also uses ReLU Activation Function (except the last layer). The parameters used for the Generator include:\n dataroot: (type: std::string) Path of the dataset\u0026rsquo;s root directory. workers: (type: int) Having more workers will increase CPU memory usage. (Check this link for more details) batch_size: (type: int) Batch Size to consider. image_size: (type: int) Size of the image to resize it to. nc: (type: int) Number of channels in the Input Image. nz: (type: int) Length of latent vector, from which the input image is taken. ngf: (type int) Depth of feature maps carried through the generator. num_epochs: (type int) Number of epochs for which the model is trained. lr: (type float) Learning Rate for training. Authors described it to be 0.0002 beta1: (type: float) Hyperparameter for Optimizer used (Adam). ngpu: (type: int) Number of GPUs available to use. (use 0 if no GPU available)  class Generator : public torch::nn::Module { private: std::string dataroot; int workers; int batch_size; int image_size; int nc; int nz; int ngf; int num_epochs; float lr; float beta1; int ngpu; public: torch::nn::Sequential main; Generator(std::string dataroot_ = \u0026#34;data/celeba\u0026#34;, int workers_ = 2, int batch_size_ = 128, int image_size_ = 64, int nc_ = 3, int nz_ = 100, int ngf_ = 64, int ndf_ = 64, int num_epochs_ = 5, float lr_ = 0.0002, float beta1_ = 0.5, int ngpu_ = 0) { // Set the arguments  dataroot = dataroot_; workers = workers_; batch_size = batch_size_; image_size = image_size_; nc = nc_; nz = nz_; ngf = ngf_; ndf = ndf_; num_epochs = num_epochs_; lr = lr_; beta1 = beta1_; ngpu = ngpu_; main = torch::nn::Sequential( torch::nn::Conv2d(torch::nn::Conv2dOptions(nz, ngf*8, 4).stride(1).padding(0).with_bias(false).transposed(true)), torch::nn::BatchNorm(ngf*8), torch::nn::Functional(torch::relu), torch::nn::Conv2d(torch::nn::Conv2dOptions(ngf*8, ngf*4, 4).stride(2).padding(1).with_bias(false).transposed(true)), torch::nn::BatchNorm(ngf*4), torch::nn::Functional(torch::relu), torch::nn::Conv2d(torch::nn::Conv2dOptions(ngf*4, ngf*2, 4).stride(2).padding(1).with_bias(false).transposed(true)), torch::nn::BatchNorm(ngf*2), torch::nn::Functional(torch::relu), torch::nn::Conv2d(torch::nn::Conv2dOptions(ngf*2, ngf, 4).stride(2).padding(1).with_bias(false).transposed(true)), torch::nn::BatchNorm(ngf), torch::nn::Functional(torch::relu), torch::nn::Conv2d(torch::nn::Conv2dOptions(ngf, nc, 4).stride(2).padding(1).with_bias(false).transposed(true)), torch::nn::Functional(torch::tanh) ); } torch::nn::Sequential main_func() { // Returns Sequential Model of the Generator  return main; } }; Note how we used Transposed Convolution, by passing .transposed(true).\nSimilarly, we define the class for Discriminator.\nclass Discriminator : public torch::nn::Module { private: std::string dataroot; int workers; int batch_size; int image_size; int nc; int nz; int ndf; int num_epochs; float lr; float beta1; int ngpu; public: torch::nn::Sequential main; Discriminator(std::string dataroot_ = \u0026#34;data/celeba\u0026#34;, int workers_ = 2, int batch_size_ = 128, int image_size_ = 64, int nc_ = 3, int nz_ = 100, int ngf_ = 64, int ndf_ = 64, int num_epochs_ = 5, float lr_ = 0.0002, float beta1_ = 0.5, int ngpu_ = 1) { dataroot = dataroot_; workers = workers_; batch_size = batch_size_; image_size = image_size_; nc = nc_; nz = nz_; ngf = ngf_; ndf = ndf_; num_epochs = num_epochs_; lr = lr_; beta1 = beta1_; ngpu = ngpu_; main = torch::nn::Sequential( torch::nn::Conv2d(torch::nn::Conv2dOptions(nc, ndf, 4).stride(2).padding(1).with_bias(false)), torch::nn::Functional(torch::leaky_relu, 0.2), torch::nn::Conv2d(torch::nn::Conv2dOptions(ndf, ndf*2, 4).stride(2).padding(1).with_bias(false)), torch::nn::BatchNorm(ndf*2), torch::nn::Functional(torch::leaky_relu, 0.2), torch::nn::Conv2d(torch::nn::Conv2dOptions(ndf*2, ndf*4, 4).stride(2).padding(1).with_bias(false)), torch::nn::BatchNorm(ndf*4), torch::nn::Functional(torch::leaky_relu, 0.2), torch::nn::Conv2d(torch::nn::Conv2dOptions(ndf*4, ndf*8, 4).stride(2).padding(1).with_bias(false)), torch::nn::BatchNorm(ndf*8), torch::nn::Functional(torch::leaky_relu, 0.2), torch::nn::Conv2d(torch::nn::Conv2dOptions(ndf*8, 1, 4).stride(1).padding(0).with_bias(false)), torch::nn::Functional(torch::sigmoid) ); } torch::nn::Sequential main_func() { return main; } }; We can initialize these networks as shown below:\n// Uses default arguments if no args passed Generator gen = Generator() Discriminator dis = Discriminator() torch::nn::Sequential gen_model = gen.main_func() torch::nn::Sequential dis_model = dis.main_func() In case you are using a GPU, you can convert the models:\ntorch::Device device = torch::kCPU; if(torch::cuda::is_available()) { device = torch::kCUDA; } gen_model-\u0026gt;to(device); dis_model-\u0026gt;to(device); Note on Data Loading: In the past blogs, I\u0026rsquo;ve discussed on loading custom data. Please refer to the previous blogs for a quick review on loading data.\nLet\u0026rsquo;s go ahead and define optimizers and train our model. We use the parameters defined by the authors, for optimizer (Adam, beta = 0.5) and learning rate of 2e-4.\ntorch::optim::Adam gen_optimizer(gen_model-\u0026gt;parameters(), torch::optim::AdamOptions(2e-4).beta1(0.5)); torch::optim::Adam dis_optimizer(dis_model-\u0026gt;parameters(), torch::optim::AdamOptions(2e-4).beta1(0.5)); Time to write our training code. We are using CelebA dataset which looks like this:\nThe dataset is huge, and contains 10,177 number of identities and around ~200k number of face images. It also contains annotations, but since GANs are a way of unsupervised learning, so they don\u0026rsquo;t actually consider annotations. Before we move on, we\u0026rsquo;ll see a quick step by step review on training the Discriminator and Generator simultaneously:\n Step-1: Train Discriminator. Remember from above, the discriminator tries to maximize the loss function such that it predicts the fake images as fake and real images as real.  As the first step for every training process, we set the gradients to zero. This helps in calculating correct gradients, and not getting confused with gradients stored in the previous iteration. First calculate discriminator loss on real images (that is, data from our dataset). We do this by getting data from the batch and labels as anything between 0.8 and 1.0 (since it\u0026rsquo;s real, we approximate it from 0.8 to 1.0). Do a forward pass to the discriminator network, and calculate output on the batch of data from our dataset. Calculate loss by using torch::binary_cross_entropy and backpropagate the loss. We now calculate discriminator loss on fake images (that is, data from the generator). For this, we take a noise of shape similar to the batch of data, and pass that noise to the generator. The labels are given zero values (as the images are fake). Again, calculate the loss by using torch::binary_cross_entropy and backpropagate the loss. Sum both the losses, discriminator loss on real images + discriminator loss on fake images. This will be our discriminator loss. We then update our parameters using the optimizer.   Step-2: Train Generator. The task of a Generator is to minimize the loss function. Since it has to produce images which can fool the discriminator, so it only has to consider fake images.  As the first step for every training process, we set the gradients to zero. This helps in calculating correct gradients, and not getting confused with gradients stored in the previous iteration. We use the fake images produced in the Step-1 and pass it to the discriminator. Fill the labels with 1. (since generator wants to fool the discriminator, by making it predict as real images). Calculate loss, by using torch::binary_cross_entropy and backpropagate the loss. Update the parameters using optimizer of the Generator.    for(int epoch=1; epoch\u0026lt;=10; epoch++) { // Store batch count in a variable  int batch_count = 0; // You can use torch::data::Example\u0026lt;\u0026gt;\u0026amp; batch: *data_loader  for(auto\u0026amp; batch: *data_loader) { // Step-1: Train the Discriminator  // Set gradients to zero  netD-\u0026gt;zero_grad(); // Calculating discriminator loss on real images  torch::Tensor images_real = batch.data.to(device); torch::Tensor labels_real = torch::empty(batch.data.size(0), device).uniform_(0.8, 1.0)); // Do a forward pass to the Discriminator network  torch::Tensor output_D_real = netD-\u0026gt;forward(images_real); // Calculate the loss  torch::Tensor loss_real_D = torch::binary_cross_entropy(output_D_real, labels_real); loss_real_D.backward(); // Calculate discriminator loss on fake images  // Generate noise and do forward pass to generate fake images  torch:Tensor fake_random = torch::randn({batch.data.size(0), args.nz, 1, 1}, device); torch::Tensor images_fake = netG-\u0026gt;forward(images_fake); torch::Tensor labels_fake = torch::zeros(batch.data.size(0), device); // Do a forward pass to the Discriminator network  torch::Tensor output_D_fake = netD-\u0026gt;forward(images_fake); // Calculate the loss  torch::Tensor loss_fake_D = torch::binary_cross_entropy(output_D_fake, labels_fake); loss_fake_D.backward(); // Total discriminator loss  torch::Tensor loss_discriminator = loss_real_D + loss_fake_D; // Update the parameters  dis_optimizer.step(); // Step-2: Train the Generator  // Set gradients to zero  netG-\u0026gt;zero_grad(); // calculating generator loss on fake images  // Change labels_fake from all zeros to all ones  labels_fake.fill_(1); // Do forward pass to the Discriminator on the fake images generated above  torch::Tensor output_G_fake = netD-\u0026gt;forward(images_fake); // Calculate loss  torch::Tensor loss_generator = torch::binary_cross_entropy(output_G_fake, labels_fake); loss_generator.backward(); // Update the parameters  gen_optimizer.step(); std::cout \u0026lt;\u0026lt; \u0026#34;Epoch: \u0026#34; \u0026lt;\u0026lt; epoch \u0026lt;\u0026lt; \u0026#34;, Batch: \u0026#34; \u0026lt;\u0026lt; batch_count \u0026lt;\u0026lt; \u0026#34;, Gen Loss: \u0026#34; \u0026lt;\u0026lt; loss_generator.item\u0026lt;float\u0026gt;() \u0026lt;\u0026lt; \u0026#34;, Discriminator Loss: \u0026#34; \u0026lt;\u0026lt; loss_discriminator.item\u0026lt;float\u0026gt;() \u0026lt;\u0026lt; std::endl; batch_count++; } } We are all set to train our first DCGAN in C++ using Libtorch. How amazing it is?\nIn the coming blog, I\u0026rsquo;ll share the results and answer a few common questions on the architecture of DCGAN.\nAcknowledgement and References I would like to thank Will Feng and Piotr for their useful suggestions. The code used in this blog, is partially analogous to the official PyTorch examples repo on DCGAN using LibTorch. I\u0026rsquo;ve also referred the original paper by Soumith Chintala and others. The sources of reference images (for Network architectures) have been acknowledged in the captions of respective images.\n","permalink":"https://krshrimali.github.io/posts/2019/09/deep-convolutional-generative-adversarial-networks-review-and-implementation-using-pytorch-c-api/","tags":["development","coding","cpp","notes","pytorch-cpp","libtorch"],"title":"Deep Convolutional Generative Adversarial Networks: Review and Implementation using PyTorch C++ API"},{"categories":["pytorch","deep learning"],"contents":"Introduction to Xeus Cling Today, we are going to run our C++ codes in the Jupyter Notebook. Sounds ambitious? Not much. Let\u0026rsquo;s see how we do it using Xeus Cling.\nI\u0026rsquo;ll quote the definition of Xeus Cling on the official documentation website.\n xeus-cling is a Jupyter kernel for C++ based on the C++ interpreter cling and the native implementation of the Jupyter protocol xeus.\n Just like we use Python Kernel in the Jupyter Notebook, we can also use a C++ based interpreter cling combined with a Jupyter protocol called Xeus to reach closer to implementing C++ code in the notebook.\nInstalling Xeus Cling using Anaconda It\u0026rsquo;s pretty straight forward to install Xeus Cling using Anaconda. I\u0026rsquo;m assuming the user has Anaconda installed. Use this command to install Xeus Cling using Anaconda: conda install -c conda-forge xeus-cling.\nNote: Before using conda commands, you need to have it in your PATH variable. Use this command to add the path to conda to your system PATH variable: export PATH=~/anaconda3/bin/:$PATH.\nThe conventional way to install any such library which can create conflicts with existing libraries, is to create an environment and then install it in the environment.\n Create a conda environment: conda create -n cpp-xeus-cling. Activate the environment you just created: source activate cpp-xeus-cling. Install xeus-cling using conda: conda install -c conda-forge xeus-cling.  Once setup, let\u0026rsquo;s go ahead and get started with Jupyter Notebook. When creating a new notebook, you will see different options for the kernel. One of them would be C++XX where XX is the C++ version.\nClick on any of the kernel for C++ and let\u0026rsquo;s start setting up environment for PyTorch C++ API.\nYou can try and implement some of the basic commands in C++.\nThis looks great, right? Let\u0026rsquo;s go ahead and set up the Deep Learning environment.\nSetting up Libtorch in Xeus Cling Just like we need to give path to Libtorch libraries in CMakeLists.txt or while setting up XCode (for OS X users) or Visual Studio (for Windows Users), we will also load the libraries in Xeus Cling.\nWe will first give the include_path of Header files and library_path for the libraries. We will also do the same for OpenCV as we need it to load images.\n#pragma cling add_library_path(\u0026#34;/Users/krshrimali/Downloads/libtorch/lib/\u0026#34;) #pragma cling add_include_path(\u0026#34;/Users/krshrimali/Downloads/libtorch/include\u0026#34;)#pragma cling add_include_path(\u0026#34;/Users/krshrimali/Downloads/libtorch/include/torch/csrc/api/include/\u0026#34;)#pragma cling add_library_path(\u0026#34;/usr/local/Cellar/opencv/4.1.0_2/lib\u0026#34;) #pragma cling add_include_path(\u0026#34;/usr/local/Cellar/opencv/4.1.0_2/include/opencv4\u0026#34;)For OS X, the libtorch libraries will be in the format of .dylib. Ignore the .a files as we only need to load the .dylib files. Similarly for Linux, load the libraries in .so format located in the lib/ folder.\nFor Mac\n#pragma cling load(\u0026#34;/Users/krshrimali/Downloads/libtorch/lib/libiomp5.dylib\u0026#34;) #pragma cling load(\u0026#34;/Users/krshrimali/Downloads/libtorch/lib/libmklml.dylib\u0026#34;) #pragma cling load(\u0026#34;/Users/krshrimali/Downloads/libtorch/lib/libc10.dylib\u0026#34;) #pragma cling load(\u0026#34;/Users/krshrimali/Downloads/libtorch/lib/libtorch.dylib\u0026#34;) #pragma cling load(\u0026#34;/Users/krshrimali/Downloads/libtorch/lib/libshm.dylib\u0026#34;) For Linux\n#pragma cling load(\u0026#34;/opt/libtorch/lib/libc10.so\u0026#34;) #pragma cling load(\u0026#34;/opt/libtorch/lib/libgomp-4f651535.so.1\u0026#34;) #pragma cling load(\u0026#34;/opt/libtorch/lib/libtorch.so\u0026#34;) For OpenCV, the list of libraries is long.\nFor Mac\n#pragma cling load(\u0026#34;/usr/local/Cellar/opencv/4.1.0_2/lib/libopencv_datasets.4.1.0.dylib\u0026#34;) #pragma cling load(\u0026#34;/usr/local/Cellar/opencv/4.1.0_2/lib/libopencv_aruco.4.1.0.dylib\u0026#34;) #pragma cling load(\u0026#34;/usr/local/Cellar/opencv/4.1.0_2/lib/libopencv_bgsegm.4.1.0.dylib\u0026#34;) #pragma cling load(\u0026#34;/usr/local/Cellar/opencv/4.1.0_2/lib/libopencv_bioinspired.4.1.0.dylib\u0026#34;) #pragma cling load(\u0026#34;/usr/local/Cellar/opencv/4.1.0_2/lib/libopencv_calib3d.4.1.0.dylib\u0026#34;) #pragma cling load(\u0026#34;/usr/local/Cellar/opencv/4.1.0_2/lib/libopencv_ccalib.4.1.0.dylib\u0026#34;) #pragma cling load(\u0026#34;/usr/local/Cellar/opencv/4.1.0_2/lib/libopencv_core.4.1.0.dylib\u0026#34;) #pragma cling load(\u0026#34;/usr/local/Cellar/opencv/4.1.0_2/lib/libopencv_dnn_objdetect.4.1.0.dylib\u0026#34;) #pragma cling load(\u0026#34;/usr/local/Cellar/opencv/4.1.0_2/lib/libopencv_dnn.4.1.0.dylib\u0026#34;) #pragma cling load(\u0026#34;/usr/local/Cellar/opencv/4.1.0_2/lib/libopencv_dpm.4.1.0.dylib\u0026#34;) #pragma cling load(\u0026#34;/usr/local/Cellar/opencv/4.1.0_2/lib/libopencv_face.4.1.0.dylib\u0026#34;) #pragma cling load(\u0026#34;/usr/local/Cellar/opencv/4.1.0_2/lib/libopencv_features2d.4.1.0.dylib\u0026#34;) #pragma cling load(\u0026#34;/usr/local/Cellar/opencv/4.1.0_2/lib/libopencv_flann.4.1.0.dylib\u0026#34;) #pragma cling load(\u0026#34;/usr/local/Cellar/opencv/4.1.0_2/lib/libopencv_freetype.4.1.0.dylib\u0026#34;) #pragma cling load(\u0026#34;/usr/local/Cellar/opencv/4.1.0_2/lib/libopencv_fuzzy.4.1.0.dylib\u0026#34;) #pragma cling load(\u0026#34;/usr/local/Cellar/opencv/4.1.0_2/lib/libopencv_gapi.4.1.0.dylib\u0026#34;) #pragma cling load(\u0026#34;/usr/local/Cellar/opencv/4.1.0_2/lib/libopencv_hfs.4.1.0.dylib\u0026#34;) #pragma cling load(\u0026#34;/usr/local/Cellar/opencv/4.1.0_2/lib/libopencv_highgui.4.1.0.dylib\u0026#34;) #pragma cling load(\u0026#34;/usr/local/Cellar/opencv/4.1.0_2/lib/libopencv_img_hash.4.1.0.dylib\u0026#34;) #pragma cling load(\u0026#34;/usr/local/Cellar/opencv/4.1.0_2/lib/libopencv_imgcodecs.4.1.0.dylib\u0026#34;) #pragma cling load(\u0026#34;/usr/local/Cellar/opencv/4.1.0_2/lib/libopencv_imgproc.4.1.0.dylib\u0026#34;) #pragma cling load(\u0026#34;/usr/local/Cellar/opencv/4.1.0_2/lib/libopencv_line_descriptor.4.1.0.dylib\u0026#34;) #pragma cling load(\u0026#34;/usr/local/Cellar/opencv/4.1.0_2/lib/libopencv_ml.4.1.0.dylib\u0026#34;) #pragma cling load(\u0026#34;/usr/local/Cellar/opencv/4.1.0_2/lib/libopencv_objdetect.4.1.0.dylib\u0026#34;) #pragma cling load(\u0026#34;/usr/local/Cellar/opencv/4.1.0_2/lib/libopencv_optflow.4.1.0.dylib\u0026#34;) #pragma cling load(\u0026#34;/usr/local/Cellar/opencv/4.1.0_2/lib/libopencv_phase_unwrapping.4.1.0.dylib\u0026#34;) #pragma cling load(\u0026#34;/usr/local/Cellar/opencv/4.1.0_2/lib/libopencv_photo.4.1.0.dylib\u0026#34;) #pragma cling load(\u0026#34;/usr/local/Cellar/opencv/4.1.0_2/lib/libopencv_plot.4.1.0.dylib\u0026#34;) #pragma cling load(\u0026#34;/usr/local/Cellar/opencv/4.1.0_2/lib/libopencv_quality.4.1.0.dylib\u0026#34;) #pragma cling load(\u0026#34;/usr/local/Cellar/opencv/4.1.0_2/lib/libopencv_reg.4.1.0.dylib\u0026#34;) #pragma cling load(\u0026#34;/usr/local/Cellar/opencv/4.1.0_2/lib/libopencv_rgbd.4.1.0.dylib\u0026#34;) #pragma cling load(\u0026#34;/usr/local/Cellar/opencv/4.1.0_2/lib/libopencv_saliency.4.1.0.dylib\u0026#34;) #pragma cling load(\u0026#34;/usr/local/Cellar/opencv/4.1.0_2/lib/libopencv_sfm.4.1.0.dylib\u0026#34;) #pragma cling load(\u0026#34;/usr/local/Cellar/opencv/4.1.0_2/lib/libopencv_shape.4.1.0.dylib\u0026#34;) #pragma cling load(\u0026#34;/usr/local/Cellar/opencv/4.1.0_2/lib/libopencv_stereo.4.1.0.dylib\u0026#34;) #pragma cling load(\u0026#34;/usr/local/Cellar/opencv/4.1.0_2/lib/libopencv_stitching.4.1.0.dylib\u0026#34;) #pragma cling load(\u0026#34;/usr/local/Cellar/opencv/4.1.0_2/lib/libopencv_structured_light.4.1.0.dylib\u0026#34;) #pragma cling load(\u0026#34;/usr/local/Cellar/opencv/4.1.0_2/lib/libopencv_superres.4.1.0.dylib\u0026#34;) #pragma cling load(\u0026#34;/usr/local/Cellar/opencv/4.1.0_2/lib/libopencv_surface_matching.4.1.0.dylib\u0026#34;) #pragma cling load(\u0026#34;/usr/local/Cellar/opencv/4.1.0_2/lib/libopencv_text.4.1.0.dylib\u0026#34;) #pragma cling load(\u0026#34;/usr/local/Cellar/opencv/4.1.0_2/lib/libopencv_tracking.4.1.0.dylib\u0026#34;) #pragma cling load(\u0026#34;/usr/local/Cellar/opencv/4.1.0_2/lib/libopencv_video.4.1.0.dylib\u0026#34;) #pragma cling load(\u0026#34;/usr/local/Cellar/opencv/4.1.0_2/lib/libopencv_videoio.4.1.0.dylib\u0026#34;) #pragma cling load(\u0026#34;/usr/local/Cellar/opencv/4.1.0_2/lib/libopencv_videostab.4.1.0.dylib\u0026#34;) #pragma cling load(\u0026#34;/usr/local/Cellar/opencv/4.1.0_2/lib/libopencv_xfeatures2d.4.1.0.dylib\u0026#34;) #pragma cling load(\u0026#34;/usr/local/Cellar/opencv/4.1.0_2/lib/libopencv_ximgproc.4.1.0.dylib\u0026#34;) #pragma cling load(\u0026#34;/usr/local/Cellar/opencv/4.1.0_2/lib/libopencv_xobjdetect.4.1.0.dylib\u0026#34;) #pragma cling load(\u0026#34;/usr/local/Cellar/opencv/4.1.0_2/lib/libopencv_xphoto.4.1.0.dylib\u0026#34;) For Linux\n#pragma cling load(\u0026#34;/usr/local/lib/libopencv_aruco.so.4.1.0\u0026#34;) #pragma cling load(\u0026#34;/usr/local/lib/libopencv_bgsegm.so.4.1.0\u0026#34;) #pragma cling load(\u0026#34;/usr/local/lib/libopencv_bioinspired.so.4.1.0\u0026#34;) #pragma cling load(\u0026#34;/usr/local/lib/libopencv_calib3d.so.4.1.0\u0026#34;) #pragma cling load(\u0026#34;/usr/local/lib/libopencv_ccalib.so.4.1.0\u0026#34;) #pragma cling load(\u0026#34;/usr/local/lib/libopencv_core.so.4.1.0\u0026#34;) #pragma cling load(\u0026#34;/usr/local/lib/libopencv_datasets.so.4.1.0\u0026#34;) #pragma cling load(\u0026#34;/usr/local/lib/libopencv_dnn_objdetect.so.4.1.0\u0026#34;) #pragma cling load(\u0026#34;/usr/local/lib/libopencv_dnn.so.4.1.0\u0026#34;) #pragma cling load(\u0026#34;/usr/local/lib/libopencv_dpm.so.4.1.0\u0026#34;) #pragma cling load(\u0026#34;/usr/local/lib/libopencv_face.so.4.1.0\u0026#34;) #pragma cling load(\u0026#34;/usr/local/lib/libopencv_features2d.so.4.1.0\u0026#34;) #pragma cling load(\u0026#34;/usr/local/lib/libopencv_flann.so.4.1.0\u0026#34;) #pragma cling load(\u0026#34;/usr/local/lib/libopencv_freetype.so.4.1.0\u0026#34;) #pragma cling load(\u0026#34;/usr/local/lib/libopencv_fuzzy.so.4.1.0\u0026#34;) #pragma cling load(\u0026#34;/usr/local/lib/libopencv_gapi.so.4.1.0\u0026#34;) #pragma cling load(\u0026#34;/usr/local/lib/libopencv_hdf.so.4.1.0\u0026#34;) #pragma cling load(\u0026#34;/usr/local/lib/libopencv_hfs.so.4.1.0\u0026#34;) #pragma cling load(\u0026#34;/usr/local/lib/libopencv_highgui.so.4.1.0\u0026#34;) #pragma cling load(\u0026#34;/usr/local/lib/libopencv_imgcodecs.so.4.1.0\u0026#34;) #pragma cling load(\u0026#34;/usr/local/lib/libopencv_img_hash.so.4.1.0\u0026#34;) #pragma cling load(\u0026#34;/usr/local/lib/libopencv_imgproc.so.4.1.0\u0026#34;) #pragma cling load(\u0026#34;/usr/local/lib/libopencv_line_descriptor.so.4.1.0\u0026#34;) #pragma cling load(\u0026#34;/usr/local/lib/libopencv_ml.so.4.1.0\u0026#34;) #pragma cling load(\u0026#34;/usr/local/lib/libopencv_objdetect.so.4.1.0\u0026#34;) #pragma cling load(\u0026#34;/usr/local/lib/libopencv_optflow.so.4.1.0\u0026#34;) #pragma cling load(\u0026#34;/usr/local/lib/libopencv_phase_unwrapping.so.4.1.0\u0026#34;) #pragma cling load(\u0026#34;/usr/local/lib/libopencv_photo.so.4.1.0\u0026#34;) #pragma cling load(\u0026#34;/usr/local/lib/libopencv_plot.so.4.1.0\u0026#34;) #pragma cling load(\u0026#34;/usr/local/lib/libopencv_reg.so.4.1.0\u0026#34;) #pragma cling load(\u0026#34;/usr/local/lib/libopencv_rgbd.so.4.1.0\u0026#34;) #pragma cling load(\u0026#34;/usr/local/lib/libopencv_saliency.so.4.1.0\u0026#34;) #pragma cling load(\u0026#34;/usr/local/lib/libopencv_sfm.so.4.1.0\u0026#34;) #pragma cling load(\u0026#34;/usr/local/lib/libopencv_shape.so.4.1.0\u0026#34;) #pragma cling load(\u0026#34;/usr/local/lib/libopencv_stereo.so.4.1.0\u0026#34;) #pragma cling load(\u0026#34;/usr/local/lib/libopencv_stitching.so.4.1.0\u0026#34;) #pragma cling load(\u0026#34;/usr/local/lib/libopencv_structured_light.so.4.1.0\u0026#34;) #pragma cling load(\u0026#34;/usr/local/lib/libopencv_superres.so.4.1.0\u0026#34;) #pragma cling load(\u0026#34;/usr/local/lib/libopencv_surface_matching.so.4.1.0\u0026#34;) #pragma cling load(\u0026#34;/usr/local/lib/libopencv_text.so.4.1.0\u0026#34;) #pragma cling load(\u0026#34;/usr/local/lib/libopencv_tracking.so.4.1.0\u0026#34;) #pragma cling load(\u0026#34;/usr/local/lib/libopencv_videoio.so.4.1.0\u0026#34;) #pragma cling load(\u0026#34;/usr/local/lib/libopencv_video.so.4.1.0\u0026#34;) #pragma cling load(\u0026#34;/usr/local/lib/libopencv_videostab.so.4.1.0\u0026#34;) #pragma cling load(\u0026#34;/usr/local/lib/libopencv_xfeatures2d.so.4.1.0\u0026#34;) #pragma cling load(\u0026#34;/usr/local/lib/libopencv_ximgproc.so.4.1.0\u0026#34;) #pragma cling load(\u0026#34;/usr/local/lib/libopencv_xobjdetect.so.4.1.0\u0026#34;) #pragma cling load(\u0026#34;/usr/local/lib/libopencv_xphoto.so.4.1.0\u0026#34;) Once done, run the cell and that\u0026rsquo;s it. We have successfully setup the environment for Libtorch and OpenCV.\nTesting Xeus Cling Notebook Let\u0026rsquo;s go ahead and include the libraries. I\u0026rsquo;ll be sharing the code snippets as well as the screenshots to make it easy for the readers to reproduce results.\n#include \u0026lt;torch/torch.h\u0026gt;#include \u0026lt;torch/script.h\u0026gt;#include \u0026lt;iostream\u0026gt;#include \u0026lt;dirent.h\u0026gt;#include \u0026lt;opencv2/opencv.hpp\u0026gt;After successfully importing libraries, we can define functions, write code and use the utilities Jupyter provides. Let\u0026rsquo;s start with playing with Tensors and the code snippets mentioned in the official PyTorch C++ Frontend Docs.\nStarting with using ATen tensor library. We\u0026rsquo;ll create two tensors and add them together. ATen comes up with functionalities of mathematical operations on the Tensors.\n#include \u0026lt;ATen/ATen.h\u0026gt; at::Tensor a = at::ones({2, 2}, at::kInt); at::Tensor b = at::randn({2, 2}); auto c = a + b.to(at::kInt); std::cout \u0026lt;\u0026lt; \u0026#34;a: \u0026#34; \u0026lt;\u0026lt; a \u0026lt;\u0026lt; std::endl; std::cout \u0026lt;\u0026lt; std::endl; std::cout \u0026lt;\u0026lt; \u0026#34;b: \u0026#34; \u0026lt;\u0026lt; b \u0026lt;\u0026lt; std::endl; std::cout \u0026lt;\u0026lt; std::endl; std::cout \u0026lt;\u0026lt; \u0026#34;c: \u0026#34; \u0026lt;\u0026lt; c \u0026lt;\u0026lt; std::endl; One of the reasons why Xeus-Cling is useful is, that you can print the outputs of intermediate steps and debug. Let\u0026rsquo;s go ahead and experiment with Autograd system of PyTorch C++ API.\nFor those who don\u0026rsquo;t know, automatic differentiation is the most important function of Deep Learning algorithms to backpropagte the loss we calculate.\n#include \u0026lt;torch/csrc/autograd/variable.h\u0026gt;#include \u0026lt;torch/csrc/autograd/function.h\u0026gt; torch::Tensor a_tensor = torch::ones({2, 2}, torch::requires_grad()); torch::Tensor b_tensor = torch::randn({2, 2}); std::cout \u0026lt;\u0026lt; a_tensor \u0026lt;\u0026lt; std::endl; std::cout \u0026lt;\u0026lt; b_tensor \u0026lt;\u0026lt; std::endl; auto c_tensor = a_tensor + b_tensor; c_tensor.backward(); // a.grad() will now hold the gradient of c w.r.t a  std::cout \u0026lt;\u0026lt; c_tensor \u0026lt;\u0026lt; std::endl; How about debugging? As you can see in the figure below, I get an error stating no member named 'size' in namespace 'cv'. This is because namespace cv has member called Size and not size.\ntorch::Tensor read_images(std::string location) { cv::Mat img = cv::imread(location, 1); cv::resize(img, img, cv::size(224, 224), cv::INTER_CUBIC); torch::Tensor img_tensor = torch::from_blob(img.data, {img.rows, img.cols, 3}, torch::kByte); img_tensor = img_tensor.permute({2, 0, 1}); return img_tensor.clone(); } To solve, we can simply change the member from size to Size. One important point to consider is, that since this works on the top of Jupyter Interface, so whenever you re-run a cell, the variable names need to be changed as it will return an error of re-defining the variables which have already been defined.\nFor testing, I have implemented Transfer Learning example that we discussed in the previous blog. This comes handy as I don\u0026rsquo;t need to load the dataset again and again.\nBonus! With this blog, I\u0026rsquo;m also happy to share a Notebook file with implementation of Transfer Learning using ResNet18 Model on Dogs vs Cats Dataset. Additionally, I\u0026rsquo;m elated to open source the code for Transfer Learning using ResNet18 Model using PyTorch C++ API.\nThe source code and the notebook file can be found here.\nDebugging - OSX Systems In case of OSX Systems, if you see any errors similar to: You are probably missing the definition of \u0026lt;function_name\u0026gt;, then try any (or all) of the following points:\n Use Xeus-Cling on a virtual environment as this might be because of conflicts with the existing libraries. Although, OSX Systems shouldn\u0026rsquo;t have C++ ABI Compatability Issues but you can still try this if problem persists.  Go to TorchCONFIG.cmake file (it should be present in \u0026lt;torch_folder\u0026gt;/share/cmake/Torch/). Change set(TORCH_CXX_FLAGS \u0026quot;-D_GLIBCXX_USE_CXX11_ABI=\u0026quot;) to set(TORCH_CXX_FLAGS \u0026quot;-D_GLIBCXX_USE_CXX11_ABI=1\u0026quot;) and reload the libraries and header files.    References  Xeus-Cling: Run C++ code in Jupyter Notebook by Vishwesh Ravi Shrimali. Documentation of Xeus-Cling.  ","permalink":"https://krshrimali.github.io/posts/2019/08/setting-up-jupyter-notebook-xeus-cling-for-libtorch-and-opencv-libraries/","tags":["development","coding","cpp","notes","pytorch-cpp","libtorch"],"title":"Setting up Jupyter Notebook (Xeus Cling) for Libtorch and OpenCV Libraries"},{"categories":["pytorch","deep learning"],"contents":"Transfer Learning \u0026ndash; Before we go ahead and discuss the Why question of Transfer Learning, let\u0026rsquo;s have a look at What is Transfer Learning? Let\u0026rsquo;s have a look at the Notes from CS231n on Transfer Learning:\n In practice, very few people train an entire Convolutional Network from scratch (with random initialization), because it is relatively rare to have a dataset of sufficient size. Instead, it is common to pretrain a ConvNet on a very large dataset (e.g. ImageNet, which contains 1.2 million images with 1000 categories), and then use the ConvNet either as an initialization or a fixed feature extractor for the task of interest.\n There are 3 scenarios possible:\n When the data you have is similar (but not enough) to data trained on pre-trained model: Take an example of a pre-trained model trained on ImageNet dataset (containing 1000 classes). And the data we have has Dogs and Cats classes. Fortunate enough, ImageNet has some of the classes of Dog and Cat breeds and thus the model must have learned important features from the data. Let\u0026rsquo;s say we don\u0026rsquo;t have enough data but since the data is similar to the breeds in the ImageNet data set, we can simply use the ConvNet (except the last FC layer) to extract features from our dataset and train only the last Linear (FC) layer. When you have enough data (and is similar to the data trained with pre-trained model): Then you might go for fine tuning the weights of all the layers in the network. This is largely due to the reason that we know we won\u0026rsquo;t overfit because we have enough data. Using pre-trained models might just be enough if you have the data which matches the classes in the original data set.  Transfer Learning came into existence (the answer of Why Transfer Learning?) because of some major reasons, which include:\n Lack of resources or data set to train a CNN. At times, we either don\u0026rsquo;t have enough data or we don\u0026rsquo;t have enough resources to train a CNN from scratch. Random Initialization of weights vs Initialization of weights from the pre-trained model. Sometimes, it\u0026rsquo;s just better to initialize weights from the pre-trained model (as it must have learned the generic features from it\u0026rsquo;s data set) instead of randomly initializing the weights.  Setting up the data with PyTorch C++ API At every stage, we will compare the Python and C++ codes to do the same thing, to make the analogy easier and understandable. Starting with setting up the data we have. Note that we do have enough data and it is also similar to the original data set of ImageNet, but since I don\u0026rsquo;t have enough resources to fine tune through the whole network, we perform Transfer Learning on the final FC layer only.\nStarting with loading the dataset, as discussed in the blogs before, I\u0026rsquo;ll just post a flow chart of procedure.\nOnce done, we can initialize the CustomDataset class:\nC++\n// List of images of Dogs and Cats, use load_data_from_folder function explained in previous blogs std::vector\u0026lt;std::string\u0026gt; list_images; // List of labels of the images std::vector\u0026lt;int\u0026gt; list_labels; auto custom_dataset = CustomDataset(list_images, list_labels).map(torch::data::transforms::Stack\u0026lt;\u0026gt;()); Python\nfrom torchvision import datasets, transforms import torch folder_path = \u0026#34;/Users/krshrimali/Documents/dataset/train/\u0026#34; transform = transforms.Compose([transforms.CenterCrop(224), transforms.ToTensor()) data = datasets.ImageFolder(root = os.path.join(folder_path), transform = transform) We then use RandomSampler to make our data loader: (Note: it\u0026rsquo;s important to use RandomSampler as we load the images sequentially and we want mixture of images in each batch of data passed to the network in an epoch)\nC++\nint batch_size = 4; auto data_loader = torch::data::make_data_loader\u0026lt;torch::data::samplers::RandomSampler\u0026gt;(std::move(custom_dataset), batch_size); Python\nbatch_size = 4 data_loader = torch.utils.data.DataLoader(dataset=data, batch_size = batch_size, shuffle = True) Loading the pre-trained model The steps to load the pre-trained model and perform Transfer Learning are listed below:\n Download the pre-trained model of ResNet18. Load pre-trained model. Change output features of the final FC layer of the model loaded. (Number of classes would change from 1000 - ImageNet to 2 - Dogs vs Cats). Define optimizer on parameters from the final FC layer to be trained. Train the FC layer on Dogs vs Cats dataset. Save the model.  Let\u0026rsquo;s go step by step.\nStep-1: Download the pre-trained model of ResNet18 Thanks to the developers, we do have C++ models available in torchvision (https://github.com/pytorch/vision/pull/728) but for this tutorial, transferring the pre- trained model from Python to C++ using torch.jit is a good idea, as most PyTorch models in the wild are written in Python right now, and people can use this tutorial to learn how to trace their Python model and transfer it to C++.\nFirst we download the pre-trained model and save it in the form of torch.jit.trace format to our local drive.\n# Reference: #TODO- Add Link from torchvision import models # Download and load the pre-trained model model = models.resnet18(pretrained=True) # Set upgrading the gradients to False for param in model.parameters(): param.requires_grad = False # Save the model except the final FC Layer resnet18 = torch.nn.Sequential(*list(resnet18.children())[:-1]) example_input = torch.rand(1, 3, 224, 224) script_module = torch.jit.trace(resnet18, example_input) script_module.save(\u0026#39;resnet18_without_last_layer.pt\u0026#39;) We will be using resnet18_without_last_layer.pt model file as our pre-trained model for transfer learning.\nStep-2: Load the pre-trained model Let\u0026rsquo;s go ahead and load the pre-trained model using torch::jit module. Note that the reason we have converted torch.nn.Module to torch.jit.ScriptModule type, is because C++ API currently does not support loading Python torch.nn.Module models directly.\nC++:\ntorch::jit::script::Module module; // argv[1] should be the path to the model module = torch::jit::load(argv[1]); /* We need to convert last layer input and output features from (512, 1000) to (512, 2) since we only have 2 classes */ torch::nn::Linear linear_layer(512, 2); // Define the optimizer on parameters of linear_layer with learning_rate = 1e-3 torch::optim::Adam optimizer(linear_layer-\u0026gt;parameters(), torch::optim::AdamOptions(1e-3)) Python:\n# We will directly load the torch.nn pre-trained model model = models.resnet18(pretrained = True) for param in model.parameters(): param.requires_grad = False model.fc = torch.nn.Linear(512, 2) for param in model.fc.parameters(): param.requires_grad = True optimizer = torch.optim.Adam(model.fc.parameters()) cost = torch.nn.CrossEntropyLoss() Trainining the FC Layer Let\u0026rsquo;s first have a look at ResNet18 Network Architecture\nThe final step is to train the Fully Connected layer that we inserted at the end of the network (linear_layer). This one should be pretty straight forward, let\u0026rsquo;s see how to do it.\nC++:\nvoid train(torch::jit::script::Module net, torch::nn::Linear lin, Dataloader\u0026amp; data_loader, torch::optim::Optimizer\u0026amp; optimizer, size_t dataset_size) { /* This function trains the network on our data loader using optimizer for given number of epochs. Parameters ================== torch::jit::script::Module net: Pre-trained model torch::nn::Linear lin: Linear layer DataLoader\u0026amp; data_loader: Training data loader torch::optim::Optimizer\u0026amp; optimizer: Optimizer like Adam, SGD etc. size_t dataset_size: Size of training dataset */ float batch_index = 0; for(int i=0; i\u0026lt;15; i++) { float mse = 0; float Acc = 0.0; for(auto\u0026amp; batch: *data_loader) { auto data = batch.data; auto target = batch.target.squeeze(); // Should be of length: batch_size  data = data.to(torch::kF32); target = target.to(torch::kInt64); std::vector\u0026lt;torch::jit::IValue\u0026gt; input; input.push_back(data); optimizer.zero_grad(); auto output = net.forward(input).toTensor(); // For transfer learning  output = output.view({output.size(0), -1}); output = lin(output); // Explicitly calculate torch::log_softmax of output from the FC Layer  auto loss = torch::nll_loss(torch::log_softmax(output, 1), target); loss.backward(); optimizer.step(); auto acc = output.argmax(1).eq(target).sum(); Acc += acc.template item\u0026lt;float\u0026gt;(); mse += loss.template item\u0026lt;float\u0026gt;(); batch_index += 1; } mse = mse/float(batch_index); // Take mean of loss  std::cout \u0026lt;\u0026lt; \u0026#34;Epoch: \u0026#34; \u0026lt;\u0026lt; i \u0026lt;\u0026lt; \u0026#34;, \u0026#34; \u0026lt;\u0026lt; \u0026#34;Accuracy: \u0026#34; \u0026lt;\u0026lt; Acc/dataset_size \u0026lt;\u0026lt; \u0026#34;, \u0026#34; \u0026lt;\u0026lt; \u0026#34;MSE: \u0026#34; \u0026lt;\u0026lt; mse \u0026lt;\u0026lt; std::endl; net.save(\u0026#34;model.pt\u0026#34;); } } Python:\nn_epochs = 15 for epoch in range(n_epochs): mse = 0.0 acc = 0 batch_index = 0 for data_batch in data_loader: batch_index += 1 image, label = data_batch optimizer.zero_grad() output = model(image) _, predicted_label = torch.max(output.data, 1) loss = cost(output, label) loss.backward() optimizer.step() mse += loss.item() # data[0] acc += torch.sum(predicted_label == label.data) mse = mse/len(data) acc = 100*acc/len(data) print(\u0026#34;Epoch: {}/{}, Loss: {:.4f}, Accuracy: {:.4f}\u0026#34;.format(epoch+1, n_epochs, mse, acc)) The code to test should also not change much except the need of optimizer.\nResults On a set of 400 images for training data, the maximum training Accuracy I could achieve was 91.25% in just less than 15 epochs using PyTorch C++ API and 89.0% using Python. (Note that this doesn\u0026rsquo;t conclude superiority in terms of accuracy between any of the two backends - C++ or Python)\nLet\u0026rsquo;s have a look at correct and wrong predictions.\nCorrect Predictions - Dogs Wrong Predictions - Dogs Correct Predictions - Cats Wrong Predictions - Cats Acknowledgements I would like to thank a few people to help me bring this out to the community. Thanks to Piotr for his comments and answers in the PyTorch Discussion forum. Thanks to Will Feng for reviewing the blog and the code and also his constant motivation to bring this out to you all. Would like to thank my constant motivation behind all my work, Vishwesh Ravi Shrimali for all his help to start with PyTorch C++ API and help the community. Special thanks to Krutika Bapat as well, for reviewing the Python equivalent code and suggesting modifications.\nAnd shout out to all the readers, please share your feedback with me in the comments below. I would love to hear if this blog helped you!\nIn the upcoming blog, I\u0026rsquo;ll be sharing something very exciting. Till then, happy learning!\n","permalink":"https://krshrimali.github.io/posts/2019/08/applying-transfer-learning-on-dogs-vs-cats-dataset-resnet18-using-pytorch-c-api/","tags":["development","coding","cpp","notes","pytorch-cpp","libtorch"],"title":"Applying Transfer Learning on Dogs vs Cats Dataset (ResNet18) using PyTorch C++ API"},{"categories":["pytorch","deep learning"],"contents":"In the last blog, we had discussed all but training and results of our custom CNN network on Dogs vs Cats dataset. Today, we\u0026rsquo;ll be making some small changes in the network and discussing training and results of the task.\nI\u0026rsquo;ll start with the network overview again, where we used a network similar to VGG-16 (with one extra Fully Connected Layer in the end). While there are absolutely no problems with that network, but since the dataset contains a lot of images (25000 in training dataset) and we were using (200x200x3) input shape to the network (which is 120,000 floating point numbers), this leads to high memory consumption. In short, I was out of RAM to store these many images during program execution.\nSo, I decided to change some minute things:\n Input Shape to the network is now 64x64x3 (12,288 parameters - around 10 times lesser than for 200x200x3). So, all the images are now resized to 64x64x3. Now, we only use 2 Convolutional Layers and 2 Max Pooling Layers to train our dataset. This helps to reduce the parameters for training, and also fastens the training process.  But this comes with a tradeoff in accuracy, which will suffice for now as our target is not to get some X accuracy, but to learn how to train the network on our dataset using PyTorch C++ API.\nQuestion: Does reducing input resolution, affects accuracy? Answer: In this case, it will. The objects in our dataset (dogs and cats) have both high level and low level features, which are visible (provides more details) more to the network with high resolution. In this way, the network is allowed to learn more features out of the dataset. However, in cases like of MNIST, it\u0026rsquo;s just fine to use 64x64 input resolution as it still allows the network to look at details of a digit and learn robust features.\nLet\u0026rsquo;s go ahead and see what has changed in the Network.\nNetwork Overview If you don\u0026rsquo;t remember from the last time, this is how our network looked.\nstruct NetImpl: public torch::nn::Module { NetImpl() { // Initialize the network  // On how to pass strides and padding: https://github.com/pytorch/pytorch/issues/12649#issuecomment-430156160  conv1_1 = register_module(\u0026#34;conv1_1\u0026#34;, torch::nn::Conv2d(torch::nn::Conv2dOptions(1, 10, 3).padding(1))); conv1_2 = register_module(\u0026#34;conv1_2\u0026#34;, torch::nn::Conv2d(torch::nn::Conv2dOptions(10, 20, 3).padding(1))); // Insert pool layer  conv2_1 = register_module(\u0026#34;conv2_1\u0026#34;, torch::nn::Conv2d(torch::nn::Conv2dOptions(20, 30, 3).padding(1))); conv2_2 = register_module(\u0026#34;conv2_2\u0026#34;, torch::nn::Conv2d(torch::nn::Conv2dOptions(30, 40, 3).padding(1))); // Insert pool layer  conv3_1 = register_module(\u0026#34;conv3_1\u0026#34;, torch::nn::Conv2d(torch::nn::Conv2dOptions(40, 50, 3).padding(1))); conv3_2 = register_module(\u0026#34;conv3_2\u0026#34;, torch::nn::Conv2d(torch::nn::Conv2dOptions(50, 60, 3).padding(1))); conv3_3 = register_module(\u0026#34;conv3_3\u0026#34;, torch::nn::Conv2d(torch::nn::Conv2dOptions(60, 70, 3).padding(1))); // Insert pool layer  conv4_1 = register_module(\u0026#34;conv4_1\u0026#34;, torch::nn::Conv2d(torch::nn::Conv2dOptions(70, 80, 3).padding(1))); conv4_2 = register_module(\u0026#34;conv4_2\u0026#34;, torch::nn::Conv2d(torch::nn::Conv2dOptions(80, 90, 3).padding(1))); conv4_3 = register_module(\u0026#34;conv4_3\u0026#34;, torch::nn::Conv2d(torch::nn::Conv2dOptions(90, 100, 3).padding(1))); // Insert pool layer  conv5_1 = register_module(\u0026#34;conv5_1\u0026#34;, torch::nn::Conv2d(torch::nn::Conv2dOptions(100, 110, 3).padding(1))); conv5_2 = register_module(\u0026#34;conv5_2\u0026#34;, torch::nn::Conv2d(torch::nn::Conv2dOptions(110, 120, 3).padding(1))); conv5_3 = register_module(\u0026#34;conv5_3\u0026#34;, torch::nn::Conv2d(torch::nn::Conv2dOptions(120, 130, 3).padding(1))); // Insert pool layer  fc1 = register_module(\u0026#34;fc1\u0026#34;, torch::nn::Linear(130*6*6, 2000)); fc2 = register_module(\u0026#34;fc2\u0026#34;, torch::nn::Linear(2000, 1000)); fc3 = register_module(\u0026#34;fc3\u0026#34;, torch::nn::Linear(1000, 100)); fc4 = register_module(\u0026#34;fc4\u0026#34;, torch::nn::Linear(100, 2)); } // Implement Algorithm  torch::Tensor forward(torch::Tensor x) { x = torch::relu(conv1_1-\u0026gt;forward(x)); x = torch::relu(conv1_2-\u0026gt;forward(x)); x = torch::max_pool2d(x, 2); x = torch::relu(conv2_1-\u0026gt;forward(x)); x = torch::relu(conv2_2-\u0026gt;forward(x)); x = torch::max_pool2d(x, 2); x = torch::relu(conv3_1-\u0026gt;forward(x)); x = torch::relu(conv3_2-\u0026gt;forward(x)); x = torch::relu(conv3_3-\u0026gt;forward(x)); x = torch::max_pool2d(x, 2); x = torch::relu(conv4_1-\u0026gt;forward(x)); x = torch::relu(conv4_2-\u0026gt;forward(x)); x = torch::relu(conv4_3-\u0026gt;forward(x)); x = torch::max_pool2d(x, 2); x = torch::relu(conv5_1-\u0026gt;forward(x)); x = torch::relu(conv5_2-\u0026gt;forward(x)); x = torch::relu(conv5_3-\u0026gt;forward(x)); x = torch::max_pool2d(x, 2); x = x.view({-1, 130*6*6}); x = torch::relu(fc1-\u0026gt;forward(x)); x = torch::relu(fc2-\u0026gt;forward(x)); x = torch::relu(fc3-\u0026gt;forward(x)); x = fc4-\u0026gt;forward(x); return torch::log_softmax(x, 1); } // Declare layers  torch::nn::Conv2d conv1_1{nullptr}; torch::nn::Conv2d conv1_2{nullptr}; torch::nn::Conv2d conv2_1{nullptr}; torch::nn::Conv2d conv2_2{nullptr}; torch::nn::Conv2d conv3_1{nullptr}; torch::nn::Conv2d conv3_2{nullptr}; torch::nn::Conv2d conv3_3{nullptr}; torch::nn::Conv2d conv4_1{nullptr}; torch::nn::Conv2d conv4_2{nullptr}; torch::nn::Conv2d conv4_3{nullptr}; torch::nn::Conv2d conv5_1{nullptr}; torch::nn::Conv2d conv5_2{nullptr}; torch::nn::Conv2d conv5_3{nullptr}; torch::nn::Linear fc1{nullptr}, fc2{nullptr}, fc3{nullptr}, fc4{nullptr}; }; As it\u0026rsquo;s visible, we had 13 Convolutional Layers, 5 Max Pooling Layers and 4 Fully Connected Layers. This leads of a lot of parameters to be trained.\nTherefore, our new network for experimentation purposes will be:\nstruct NetworkImpl : public torch::nn::Module { NetImpl(int64_t channels, int64_t height, int64_t width) { conv1_1 = register_module(\u0026#34;conv1\u0026#34;, torch::nn::Conv2d(torch::nn::Conv2dOptions(3, 50, 5).stride(2))); conv2_1 = register_module(\u0026#34;conv2\u0026#34;, torch::nn::Conv2d(torch::nn::Conv2dOptions(50, 100, 7).stride(2))); // Used to find the output size till previous convolutional layers \tn(get_output_shape(channels, height, width)); fc1 = register_module(\u0026#34;fc1\u0026#34;, torch::nn::Linear(n, 120)); fc2 = register_module(\u0026#34;fc2\u0026#34;, torch::nn::Linear(120, 100)); fc3 = register_module(\u0026#34;fc3\u0026#34;, torch::nn::Linear(100, 2)); register_module(\u0026#34;conv1\u0026#34;, conv1); register_module(\u0026#34;conv2\u0026#34;, conv2); register_module(\u0026#34;fc1\u0026#34;, fc1); register_module(\u0026#34;fc2\u0026#34;, fc2); register_module(\u0026#34;fc3\u0026#34;, fc3); } // Implement forward pass of each batch to the network  torch::Tensor forward(torch::Tensor x) { x = torch::relu(torch::max_pool2d(conv1(x), 2)); x = torch::relu(torch::max_pool2d(conv2(x), 2)); // Flatten  x = x.view({-1, n}); x = torch::relu(fc1(x)); x = torch::relu(fc2(x)); x = torch::log_softmax(fc3(x), 1); return x; }; // Function to calculate output size of input tensor after Convolutional layers  int64_t get_output_shape(int64_t channels, int64_t height, int64_t width) { // Initialize a Tensor with zeros of input shape  torch::Tensor x_sample = torch::zeros({1, channels, height, width}); x_sample = torch::max_pool2d(conv1(x_sample), 2); x_sample = torch::max_pool2d(conv2(x_sample), 2); // Return batch_size (here, 1) * channels * height * width of x_sample  return x_sample.numel(); } }; In our new network, we use 2 Convolutional Layers with Max Pooling and ReLU Activation, and 3 Fully Connected Layers. This, as we mentioned above, reduces the number of parameters for training.\nLet us train our network on the dataset now.\nTraining Let\u0026rsquo;s discuss steps of training a CNN on our dataset:\n Set network to training mode using net-\u0026gt;train(). Iterate through every batch of our data loader:  Extract data and labels using: auto data = batch.data; auto target = batch.target.squeeze(); ``\n Clear gradients of optimizer: optimizer.zero_grad() Forward pass the batch of data to the network: auto output = net-\u0026gt;forward(data); Calculate Negative Log Likelihood loss (since we use log_softmax() layer at the end): auto loss = torch::nll_loss(output, target); Backpropagate Loss: auto loss = loss.backward() Update the weights: optimizer.step(); Calculate Training Accuracy and Mean Squared Error: auto acc = output.argmax(1).eq(target).sum(); mse += loss.template item\u0026lt;float\u0026gt;(); ``\n   Save the model using torch::save(net, \u0026quot;model.pt\u0026quot;);.  Let\u0026rsquo;s try to convert the above steps to a train() function.\nvoid train(ConvNet\u0026amp; net, DataLoader\u0026amp; data_loader, torch::optim::Optimizer\u0026amp; optimizer, size_t dataset_size, int epoch) { /* This function trains the network on our data loader using optimizer for given number of epochs. Parameters ================== ConvNet\u0026amp; net: Network struct DataLoader\u0026amp; data_loader: Training data loader torch::optim::Optimizer\u0026amp; optimizer: Optimizer like Adam, SGD etc. size_t dataset_size: Size of training dataset int epoch: Number of epoch for training */ net-\u0026gt;train(); size_t batch_index = 0; float mse = 0; float Acc = 0.0; for(auto\u0026amp; batch: *data_loader) { auto data = batch.data; auto target = batch.target.squeeze(); // Should be of length: batch_size  data = data.to(torch::kF32); target = target.to(torch::kInt64); optimizer.zero_grad(); auto output = net-\u0026gt;forward(data); auto loss = torch::nll_loss(output, target); loss.backward(); optimizer.step(); auto acc = output.argmax(1).eq(target).sum(); Acc += acc.template item\u0026lt;float\u0026gt;(); mse += loss.template item\u0026lt;float\u0026gt;(); batch_index += 1; count++; } mse = mse/float(batch_index); // Take mean of loss  std::cout \u0026lt;\u0026lt; \u0026#34;Epoch: \u0026#34; \u0026lt;\u0026lt; epoch \u0026lt;\u0026lt; \u0026#34;, \u0026#34; \u0026lt;\u0026lt; \u0026#34;Accuracy: \u0026#34; \u0026lt;\u0026lt; Acc/dataset_size \u0026lt;\u0026lt; \u0026#34;, \u0026#34; \u0026lt;\u0026lt; \u0026#34;MSE: \u0026#34; \u0026lt;\u0026lt; mse \u0026lt;\u0026lt; std::endl; torch::save(net, \u0026#34;best_model_try.pt\u0026#34;); } Similarly, we also define a Test Function to test our network on the test dataset. The steps include:\n Set network to eval mode: network-\u0026gt;eval(). Iterate through every batch of test data.  Extract data and labels. Forward pass the batch of data to the network. Calculate NLL Loss and Accuracy   Print test loss and accuracy.  The code for the test() function is below:\nvoid test(ConvNet\u0026amp; network, DataLoader\u0026amp; loader, size_t data_size) { size_t batch_index = 0; network-\u0026gt;eval(); float Loss = 0, Acc = 0; for (const auto\u0026amp; batch : *loader) { auto data = batch.data; auto targets = batch.target.view({-1}); data = data.to(torch::kF32); targets = targets.to(torch::kInt64); auto output = network-\u0026gt;forward(data); auto loss = torch::nll_loss(output, targets); auto acc = output.argmax(1).eq(targets).sum(); Loss += loss.template item\u0026lt;float\u0026gt;(); Acc += acc.template item\u0026lt;float\u0026gt;(); } cout \u0026lt;\u0026lt; \u0026#34;Test Loss: \u0026#34; \u0026lt;\u0026lt; Loss/data_size \u0026lt;\u0026lt; \u0026#34;, Acc:\u0026#34; \u0026lt;\u0026lt; Acc/data_size \u0026lt;\u0026lt; endl; } Results I trained my network on the dataset for 100 Epochs.\nThe best training and testing accuracies are given below:\n Best Training Accuracy: 99.82% Best Testing Accuracy: 82.43%  Let\u0026rsquo;s look at some of the correct and wrong predictions.\nCorrect Predictions (Dogs) Correct Predictions (Cats) Wrong Predictions (Dogs) Wrong Predictions (Cats) Clearly, the network has done well for just a 2 Convolutional and 3 FC Layer Network. It seems to have focused more on the posture of the animal (and body). We can make the network learn more robust features, with a more deeper CNN (like VGG-16). We\u0026rsquo;ll be discussing on using pretrained weights on Dogs vs Cats Dataset using PyTorch C++ API and also Transfer Learning Approach in C++.\nHappy Learning!\n","permalink":"https://krshrimali.github.io/posts/2019/07/classifying-dogs-vs-cats-using-pytorch-c-part-2/","tags":["development","coding","cpp","notes","pytorch-cpp","libtorch"],"title":"Classifying Dogs vs Cats using PyTorch C++: Part 2"},{"categories":["pytorch","deep learning"],"contents":"Hi Everyone! So excited to be back with another blog in the series of PyTorch C++ Blogs.\nToday, we are going to see a practical example of applying a CNN to a Custom Dataset - Dogs vs Cats. This is going to be a short post of showing results and discussion about hyperparameters and loss functions for the task, as code snippets and explanation has been provided here, here and here.\nNote: This is Part-1 of the blog on Dogs vs Cats Classification using PyTorch C++ API.\nDataset Overview Let\u0026rsquo;s have a look at the dataset and it\u0026rsquo;s statistics. Dogs vs Cats dataset has been taken from the famous Kaggle Competition.\nThe training set contains 25k images combined of dogs and cats. The data can be downloaded from this link.\nLet\u0026rsquo;s have a look at sample of the data:\nAs we can see, the dataset contains images of cats and dogs with multiple instances in the same sample as well.\nLoading Data Although we have discussed this before (here), but let\u0026rsquo;s just see how we load the data. Since this is a binary classification problem (2 classes: Dog and Cat), we will have labels as 0 (for a Cat) and 1 (for a Dog). The data comes in two zip files:\n train.zip: Data to be used for training test.zip: Data to be used for testing  The train.zip file contains files with filenames like \u0026lt;class\u0026gt;.\u0026lt;number\u0026gt;.jpg where:\n class can be either cat or dog. number represents the count of the sample.  For example: cat.100.jpg and dog.1.jpg. In order to load the data, we will move the cat images to train/cat folder and dog images to train/dog folder. You can accomplish this using shutil module:\n# This code moves cat and dog images to train/cat and train/dog folders respectively import shutil, os files = os.listdir(\u0026#39;train/\u0026#39;) count_cat = 0 # Number representing count of the cat image count_dog = 0 # Number representing count of the dog image for file in files: if(file.startswith(\u0026#39;cat\u0026#39;) and file.endswith(\u0026#39;jpg\u0026#39;)): count_cat += 1 shutil.copy(\u0026#39;train/\u0026#39; + file, \u0026#39;train/cat/\u0026#39; + str(count_cat) + \u0026#34;.jpg\u0026#34;) elif(file.startswith(\u0026#39;dog\u0026#39;) and file.endswith(\u0026#39;jpg\u0026#39;)): count_dog += 1 shutil.copy(\u0026#39;test/\u0026#39; + file, \u0026#39;train/dog/\u0026#39; + str(count_dog) + \u0026#39;.jpg\u0026#39;) Once done, let\u0026rsquo;s go ahead and load this data. Since we have discussed this before, I\u0026rsquo;ll just paste the snippet here.\ntorch::Tensor read_data(std::string loc) { // Read Image from the location of image \tcv::Mat img = cv::imread(loc, 0); cv::resize(img, img, cv::Size(200, 200), cv::INTER_CUBIC); std::cout \u0026lt;\u0026lt; \u0026#34;Sizes: \u0026#34; \u0026lt;\u0026lt; img.size() \u0026lt;\u0026lt; std::endl; torch::Tensor img_tensor = torch::from_blob(img.data, {img.rows, img.cols, 1}, torch::kByte); img_tensor = img_tensor.permute({2, 0, 1}); // Channels x Height x Width  return img_tensor.clone(); }; torch::Tensor read_label(int label) { torch::Tensor label_tensor = torch::full({1}, label); return label_tensor.clone(); } vector\u0026lt;torch::Tensor\u0026gt; process_images(vector\u0026lt;string\u0026gt; list_images) { cout \u0026lt;\u0026lt; \u0026#34;Reading images...\u0026#34; \u0026lt;\u0026lt; endl; vector\u0026lt;torch::Tensor\u0026gt; states; for (std::vector\u0026lt;string\u0026gt;::iterator it = list_images.begin(); it != list_images.end(); ++it) { cout \u0026lt;\u0026lt; \u0026#34;Location being read: \u0026#34; \u0026lt;\u0026lt; *it \u0026lt;\u0026lt; endl; torch::Tensor img = read_data(*it); states.push_back(img); } cout \u0026lt;\u0026lt; \u0026#34;Reading and Processing images done!\u0026#34; \u0026lt;\u0026lt; endl; return states; } vector\u0026lt;torch::Tensor\u0026gt; process_labels(vector\u0026lt;int\u0026gt; list_labels) { cout \u0026lt;\u0026lt; \u0026#34;Reading labels...\u0026#34; \u0026lt;\u0026lt; endl; vector\u0026lt;torch::Tensor\u0026gt; labels; for (std::vector\u0026lt;int\u0026gt;::iterator it = list_labels.begin(); it != list_labels.end(); ++it) { torch::Tensor label = read_label(*it); labels.push_back(label); } cout \u0026lt;\u0026lt; \u0026#34;Labels reading done!\u0026#34; \u0026lt;\u0026lt; endl; return labels; } /* This function returns a pair of vector of images paths (strings) and labels (integers) */ std::pair\u0026lt;vector\u0026lt;string\u0026gt;,vector\u0026lt;int\u0026gt;\u0026gt; load_data_from_folder(vector\u0026lt;string\u0026gt; folders_name) { vector\u0026lt;string\u0026gt; list_images; vector\u0026lt;int\u0026gt; list_labels; int label = 0; for(auto const\u0026amp; value: folders_name) { string base_name = value + \u0026#34;/\u0026#34;; cout \u0026lt;\u0026lt; \u0026#34;Reading from: \u0026#34; \u0026lt;\u0026lt; base_name \u0026lt;\u0026lt; endl; DIR* dir; struct dirent *ent; if((dir = opendir(base_name.c_str())) != NULL) { while((ent = readdir(dir)) != NULL) { string filename = ent-\u0026gt;d_name; if(filename.length() \u0026gt; 4 \u0026amp;\u0026amp; filename.substr(filename.length() - 3) == \u0026#34;jpg\u0026#34;) { cout \u0026lt;\u0026lt; base_name + ent-\u0026gt;d_name \u0026lt;\u0026lt; endl; // cv::Mat temp = cv::imread(base_name + \u0026#34;/\u0026#34; + ent-\u0026gt;d_name, 1); \tlist_images.push_back(base_name + ent-\u0026gt;d_name); list_labels.push_back(label); } } closedir(dir); } else { cout \u0026lt;\u0026lt; \u0026#34;Could not open directory\u0026#34; \u0026lt;\u0026lt; endl; // return EXIT_FAILURE; \t} label += 1; } return std::make_pair(list_images, list_labels); } The above snippet has the utility functions we need. Here is a quick summary of what they do:\n  load_data_from_folder: This function returns a tuple of list of image paths (string) and list of labels (int). It takes a vector of folders names (string type) as parameter.\n  process_images: This function returns a vector of Tensors (images). This function calls read_data function which reads, resizes and converts an image to a Torch Tensor. It takes a vector of image paths (string) as parameter.\n  process_labels: Similar to process_images function, this function returns a vector of Tensors (labels). This function calls read_label function which takes an int as a parameter (label: 0 or 1) and returns a Tensor.\n  Let\u0026rsquo;s now go ahead and see how we load the data. For this, we first need to define the Dataset class. This class should initialize two variables: one for images and one for labels. As discussed here, we\u0026rsquo;ll also define get() and size() functions.\nclass CustomDataset : public torch::data::Dataset\u0026lt;CustomDataset\u0026gt; { private: /* data */ // Should be 2 tensors \tvector\u0026lt;torch::Tensor\u0026gt; states, labels; public: CustomDataset(vector\u0026lt;string\u0026gt; list_images, vector\u0026lt;int\u0026gt; list_labels) { states = process_images(list_images); labels = process_labels(list_labels); }; torch::data::Example\u0026lt;\u0026gt; get(size_t index) override { /* This should return {torch::Tensor, torch::Tensor} */ torch::Tensor sample_img = states.at(index); torch::Tensor sample_label = labels.at(index); return {sample_img.clone(), sample_label.clone()}; }; torch::optional\u0026lt;size_t\u0026gt; size() const override { return states.size(); }; }; Once done, we can go ahead and initialize the Dataset in our main() function.\nint main(int argc, char const *argv[]) { // Load the model.  // Read Data  vector\u0026lt;string\u0026gt; folders_name; folders_name.push_back(\u0026#34;/home/krshrimali/Documents/data-dogs-cats/train/cat\u0026#34;); folders_name.push_back(\u0026#34;/home/krshrimali/Documents/data-dogs-cats/train/dog\u0026#34;); std::pair\u0026lt;vector\u0026lt;string\u0026gt;, vector\u0026lt;int\u0026gt;\u0026gt; pair_images_labels = load_data_from_folder(folders_name); vector\u0026lt;string\u0026gt; list_images = pair_images_labels.first; vector\u0026lt;int\u0026gt; list_labels = pair_images_labels.second; auto custom_dataset = CustomDataset(list_images, list_labels).map(torch::data::transforms::Stack\u0026lt;\u0026gt;()); } Network Overview To make things easy to read (a programmer\u0026rsquo;s mantra), we define our network in a header file. We will use a CNN network initially for this binary classification task. Since I\u0026rsquo;m not using a GPU, the training is slow and that\u0026rsquo;s why I experimented it only for 10 epochs. The whole objective is to discuss and show how to use PyTorch C++ API for this. You can always run it for more epochs or change the network parameters.\nstruct NetImpl: public torch::nn::Module { NetImpl() { // Initialize the network  // On how to pass strides and padding: https://github.com/pytorch/pytorch/issues/12649#issuecomment-430156160  conv1_1 = register_module(\u0026#34;conv1_1\u0026#34;, torch::nn::Conv2d(torch::nn::Conv2dOptions(1, 10, 3).padding(1))); conv1_2 = register_module(\u0026#34;conv1_2\u0026#34;, torch::nn::Conv2d(torch::nn::Conv2dOptions(10, 20, 3).padding(1))); // Insert pool layer  conv2_1 = register_module(\u0026#34;conv2_1\u0026#34;, torch::nn::Conv2d(torch::nn::Conv2dOptions(20, 30, 3).padding(1))); conv2_2 = register_module(\u0026#34;conv2_2\u0026#34;, torch::nn::Conv2d(torch::nn::Conv2dOptions(30, 40, 3).padding(1))); // Insert pool layer  conv3_1 = register_module(\u0026#34;conv3_1\u0026#34;, torch::nn::Conv2d(torch::nn::Conv2dOptions(40, 50, 3).padding(1))); conv3_2 = register_module(\u0026#34;conv3_2\u0026#34;, torch::nn::Conv2d(torch::nn::Conv2dOptions(50, 60, 3).padding(1))); conv3_3 = register_module(\u0026#34;conv3_3\u0026#34;, torch::nn::Conv2d(torch::nn::Conv2dOptions(60, 70, 3).padding(1))); // Insert pool layer  conv4_1 = register_module(\u0026#34;conv4_1\u0026#34;, torch::nn::Conv2d(torch::nn::Conv2dOptions(70, 80, 3).padding(1))); conv4_2 = register_module(\u0026#34;conv4_2\u0026#34;, torch::nn::Conv2d(torch::nn::Conv2dOptions(80, 90, 3).padding(1))); conv4_3 = register_module(\u0026#34;conv4_3\u0026#34;, torch::nn::Conv2d(torch::nn::Conv2dOptions(90, 100, 3).padding(1))); // Insert pool layer  conv5_1 = register_module(\u0026#34;conv5_1\u0026#34;, torch::nn::Conv2d(torch::nn::Conv2dOptions(100, 110, 3).padding(1))); conv5_2 = register_module(\u0026#34;conv5_2\u0026#34;, torch::nn::Conv2d(torch::nn::Conv2dOptions(110, 120, 3).padding(1))); conv5_3 = register_module(\u0026#34;conv5_3\u0026#34;, torch::nn::Conv2d(torch::nn::Conv2dOptions(120, 130, 3).padding(1))); // Insert pool layer  fc1 = register_module(\u0026#34;fc1\u0026#34;, torch::nn::Linear(130*6*6, 2000)); fc2 = register_module(\u0026#34;fc2\u0026#34;, torch::nn::Linear(2000, 1000)); fc3 = register_module(\u0026#34;fc3\u0026#34;, torch::nn::Linear(1000, 100)); fc4 = register_module(\u0026#34;fc4\u0026#34;, torch::nn::Linear(100, 2)); } // Implement Algorithm  torch::Tensor forward(torch::Tensor x) { x = torch::relu(conv1_1-\u0026gt;forward(x)); x = torch::relu(conv1_2-\u0026gt;forward(x)); x = torch::max_pool2d(x, 2); x = torch::relu(conv2_1-\u0026gt;forward(x)); x = torch::relu(conv2_2-\u0026gt;forward(x)); x = torch::max_pool2d(x, 2); x = torch::relu(conv3_1-\u0026gt;forward(x)); x = torch::relu(conv3_2-\u0026gt;forward(x)); x = torch::relu(conv3_3-\u0026gt;forward(x)); x = torch::max_pool2d(x, 2); x = torch::relu(conv4_1-\u0026gt;forward(x)); x = torch::relu(conv4_2-\u0026gt;forward(x)); x = torch::relu(conv4_3-\u0026gt;forward(x)); x = torch::max_pool2d(x, 2); x = torch::relu(conv5_1-\u0026gt;forward(x)); x = torch::relu(conv5_2-\u0026gt;forward(x)); x = torch::relu(conv5_3-\u0026gt;forward(x)); x = torch::max_pool2d(x, 2); x = x.view({-1, 130*6*6}); x = torch::relu(fc1-\u0026gt;forward(x)); x = torch::relu(fc2-\u0026gt;forward(x)); x = torch::relu(fc3-\u0026gt;forward(x)); x = fc4-\u0026gt;forward(x); return torch::log_softmax(x, 1); } // Declare layers  torch::nn::Conv2d conv1_1{nullptr}; torch::nn::Conv2d conv1_2{nullptr}; torch::nn::Conv2d conv2_1{nullptr}; torch::nn::Conv2d conv2_2{nullptr}; torch::nn::Conv2d conv3_1{nullptr}; torch::nn::Conv2d conv3_2{nullptr}; torch::nn::Conv2d conv3_3{nullptr}; torch::nn::Conv2d conv4_1{nullptr}; torch::nn::Conv2d conv4_2{nullptr}; torch::nn::Conv2d conv4_3{nullptr}; torch::nn::Conv2d conv5_1{nullptr}; torch::nn::Conv2d conv5_2{nullptr}; torch::nn::Conv2d conv5_3{nullptr}; torch::nn::Linear fc1{nullptr}, fc2{nullptr}, fc3{nullptr}, fc4{nullptr}; }; We will initialize this network and pass each batch of our data once in an epoch.\n// Initialize the Network auto net = std::make_shared\u0026lt;NetImpl\u0026gt;(); Training the Network on Dogs vs Cats Dataset We had before discussed code for training here. I suggest the reader to go through that blog in order to train the dataset. I\u0026rsquo;ll give more insights on training in the next blog!\nThat\u0026rsquo;s it for today. I\u0026rsquo;ll be back with Part-2 of this \u0026ldquo;Dogs vs Cats Classification\u0026rdquo; with training, experimentation and results. We\u0026rsquo;ll also discuss on using different networks, and in the Part-3, we\u0026rsquo;ll discuss using Transfer Learning for this classification task.\n","permalink":"https://krshrimali.github.io/posts/2019/07/classifying-dogs-vs-cats-using-pytorch-c-api-part-1/","tags":["development","coding","cpp","notes","pytorch-cpp","libtorch"],"title":"Classifying Dogs vs Cats using PyTorch C++ API: Part-1"},{"categories":["pytorch","deep learning"],"contents":"Recap of the last blog Before we move on, it\u0026rsquo;s important what we covered in the last blog. We\u0026rsquo;ll be going forward from loading Custom Dataset to now using the dataset to train our VGG-16 Network. Previously, we were able to load our custom dataset using the following template:\nNote: Those who are already aware of loading a custom dataset can skip this section.\nclass CustomDataset : public torch::data::dataset\u0026lt;CustomDataset\u0026gt; { private: // Declare 2 vectors of tensors for images and labels  vector\u0026lt;torch::Tensor\u0026gt; images, labels; public: // Constructor  CustomDataset(vector\u0026lt;string\u0026gt; list_images, vector\u0026lt;string\u0026gt; list_labels) { images = process_images(list_images); labels = process_labels(list_labels); }; // Override get() function to return tensor at location index  torch::data::Example\u0026lt;\u0026gt; get(size_t index) override { torch::Tensor sample_img = images.at(index); torch::Tensor sample_label = labels.at(index); return {sample_img.clone(), sample_label.clone()}; }; // Return the length of data  torch::optional\u0026lt;size_t\u0026gt; size() const override { return labels.size(); }; }; int main(int argc, char** argv) { vector\u0026lt;string\u0026gt; list_images; // list of path of images  vector\u0026lt;int\u0026gt; list_labels; // list of integer labels  // Dataset init and apply transforms - None!  auto custom_dataset = CustomDataset(list_images, list_labels).map(torch::data::transforms::Stack\u0026lt;\u0026gt;()); } These were the steps we followed last time:\nOverview: How to pass batches to our network? Since we have our dataset loaded, let\u0026rsquo;s see how to pass batches of data to our network. Before we go on and see how PyTorch C++ API does it, let\u0026rsquo;s see how it\u0026rsquo;s done in Python.\ndataset_loader = torch.utils.data.DataLoader(custom_dataset, batch_size=4, shuffle=True) Just a short review of what DataLoader() class does: It loads the data and returns single or multiple iterators over the dataset. We pass in our object from Dataset class (here, custom_dataset). We will do the same process in C++ using the following template:\nauto data_loader = torch::data::make_data_loader\u0026lt;torch::data::samplers::SequentialSampler\u0026gt;( std::move(custom_dataset), batch_size ); In brief, we are loading our data using SequentialSampler class which samples our data in the same order that we provided it with. Have a look at the SequentialSampler class here.\nFor the definition of this function: torch::data::make_data_loader here. A short screenshot from the documentation is given below.\nLet\u0026rsquo;s go ahead and learn to iterate through our data loader and pass each batch of data and labels to our network. For once, imagine that we have a struct named Net which defines our network and forward() function which parses the data through each layer and returns the output.\nfor(auto\u0026amp; batch: *data_loader) { auto data = batch.data; auto target = batch.target.squeeze(); } So we have retrieved our data and label (target) - which also depends on the batch size. If you have batch_size as 4 in the torch::data::make_data_loader() function, then size of the target and data will be 4.\nDefining the Hyperparameters in Libtorch Remember the Hyperparameters we need to define for training? Let\u0026rsquo;s take a quick review of what they are:\n Batch Size Optimizer Loss Function  We have used batch_size parameter above while making the data loader. For defining optimizer, we\u0026rsquo;ll go for Adam Optimizer here:\n// We need to define the network first auto net = std::make_shared\u0026lt;Net\u0026gt;(); torch::optim::Adam optimizer(net-\u0026gt;parameters(), torch::optim::AdamOptions(1e-3)); Note that the PyTorch C++ API supports below listed optimizers:\n RMSprop SGD Adam Adagrad LBFGS LossClosureOptimizer  As mentioned in the documentation of torch.optim package:\nThe documentation is self explanatory, so all we need to do is pass parameters of our Network which will be optimized using our optimizer, and pass in the learning rate like above. To know about parameters we can pass through AdamOptions, check out this documentation page.\nLet\u0026rsquo;s go ahead and learn to define Loss Function in PyTorch C++ API. For an example, we\u0026rsquo;ll define nll_loss (Negative Log Likelihood Loss Function):\nauto output = net-\u0026gt;forward(data); auto loss = torch::nll_loss(output, target); // To backpropagate loss loss.backward() If you need to output the loss, use: loss.item\u0026lt;float\u0026gt;().\nTraining the Network We are close to our last step! Training the network is almost similar to the way we do in Python. That\u0026rsquo;s why, I\u0026rsquo;ll include the code snippet here which should be self explanatory (since we have already discussed the core parts of it).\ndataset_size = custom_dataset.size().value(); int n_epochs = 10; // Number of epochs  for(int epoch=1; epoch\u0026lt;=n_epochs; epoch++) { for(auto\u0026amp; batch: *data_loader) { auto data = batch.data; auto target = batch.target.squeeze(); // Convert data to float32 format and target to Int64 format  // Assuming you have labels as integers  data = data.to(torch::kF2); target = target.to(torch::kInt64); // Clear the optimizer parameters  optimizer.zero_grad(); auto output = net-\u0026gt;forward(data); auto loss = torch::nll_loss(output, target); // Backpropagate the loss  loss.backward(); // Update the parameters  optimizer.step(); cout \u0026lt;\u0026lt; \u0026#34;Train Epoch: %d/%ld [%5ld/%5d] Loss: %.4f\u0026#34; \u0026lt;\u0026lt; epoch \u0026lt;\u0026lt; n_epochs \u0026lt;\u0026lt; batch_index * batch.data.size(0) \u0026lt;\u0026lt; dataset_size \u0026lt;\u0026lt; loss.item\u0026lt;float\u0026gt;() \u0026lt;\u0026lt; endl; } } // Save the model torch::save(net, \u0026#34;best_model.pt\u0026#34;); In the next blog (coming soon), we\u0026rsquo;ll be discussing about Making Predictions using our network and will also show an example of training our network on a benchmark dataset.\n","permalink":"https://krshrimali.github.io/posts/2019/07/training-a-network-on-custom-dataset-using-pytorch-c-api/","tags":["development","coding","cpp","notes","pytorch-cpp","libtorch"],"title":"Training a Network on Custom Dataset using PyTorch C++ API"},{"categories":["pytorch","Deep Learning"],"contents":"I\u0026rsquo;m happy to announce a Series of Blog Posts on PyTorch C++ API. Check out the blogs in the series here.\nHappy Reading!\n","permalink":"https://krshrimali.github.io/posts/2019/07/announcing-a-series-of-blogs-on-pytorch-c-api/","tags":["development","coding","cpp","notes","pytorch-cpp","libtorch"],"title":"Announcing a series of blogs on PyTorch C++ API"},{"categories":["pytorch","deep learning"],"contents":"Overview: How C++ API loads data? In the last blog, we discussed application of a VGG-16 Network on MNIST Data. For those, who are reading this blog for the first time, here is how we had loaded MNIST data:\nauto data_loader = torch::data::make_data_loader\u0026lt;torch::data::samplers::SequentialSampler\u0026gt;( std::move(torch::data::datasets::MNIST(\u0026#34;../../data\u0026#34;).map(torch::data::transforms::Normalize\u0026lt;\u0026gt;(0.13707, 0.3081))).map( torch::data::transforms::Stack\u0026lt;\u0026gt;()), 64); Let\u0026rsquo;s break this piece by piece, as for beginners, this may be unclear. First, we ask the C++ API to load data (images and labels) into tensors.\n// Your data should be at: ../data position auto data_set = torch::data::datasets::MNIST(\u0026#34;../data\u0026#34;); If you have this question on how the API loads the images and labels to tensors - we\u0026rsquo;ll get to that. For now, just take it as a black box, which loads the data. Next, we apply transforms (like normalizing to ImageNet standards):\nauto data_set = data_set.map(torch::data::transforms::Normalize\u0026lt;\u0026gt;(0.13707, 0.3081)).map(torch::data::transforms::Stack\u0026lt;\u0026gt;()) For the sake of batch size, let\u0026rsquo;s divide the data for batch size as 64.\nstd::move(data_set, 64); Once this all is done, we can iterate through the data loader and pass each batch to the network. It\u0026rsquo;s time to understand how this all works, let\u0026rsquo;s go ahead and look at the source code of torch::data::datasets::MNIST class.\nnamespace torch { namespace data { namespace datasets { /// The MNIST dataset. class TORCH_API MNIST : public Dataset\u0026lt;MNIST\u0026gt; { public: /// The mode in which the dataset is loaded.  enum class Mode { kTrain, kTest }; /// Loads the MNIST dataset from the root path.  ///  /// The supplied root path should contain the content of the unzipped  /// MNIST dataset, available from http://yann.lecun.com/exdb/mnist.  explicit MNIST(const std::string\u0026amp; root, Mode mode = Mode::kTrain); /// Returns the Example at the given index.  Example\u0026lt;\u0026gt; get(size_t index) override; /// Returns the size of the dataset.  optional\u0026lt;size_t\u0026gt; size() const override; /// Returns true if this is the training subset of MNIST.  bool is_train() const noexcept; /// Returns all images stacked into a single tensor.  const Tensor\u0026amp; images() const; /// Returns all targets stacked into a single tensor.  const Tensor\u0026amp; targets() const; private: Tensor images_, targets_; }; } // namespace datasets } // namespace data } // namespace torch Reference: https://github.com/pytorch/pytorch/blob/master/torch/csrc/api/include/torch/data/datasets/mnist.h\nAssuming the reader has done some basic C++ before reading this, they will be very well aware of how to initialize a C++ Class. Let\u0026rsquo;s go step by step. What happens when we initialize the class? Let\u0026rsquo;s look at the definition of constructor of the class MNIST at mnist.cpp:\nMNIST::MNIST(const std::string\u0026amp; root, Mode mode) : images_(read_images(root, mode == Mode::kTrain)), targets_(read_targets(root, mode == Mode::kTrain)) {} Observing the above snippet, it\u0026rsquo;s clear that the constructor calls read_images(root, mode) and read_targets for loading images and labels into tensors. Let\u0026rsquo;s go to the source code of read_images() and read_targets().\n read_images():  Tensor read_images(const std::string\u0026amp; root, bool train) { // kTrainImagesFilename and kTestImagesFilename are specific to MNIST dataset here  // No need for using join_paths here  const auto path = join_paths(root, train ? kTrainImagesFilename : kTestImagesFilename); // Load images  std::ifstream images(path, std::ios::binary); TORCH_CHECK(images, \u0026#34;Error opening images file at \u0026#34;, path); // kTrainSize = len(training data)  // kTestSize = len(testing_data)  const auto count = train ? kTrainSize : kTestSize; // Specific to MNIST data  // From http://yann.lecun.com/exdb/mnist/  expect_int32(images, kImageMagicNumber); expect_int32(images, count); expect_int32(images, kImageRows); expect_int32(images, kImageColumns); // This converts images to tensors  // Allocate an empty tensor of size of image (count, channels, height, width)  auto tensor = torch::empty({count, 1, kImageRows, kImageColumns}, torch::kByte); // Read image and convert to tensor  images.read(reinterpret_cast\u0026lt;char*\u0026gt;(tensor.data_ptr()), tensor.numel()); // Normalize the image from 0 to 255 to 0 to 1  return tensor.to(torch::kFloat32).div_(255); } read_targets():  Tensor read_targets(const std::string\u0026amp; root, bool train) { // Specific to MNIST dataset (kTrainImagesFilename and kTestTargetsFilename)  const auto path = join_paths(root, train ? kTrainTargetsFilename : kTestTargetsFilename); // Read the labels  std::ifstream targets(path, std::ios::binary); TORCH_CHECK(targets, \u0026#34;Error opening targets file at \u0026#34;, path); // kTrainSize = len(training_labels)  // kTestSize = len(testing_labels)  const auto count = train ? kTrainSize : kTestSize; expect_int32(targets, kTargetMagicNumber); expect_int32(targets, count); // Allocate an empty tensor of size of number of labels  auto tensor = torch::empty(count, torch::kByte); // Convert to tensor  targets.read(reinterpret_cast\u0026lt;char*\u0026gt;(tensor.data_ptr()), count); return tensor.to(torch::kInt64); } Since we are now done with how the constructor works, let\u0026rsquo;s go ahead and see what other functions does the class inherit.\nExample\u0026lt;\u0026gt; MNIST::get(size_t index) { return {images_[index], targets_[index]}; } optional\u0026lt;size_t\u0026gt; MNIST::size() const { return images_.size(0); } The above two functions: get(size_t) and size() are used to get a sample image and label and length of the data respectively.\nThe Pipeline Since we are now clear with the possible pipeline of loading custom data:\n Read Images and Labels Convert to Tensors Write get() and size() functions Initialize the class with paths of images and labels Pass it to the data loader  Coding your own Custom Data Loader Let\u0026rsquo;s first write the template of our custom data loader:\n// Include libraries #include \u0026lt;ATen/ATen.h\u0026gt;#include \u0026lt;torch/torch.h\u0026gt;#include \u0026lt;iostream\u0026gt;#include \u0026lt;vector\u0026gt;#include \u0026lt;tuple\u0026gt;#include \u0026lt;opencv2/opencv.hpp\u0026gt;#include \u0026lt;string\u0026gt; /* Convert and Load image to tensor from location argument */ torch::Tensor read_data(std::string location) { // Read Data here  // Return tensor form of the image  return torch::Tensor; } /* Converts label to tensor type in the integer argument */ torch::Tensor read_label(int label) { // Read label here  // Convert to tensor and return  return torch::Tensor; } /* Loads images to tensor type in the string argument */ vector\u0026lt;torch::Tensor\u0026gt; process_images(vector\u0026lt;string\u0026gt; list_images) { cout \u0026lt;\u0026lt; \u0026#34;Reading Images...\u0026#34; \u0026lt;\u0026lt; endl; // Return vector of Tensor form of all the images  return vector\u0026lt;torch::Tensor\u0026gt;; } /* Loads labels to tensor type in the string argument */ vector\u0026lt;torch::Tensor\u0026gt; process_labels(vector\u0026lt;string\u0026gt; list_labels) { cout \u0026lt;\u0026lt; \u0026#34;Reading Labels...\u0026#34; \u0026lt;\u0026lt; endl; // Return vector of Tensor form of all the labels  return vector\u0026lt;torch::Tensor\u0026gt;; } class CustomDataset : public torch::data::dataset\u0026lt;CustomDataset\u0026gt; { private: // Declare 2 vectors of tensors for images and labels  vector\u0026lt;torch::Tensor\u0026gt; images, labels; public: // Constructor  CustomDataset(vector\u0026lt;string\u0026gt; list_images, vector\u0026lt;string\u0026gt; list_labels) { images = process_images(list_images); labels = process_labels(list_labels); }; // Override get() function to return tensor at location index  torch::data::Example\u0026lt;\u0026gt; get(size_t index) override { torch::Tensor sample_img = images.at(index); torch::Tensor sample_label = labels.at(index); return {sample_img.clone(), sample_label.clone()}; }; // Return the length of data  torch::optional\u0026lt;size_t\u0026gt; size() const override { return labels.size(); }; }; We are almost there, all we need to do is - Read Images and Labels to torch::Tensor type. I\u0026rsquo;ll be using OpenCV to read images, as it also helps later on to visualize results.\nReading Images:\nThe process to read an image in OpenCV is trivial: cv::imread(std::string location, int). We then convert it to a tensor. Note that a tensor is of form (batch size, channels, height, width), so we also permute the tensor to that form.\ntorch::Tensor read_data(std::string loc) { // Read Image from the location of image \tcv::Mat img = cv::imread(loc, 1); // Convert image to tensor \ttorch::Tensor img_tensor = torch::from_blob(img.data, {img.rows, img.cols, 3}, torch::kByte); img_tensor = img_tensor.permute({2, 0, 1}); // Channels x Height x Width  return img_tensor.clone(); }; Reading Labels:\n// Read Label (int) and convert to torch::Tensor type torch::Tensor read_label(int label) { torch::Tensor label_tensor = torch::full({1}, label); return label_tensor.clone(); } Final Code Let\u0026rsquo;s wrap up the code!\n// Include libraries #include \u0026lt;ATen/ATen.h\u0026gt;#include \u0026lt;torch/torch.h\u0026gt;#include \u0026lt;iostream\u0026gt;#include \u0026lt;vector\u0026gt;#include \u0026lt;tuple\u0026gt;#include \u0026lt;opencv2/opencv.hpp\u0026gt;#include \u0026lt;string\u0026gt; /* Convert and Load image to tensor from location argument */ torch::Tensor read_data(std::string loc) { // Read Data here  // Return tensor form of the image  cv::Mat img = cv::imread(loc, 1); cv::resize(img, img, cv::Size(1920, 1080), cv::INTER_CUBIC); std::cout \u0026lt;\u0026lt; \u0026#34;Sizes: \u0026#34; \u0026lt;\u0026lt; img.size() \u0026lt;\u0026lt; std::endl; torch::Tensor img_tensor = torch::from_blob(img.data, {img.rows, img.cols, 3}, torch::kByte); img_tensor = img_tensor.permute({2, 0, 1}); // Channels x Height x Width  return img_tensor.clone(); } /* Converts label to tensor type in the integer argument */ torch::Tensor read_label(int label) { // Read label here  // Convert to tensor and return  torch::Tensor label_tensor = torch::full({1}, label); return label_tensor.clone(); } /* Loads images to tensor type in the string argument */ vector\u0026lt;torch::Tensor\u0026gt; process_images(vector\u0026lt;string\u0026gt; list_images) { cout \u0026lt;\u0026lt; \u0026#34;Reading Images...\u0026#34; \u0026lt;\u0026lt; endl; // Return vector of Tensor form of all the images  vector\u0026lt;torch::Tensor\u0026gt; states; for (std::vector\u0026lt;string\u0026gt;::iterator it = list_images.begin(); it != list_images.end(); ++it) { torch::Tensor img = read_data(*it); states.push_back(img); } return states; } /* Loads labels to tensor type in the string argument */ vector\u0026lt;torch::Tensor\u0026gt; process_labels(vector\u0026lt;string\u0026gt; list_labels) { cout \u0026lt;\u0026lt; \u0026#34;Reading Labels...\u0026#34; \u0026lt;\u0026lt; endl; // Return vector of Tensor form of all the labels  vector\u0026lt;torch::Tensor\u0026gt; labels; for (std::vector\u0026lt;int\u0026gt;::iterator it = list_labels.begin(); it != list_labels.end(); ++it) { torch::Tensor label = read_label(*it); labels.push_back(label); } return labels; } class CustomDataset : public torch::data::dataset\u0026lt;CustomDataset\u0026gt; { private: // Declare 2 vectors of tensors for images and labels  vector\u0026lt;torch::Tensor\u0026gt; images, labels; public: // Constructor  CustomDataset(vector\u0026lt;string\u0026gt; list_images, vector\u0026lt;string\u0026gt; list_labels) { images = process_images(list_images); labels = process_labels(list_labels); }; // Override get() function to return tensor at location index  torch::data::Example\u0026lt;\u0026gt; get(size_t index) override { torch::Tensor sample_img = images.at(index); torch::Tensor sample_label = labels.at(index); return {sample_img.clone(), sample_label.clone()}; }; // Return the length of data  torch::optional\u0026lt;size_t\u0026gt; size() const override { return labels.size(); }; }; int main(int argc, char** argv) { vector\u0026lt;string\u0026gt; list_images; // list of path of images  vector\u0026lt;int\u0026gt; list_labels; // list of integer labels  // Dataset init and apply transforms - None!  auto custom_dataset = CustomDataset(list_images, list_labels).map(torch::data::transforms::Stack\u0026lt;\u0026gt;()); } That\u0026rsquo;s it for today! In the next blog, we\u0026rsquo;ll use this custom data loader and implement a CNN on our data. By then, happy learning. Hope you liked this blog. :)\n","permalink":"https://krshrimali.github.io/posts/2019/07/custom-data-loading-using-pytorch-c-api/","tags":["development","coding","cpp","notes","pytorch-cpp","libtorch"],"title":"Custom Data Loading using PyTorch C++ API"},{"categories":["pytorch","deep learning"],"contents":"Environment Setup [Ubuntu 16.04, 18.04] Note: If you have already finished installing PyTorch C++ API, please skip this section.\n  Download libtorch:\n CPU Version: wget https://download.pytorch.org/libtorch/cpu/libtorch-shared-with-deps-latest.zip -O libtorch.zip GPU Version (CUDA 9.0): wget https://download.pytorch.org/libtorch/cu90/libtorch-shared-with-deps-latest.zip -O libtorch.zip GPU Version (CUDA 10.0): wget https://download.pytorch.org/libtorch/cu100/libtorch-shared-with-deps-latest.zip    Unzip libtorch.zip:\n unzip libtorch.zip    We\u0026rsquo;ll use the absolute path of extracted directory (libtorch) later on.\nImplementation The VGG-16 Network is shown in the Figure below.\nWe\u0026rsquo;ll start of by first including libtorch header file.\n#include \u0026lt;torch/torch.h\u0026gt;\nWe\u0026rsquo;ll then go ahead and define the network. We\u0026rsquo;ll inherit layers from torch::nn::Module.\n/* Sample code for training a FCN on MNIST dataset using PyTorch C++ API */ /* This code uses VGG-16 Layer Network */ struct Net: torch::nn::Module { // VGG-16 Layer  // conv1_1 - conv1_2 - pool 1 - conv2_1 - conv2_2 - pool 2 - conv3_1 - conv3_2 - conv3_3 - pool 3 -  // conv4_1 - conv4_2 - conv4_3 - pool 4 - conv5_1 - conv5_2 - conv5_3 - pool 5 - fc6 - fc7 - fc8  // Note: pool 5 not implemented as no need for MNIST dataset  Net() { // Initialize VGG-16  // On how to pass strides and padding: https://github.com/pytorch/pytorch/issues/12649#issuecomment-430156160  conv1_1 = register_module(\u0026#34;conv1_1\u0026#34;, torch::nn::Conv2d(torch::nn::Conv2dOptions(1, 10, 3).padding(1))); conv1_2 = register_module(\u0026#34;conv1_2\u0026#34;, torch::nn::Conv2d(torch::nn::Conv2dOptions(10, 20, 3).padding(1))); // Insert pool layer  conv2_1 = register_module(\u0026#34;conv2_1\u0026#34;, torch::nn::Conv2d(torch::nn::Conv2dOptions(20, 30, 3).padding(1))); conv2_2 = register_module(\u0026#34;conv2_2\u0026#34;, torch::nn::Conv2d(torch::nn::Conv2dOptions(30, 40, 3).padding(1))); // Insert pool layer  conv3_1 = register_module(\u0026#34;conv3_1\u0026#34;, torch::nn::Conv2d(torch::nn::Conv2dOptions(40, 50, 3).padding(1))); conv3_2 = register_module(\u0026#34;conv3_2\u0026#34;, torch::nn::Conv2d(torch::nn::Conv2dOptions(50, 60, 3).padding(1))); conv3_3 = register_module(\u0026#34;conv3_3\u0026#34;, torch::nn::Conv2d(torch::nn::Conv2dOptions(60, 70, 3).padding(1))); // Insert pool layer  conv4_1 = register_module(\u0026#34;conv4_1\u0026#34;, torch::nn::Conv2d(torch::nn::Conv2dOptions(70, 80, 3).padding(1))); conv4_2 = register_module(\u0026#34;conv4_2\u0026#34;, torch::nn::Conv2d(torch::nn::Conv2dOptions(80, 90, 3).padding(1))); conv4_3 = register_module(\u0026#34;conv4_3\u0026#34;, torch::nn::Conv2d(torch::nn::Conv2dOptions(90, 100, 3).padding(1))); // Insert pool layer  conv5_1 = register_module(\u0026#34;conv5_1\u0026#34;, torch::nn::Conv2d(torch::nn::Conv2dOptions(100, 110, 3).padding(1))); conv5_2 = register_module(\u0026#34;conv5_2\u0026#34;, torch::nn::Conv2d(torch::nn::Conv2dOptions(110, 120, 3).padding(1))); conv5_3 = register_module(\u0026#34;conv5_3\u0026#34;, torch::nn::Conv2d(torch::nn::Conv2dOptions(120, 130, 3).padding(1))); // Insert pool layer  fc1 = register_module(\u0026#34;fc1\u0026#34;, torch::nn::Linear(130, 50)); fc2 = register_module(\u0026#34;fc2\u0026#34;, torch::nn::Linear(50, 20)); fc3 = register_module(\u0026#34;fc3\u0026#34;, torch::nn::Linear(20, 10)); } // Implement Algorithm  torch::Tensor forward(torch::Tensor x) { x = torch::relu(conv1_1-\u0026gt;forward(x)); x = torch::relu(conv1_2-\u0026gt;forward(x)); x = torch::max_pool2d(x, 2); x = torch::relu(conv2_1-\u0026gt;forward(x)); x = torch::relu(conv2_2-\u0026gt;forward(x)); x = torch::max_pool2d(x, 2); x = torch::relu(conv3_1-\u0026gt;forward(x)); x = torch::relu(conv3_2-\u0026gt;forward(x)); x = torch::relu(conv3_3-\u0026gt;forward(x)); x = torch::max_pool2d(x, 2); x = torch::relu(conv4_1-\u0026gt;forward(x)); x = torch::relu(conv4_2-\u0026gt;forward(x)); x = torch::relu(conv4_3-\u0026gt;forward(x)); x = torch::max_pool2d(x, 2); x = torch::relu(conv5_1-\u0026gt;forward(x)); x = torch::relu(conv5_2-\u0026gt;forward(x)); x = torch::relu(conv5_3-\u0026gt;forward(x)); x = x.view({-1, 130}); x = torch::relu(fc1-\u0026gt;forward(x)); x = torch::relu(fc2-\u0026gt;forward(x)); x = fc3-\u0026gt;forward(x); return torch::log_softmax(x, 1); } // Declare layers  torch::nn::Conv2d conv1_1{nullptr}; torch::nn::Conv2d conv1_2{nullptr}; torch::nn::Conv2d conv2_1{nullptr}; torch::nn::Conv2d conv2_2{nullptr}; torch::nn::Conv2d conv3_1{nullptr}; torch::nn::Conv2d conv3_2{nullptr}; torch::nn::Conv2d conv3_3{nullptr}; torch::nn::Conv2d conv4_1{nullptr}; torch::nn::Conv2d conv4_2{nullptr}; torch::nn::Conv2d conv4_3{nullptr}; torch::nn::Conv2d conv5_1{nullptr}; torch::nn::Conv2d conv5_2{nullptr}; torch::nn::Conv2d conv5_3{nullptr}; torch::nn::Linear fc1{nullptr}, fc2{nullptr}, fc3{nullptr}; }; Once done, we can go ahead and test the network on our sample dataset. Let\u0026rsquo;s go ahead and load data first. We\u0026rsquo;ll be using 10 epochs, learning rate (0.01), and nll_loss as loss functio.\nint main() { // Create multi-threaded data loader for MNIST data \tauto data_loader = torch::data::make_data_loader\u0026lt;torch::data::samplers::SequentialSampler\u0026gt;( std::move(torch::data::datasets::MNIST(\u0026#34;/absolute/path/to/data\u0026#34;).map(torch::data::transforms::Normalize\u0026lt;\u0026gt;(0.13707, 0.3081)).map( torch::data::transforms::Stack\u0026lt;\u0026gt;())), 64); // Build VGG-16 Network  auto net = std::make_shared\u0026lt;Net\u0026gt;(); torch::optim::SGD optimizer(net-\u0026gt;parameters(), 0.01); // Learning Rate 0.01  // net.train();  for(size_t epoch=1; epoch\u0026lt;=10; ++epoch) { size_t batch_index = 0; // Iterate data loader to yield batches from the dataset \tfor (auto\u0026amp; batch: *data_loader) { // Reset gradients \toptimizer.zero_grad(); // Execute the model \ttorch::Tensor prediction = net-\u0026gt;forward(batch.data); // Compute loss value \ttorch::Tensor loss = torch::nll_loss(prediction, batch.target); // Compute gradients \tloss.backward(); // Update the parameters \toptimizer.step(); // Output the loss and checkpoint every 100 batches \tif (++batch_index % 2 == 0) { std::cout \u0026lt;\u0026lt; \u0026#34;Epoch: \u0026#34; \u0026lt;\u0026lt; epoch \u0026lt;\u0026lt; \u0026#34; | Batch: \u0026#34; \u0026lt;\u0026lt; batch_index \u0026lt;\u0026lt; \u0026#34; | Loss: \u0026#34; \u0026lt;\u0026lt; loss.item\u0026lt;float\u0026gt;() \u0026lt;\u0026lt; std::endl; torch::save(net, \u0026#34;net.pt\u0026#34;); } } } } For code, check out my repo here: https://github.com/krshrimali/Digit-Recognition-MNIST-SVHN-PyTorch-CPP\nIn the next blog, we will discuss about another network on MNIST and SVHN Dataset.\nReferences  https://pytorch.org/cppdocs/ http://yann.lecun.com/exdb/mnist/  ","permalink":"https://krshrimali.github.io/posts/2019/06/introduction-to-pytorch-c-api-mnist-digit-recognition-using-vgg-16-network/","tags":["development","coding","cpp","notes","pytorch-cpp","libtorch"],"title":"Introduction to PyTorch C++ API: MNIST Digit Recognition using VGG-16 Network"},{"categories":null,"contents":"Hello World! I\u0026rsquo;m Kushashwa Ravi Shrimali (friends call me Kush, world knows me as krshrimali), a Computer Science and Engineering Graduate from IIIT-NR. I work as a software developer (PyTorch team) at Quansight Labs. I love development, optimization and reading source codes! Have explored a lot of fields during my graduation, development is what I found the most intuitive for myself.\nIf you want to help support my journey, please care to check my patreon page here. I\u0026rsquo;ll really appreciate any support! :)\nOver the weekends - I contribute to opensource, go live on YouTube and also write blogs (where you are right now). Make sure to check out my YouTube channel if you like it! But that\u0026rsquo;s not it, I like singing, dancing on any random music I like and discussing science \u0026amp; politics.\nI love writing impactful codes! My work doesn\u0026rsquo;t stop at getting the accuracy. I love optimizing the models to help deploy in real-time.\nPlease navigate to categories, to start reading my blogs (category-wise).\nWork Experience  Quansight Labs (Software Developer) (June 2021 to Present)  Contributing to PyTorch, check my work here: https://github.com/pytorch/pytorch/pulls/krshrimali   Sybill AI (Applied ML Engineer), Bangalore, India (February 2021 to May 2021)  Designed multiâ€‘modal models in ML pipeline. Working on designing ML test framework and using MLFlow for ML experiments and hyper parameter tracking.   NVIDIA (Deep Learning Frameworks Team, PyTorch), Santa Clara, US  On site internship at NVIDIA Head Quarters, Santa Clara. See the following PRs if you want to know what I worked on.  https://github.com/pytorch/pytorch/pull/33322 https://github.com/pytorch/pytorch/pull/33063 https://github.com/pytorch/pytorch/pull/33063   Mentor: Michael Carilli (Senior Software Developer, NVIDIA) Manager: Christian Sarofeen   Dukaan (R\u0026amp;D Deep Learning Engineer), Bangalore, India (September 2020 to October 2020)  Worked as R\u0026amp;D DL Engineer. Contributed in training custom CV and NLP models for deployment for Dukaan App. Note: Had to leave and come back home for indefinite time because of COVID Emergency at home.   Care AI (Software Developer Intern, AI), Florida, US (April 2020 to August 2020)  Remotely working (due to COVID-19 pandemic). Major responsibilities include:  Optimizing workflow using TensorRT (mainly using C++ API).     Applying Deep Learning, Speech Recognition and Machine Learning models on Android Apps.   Rapid Rich Object Search (ROSE) Labs, NTU Singapore  On-site internship at ROSE Labs, NTU Singapore under Prof. Alex Kot (Director, ROSE Labs). Worked on Automatic License Plate Recognition technique in real time using PyTorch in both C++ and Python. Used YOLO based network and achieved 85% accuracy for Single Line License Plates and Double Line License Plates (tested on real time dataset of Singaporean Vehicles). Mentor: Prof. Alex Kot, Dennis Sng (Deputy Director \u0026amp; Principal Scientist) and Devadeep Shyam (Project Mangaer, AI)   Big Vision LLC and LearnOpenCV, California  Worked from home in writing blogs and working for clients for Big Vision LLC. Became OpenCV Contributor during my internship, and worked on several projects using C, C++ and Python. Mentor: Dr. Satya Mallick (CEO, LearnOpenCV)   Indian Institute of Information Technology and Management, Gwalior  On-site internship at IIITM Gwalior and worked on A* Search Algorithm to understand communication between a robot and a UAV. Co-authored a paper with Ashish Upadhyay and Prof. Anupam Shukla: https://www.sciencedirect.com/science/article/pii/S1877050918309979. Mentor: Prof. Anupam Shukla (now Director, IIIT Pune)   OpenStudy, Palo Alto  The responsibilities included to actively promote the website and itâ€™s cause amongst the students and to guide them through the beginning steps for the website. Made sure the smooth following of Code of Conduct of OpenStudy. Also worked as a leader of OpenStudy Newsletter Programme for two editions as well as Designer in one edition. Worked under Preetha Ram and Ashwin Ram (now: Technical Director of AI, Google Cloud)   Student Activity Concil, IIIT Naya Raipur  Coordinated clubs running under SAC: Artificial Intelligence and Machine Learning Club, The Society of Coders. Also became the Technical Coordinator of Annual Technical and Cultural Fest - TechNovate 2019.    YouTube Channel This deserves another section since I\u0026rsquo;ve put a lot of hard work into this. :)\n https://youtube.com/c/kushashwaraviShrimali  I upload videos on weekdays, after my work hours and go live on weekends to talk about AI, development and importance of optimization.\nSkills  Programming Language: C++, CUDA, C, Shell, Python, Perl Tools: LaTeX, Git, MATLAB, Jupyter, Android Studio Frameworks: OpenCV, PyTorch, Tensorflow Libraries: NumPy, Matplotlib, scikit-learn, OpenCV, PyTorch, Keras, Tensorflow, Spacy  Non-Academic Interests and Achievements I involve myself in non-academic activities like Debating, blogging, playing Table Tennis and more. I was also a Scout in my high school. I love World History and love discussing on social and international topics. I\u0026rsquo;ve won several competitions in Debates (more about them in my CV).\nContact Me Feel free to reach out to me on Twitter, or on email: kushashwaravishrimali@gmail.com\n","permalink":"https://krshrimali.github.io/about/","tags":null,"title":"About"},{"categories":null,"contents":"","permalink":"https://krshrimali.github.io/categories/","tags":null,"title":"Categories"}]