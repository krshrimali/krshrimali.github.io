[{"content":"I\u0026rsquo;ve had many iterations on this project that I\u0026rsquo;m going to talk about today, over the last 1.5-2 years. There are good and bad things about it. Good is that I\u0026rsquo;m persisting with completing it one day, bad is that I\u0026rsquo;ve not done it yet.\nIn this blog, I\u0026rsquo;ll quickly touch upon why it has taken so much time and what\u0026rsquo;s happening behind the scenes.\nAbout the project: For a piece of code, this project is going to help you with questions like:\n\u0026ldquo;Where is this logic tested?\u0026rdquo; \u0026ldquo;Which other features does this logic mostly pair with?\u0026rdquo; \u0026ldquo;What other things are dependent on this logic?\u0026rdquo; \u0026ldquo;Who is the right person to reach out to, for any queries?\u0026rdquo;\nI hope these points are enough motivation for someone to understand the importance of this project.\nProgress: I\u0026rsquo;ve had a working model in 2023 when I started, and plugins ready for neovim and vscode but I realized this project - for a better direction - needs to act like an LSP. Do a lot of async work, like indexing, and make sure results are quicker and reliable.\nNow, converting the initial binary code into server, wasn\u0026rsquo;t easy at all, for several reasons:\nI started with a simple JSON as the DB, for initial PoC. JSON is not scalable, but that wasn\u0026rsquo;t the bottleneck back then. Bottelneck was reading a huge JSON file - so the immediate solution was to use DB Sharding concept - divide your huge file into chunks, and know which one to read when a user queries. Making this a proper async server hasn\u0026rsquo;t been easy. Rust (or for that matter, any language) async programming comes with its own debugging issues, and complex code. I\u0026rsquo;ve had to rewrite the codebase several times to make it work. And of course, apart from the technical excuses, there has been life:\nWork has kept me very busy, for good. Of course, no excuses but that\u0026rsquo;s what pays me and I like being religious towards it - so yep, I enjoy spending most of my time there. Saturation - Of course, working on a single thing for 2 years can get tiring. I\u0026rsquo;ve had some other projects picked up, and targeting physical fitness as well, so yes - I\u0026rsquo;ve not really spent 2 years on this project only :D obviously! Anyways, there is some motivation again to help finish this. Mostly for the reasons that I want to use it personally, and I think it is going to be of huge help to me at work and personally as well.\nWhat\u0026rsquo;s next? I\u0026rsquo;m going to revamp this whole project. The way it was designed before, doesn\u0026rsquo;t seem right to me anymore. I think I had too many API details exposed on the Database level for the sake of making it work:\n#[derive(Default, Clone)] pub struct DB { pub index: u32, // The line of code that you are at, right now? TODO: pub folder_path: String, // Current folder path that this DB is processing, or the binary is running pub curr_items: u32, // TODO: pub mapping_file_name: String, // This is for storing which file is in which folder/file? \u0026lt;-- TODO: pub current_data: DBType, // The data that we have from the loaded DB into our inhouse member pub db_file_path: String, pub mapping_file_path: String, pub mapping_data: MappingDBType, pub curr_file_path: String, pub workspace_path: String, } As you can already see, this is just too much detail for - even me to understand after some time. I know, at some point of time, even if this works, I\u0026rsquo;m going to face the issue of \u0026ldquo;Wait, what does this mean? Why did I write it?\u0026rdquo;. So, better fix it now. It\u0026rsquo;s also a code quality thing I\u0026rsquo;ve in mind that I hate debugging things further. I\u0026rsquo;ve had some luck while fixing here where I was at least able to find the bug, but then fixing it would just need me to fix the DB asynchronous writing and reading.\nIt\u0026rsquo;s not ergonomic for me and the readers as well to look at the piece of code in the current state and understand what caused what :)\nI\u0026rsquo;ll share more technical details in the next blog, but for now, this would be it.\n","permalink":"https://krshrimali.github.io/posts/2025/03/progress-update-context-pilot-revamping/","summary":"\u003cp\u003eI\u0026rsquo;ve had many iterations on this project that I\u0026rsquo;m going to talk about today, over the last 1.5-2 years. There are good and bad things about it. Good is that I\u0026rsquo;m persisting with completing it one day, bad is that I\u0026rsquo;ve not done it yet.\u003c/p\u003e\n\u003cp\u003eIn this blog, I\u0026rsquo;ll quickly touch upon why it has taken so much time and what\u0026rsquo;s happening behind the scenes.\u003c/p\u003e\n\u003ch2 id=\"about-the-project\"\u003eAbout the project:\u003c/h2\u003e\n\u003cp\u003eFor a piece of code, this project is going to help you with questions like:\u003c/p\u003e","title":"Progress Update: Context Pilot (Revamping)"},{"content":"It\u0026rsquo;s the Day-1 of staying curious. I\u0026rsquo;ll talk more about it towards the end, but first - let\u0026rsquo;s get to the topic.\nStarting from my college life, I\u0026rsquo;ve always been excited to know the \u0026ldquo;how\u0026rdquo;, \u0026ldquo;why\u0026rdquo;, \u0026ldquo;what\u0026rdquo;, \u0026ldquo;where\u0026rdquo; questions of each data structure we would study. This series is not just about data structures though, anything that you can imagine with Computer Science - I hope to cover some of those in some more blog posts.\nI want to cover Linked Lists today and some of its applications.\nNOTE: FYI, I did go through some other usages of LinkedLists before on my YouTube Channel: https://youtu.be/Czu5dHq86Tk?si=zng7vUrGNPQwC0hB - if any of you would like to check that out.\nMotivation Linked Lists are probably only focused for interview preparations (at least from my experience with folks). But I want to show some more interesting things linked lists already do in the world of computers. Let\u0026rsquo;s start with a simple example of reversing a Linked List to serve a revision for most of you:\n#include \u0026lt;iostream\u0026gt; struct LL { int data; LL* next; }; LL* reverse(LL* head) { LL* prev = nullptr; LL* current = head; LL* next = nullptr; while (current != nullptr) { next = current-\u0026gt;next; current-\u0026gt;next = prev; prev = current; current = next; } head = prev; return head; } void printLL(LL* head) { while (head != nullptr) { std::cout \u0026lt;\u0026lt; head-\u0026gt;data \u0026lt;\u0026lt; \u0026#34; \u0026#34;; head = head-\u0026gt;next; } std::cout \u0026lt;\u0026lt; std::endl; } int main() { LL* head = new LL(); head-\u0026gt;data = 1; head-\u0026gt;next = new LL(); head-\u0026gt;next-\u0026gt;data = 2; head-\u0026gt;next-\u0026gt;next = new LL(); head-\u0026gt;next-\u0026gt;next-\u0026gt;data = 3; head-\u0026gt;next-\u0026gt;next-\u0026gt;next = nullptr; printLL(head); LL* reversedHead = reverse(head); printLL(reversedHead); } The output will be simple, we\u0026rsquo;ll have 1 2 3 and 3 2 1 printed in two lines. This is a simple example of reversing a linked list.\nBut what if I tell you that Linked Lists are also used for garbage collection in programming languages. Yes, that\u0026rsquo;s right! Linked Lists are used to keep track of memory that is not being used by the program and can be freed up. This is just one of the many applications of Linked Lists, and let\u0026rsquo;s explore this further. Why not discuss Garbage Collection a bit?\nGarbage Collection Note: While the definition might stay constant, but implementation details might vary from language to language.\nGarbage Collection is a process of automatically finding and freeing up memory that is no longer being used by the program. This is done by the Garbage Collector, which is a part of the runtime environment of the programming language.\nTo make it simple, let\u0026rsquo;s consider a simple example:\nint main() { int* ptr = new int(5); ptr = nullptr; return 0; } In the above code, we have allocated memory for an integer and then set the pointer to nullptr. This means that the memory allocated for the integer is no longer being used by the program. This is where the Garbage Collector comes into play. It will automatically find this memory and free it up so that it can be used by other parts of the program. This is a very simple example, but in real-world applications, memory management can get very complex.\nDo note that C++ does not have a Garbage Collector (at least not with the standard library). There are however external libraries that can be used for garbage collection in C++. One of them is the Boehm-Demers-Weiser Garbage Collector.\nGoing a bit in detail, there are a few strategies that are opted, one of the less popular ones (w.r.t being implemented in languages) is \u0026ldquo;Free List\u0026rdquo;.\nFree List FreeList is essentially a data structure implemented using LinkedList that keeps track of the memory blocks that are not being used by the program. The next time when the program needs memory, it can check the FreeList to see if there are any memory blocks that can be reused.\nLet\u0026rsquo;s again understand this with an example:\nstruct MemoryBlock { int data; MemoryBlock* next; }; MemoryBlock* freeList = nullptr; MemoryBlock* allocateMemoryBlock() { if (freeList == nullptr) { // Allocate a new memory block return new MemoryBlock(); } else { MemoryBlock* block = freeList; freeList = freeList-\u0026gt;next; return block; } } In the above code, we have a MemoryBlock structure that represents a memory block. We have a freeList that keeps track of the memory blocks that are not being used. The allocateMemoryBlock function checks if there are any memory blocks in the freeList that can be reused. If there are no memory blocks in the freeList, it allocates a new memory block. This won\u0026rsquo;t work for most of the cases though, mostly because the requirement for the next memory block could be more then what we have in the freeList\u0026rsquo;s head pointer.\nThere are different variants of how FreeList can be implemented, but the basic idea remains the same - to keep track of memory blocks that are not being used by the program. I don\u0026rsquo;t want to copy and paste the content from FreeList\u0026rsquo;s Wikipedia page, but you can read more about it here. Coming from the same Wikipedia page, apparently - OCaml does use FreeList for satisfying its memory allocation requests (source: https://dev.realworldocaml.org/garbage-collector.html). I\u0026rsquo;ll definitely suggest y\u0026rsquo;all to give it a read.\nA very popular strategy is Mark-and-Sweep. I\u0026rsquo;ll cover this in a separate blog post when we highlight Garbage Collection in detail. Please note that, this is simply a strategy and use of LinkedLists is independent of what strategies you use.\nGiven that y\u0026rsquo;all might have gain some context already, let\u0026rsquo;s now understand why would we need Linked Lists for Garbage Collection.\nWhy Linked Lists? Dynamic Size: Linked Lists can grow or shrink in size during the execution of the program. They are stored in the heap memory, which is dynamic in nature. Many of you might be already aware - insertions and deletions are O(1) in Linked Lists. This is because we can just change the pointers and the memory is already allocated. No Memory Wastage: Linked Lists do not waste memory. If we have a linked list of size 5, it will only take 5 memory blocks. This is not the case with arrays, where we have to allocate memory for the maximum size of the array. Flexible: You can store more complex data in a LinkedList (easily). Data could be of varied size as well. Enough reasons for us to look at Linked Lists in a different light, right?\nIf you\u0026rsquo;re curious on more implementation, I was looking at this repository, and it\u0026rsquo;s a nicely written simple garbage collector in C. I\u0026rsquo;ll definitely suggest y\u0026rsquo;all to give it a read.\nThink of what you\u0026rsquo;ll want your garbage collector to do post a malloc call? How would you want to manage the memory blocks? What if you have a free call? How would you want to manage the memory blocks then? Let\u0026rsquo;s answer these questions before wrapping up.\nIf you remember, the MemoryBlock struct we defined earlier, we can use that to manage the memory blocks. We can have a freeList that keeps track of the memory blocks that are not being used. When we allocate a new memory block, we can check if there are any memory blocks in the freeList that can be reused. If there are no memory blocks in the freeList, we can allocate a new memory block. When we free a memory block, we can add it to the freeList so that it can be reused later.\nstruct GarbageCollector { MemoryBlock* freeList; } gc; MemoryBlock* allocateMemoryBlock() { if (gc.freeList == nullptr) { // Allocate a new memory block return new MemoryBlock(); } else { MemoryBlock* block = gc.freeList; gc.freeList = gc.freeList-\u0026gt;next; return block; } } A good implementation would be to also consider coalescing the memory blocks. This means that if there are two adjacent memory blocks in the freeList, we can merge them into a single memory block. This can help in reducing fragmentation and improving memory utilization (specially for the cases when the required size is more than the individual blocks). Maybe something y\u0026rsquo;all can try implementing? üöÄ\nFor this blog, I hope this would have given you more questions to ask and more things to explore. I\u0026rsquo;ll be back with more such interesting topics in the next blog post. Stay tuned!\nThank you for reading! üöÄ\n","permalink":"https://krshrimali.github.io/posts/2024/10/where-are-linked-lists-used/","summary":"\u003cp\u003eIt\u0026rsquo;s the Day-1 of \u003cem\u003estaying\u003c/em\u003e curious. I\u0026rsquo;ll talk more about it towards the end, but first - let\u0026rsquo;s get to the topic.\u003c/p\u003e\n\u003cp\u003eStarting from my college life, I\u0026rsquo;ve always been excited to know the \u0026ldquo;how\u0026rdquo;, \u0026ldquo;why\u0026rdquo;, \u0026ldquo;what\u0026rdquo;, \u0026ldquo;where\u0026rdquo; questions of each data structure we would study. This series is not just about data structures though, anything that you can imagine with Computer Science - I hope to cover some of those in some more blog posts.\u003c/p\u003e","title":"Where are Linked Lists used?"},{"content":"So this happened today, I was attempting a typing speed test on monkeytype and quickly realised a few things that kind of related in my personal life. Thought I\u0026rsquo;ll share with y\u0026rsquo;all and I\u0026rsquo;m confident you all will relate!\nFor context, I\u0026rsquo;ve been at around 120 wpm (avg) to 135 wpm (max) in a 30 seconds speed test for quite some time now (maybe more than a year at least). And very honestly, I\u0026rsquo;m happy about it, I don\u0026rsquo;t wwant to grow my typing speed further - in fact I never planned it before as well. I used to just type as fast as my fingers would allow, and turns out it was good enough for me to do my work. Right now, my goal is more towards sustaining my wrists and fingers along with work, as it definitely pains a lot if your positioning is not right.\nI\u0026rsquo;ve moved to another city (temporarily, for personal reasons) and I\u0026rsquo;m away from my beautiful (and sacred? idk, I just worship it a lot) setup, which also means that I haven\u0026rsquo;t had time yet to stream/upload videos/write blogs. More on this in another blog maybe, but today I was just attempting a typing speed test. Initially, I was around 120, but in the next one, I scored 135. Screenshot attached below:\nI started thinking - what was different this time? Terrain? Keyboard? Hands? Person? Time? Nope! Luck? Maybe. But one thing stood out, I was reading the next word while I was typing the current one. I started off by reading 2 words, so I knew A and B, and when I would type B - I would already know it so I\u0026rsquo;ll just end up reading C and so on. This gave me an idea, we perform better when we prepare ourselves for what\u0026rsquo;s gonna happen in the next closest move, right? Yes, of course I \u0026ldquo;knew\u0026rdquo; what was the next word this time but that\u0026rsquo;s an effort I made for myself to read what\u0026rsquo;s going to happen next and prepare my fingers accordingly. Focus on the present, yes, but are you preparing yourself for the near future? Did you ever imagine what it would look like?\nI\u0026rsquo;ve done this for some time now, whenever I\u0026rsquo;ll go out running, or do any physical exercise - I would imagine myself being happy after I\u0026rsquo;m done accomplishing my goal. Of course I won\u0026rsquo;t be as built as others, but I accomplished something that I planned. And if I failed, I know what I missed out on. And that would help me push forward for being more disciplined and resilient next time.\nThe problem is, we don\u0026rsquo;t even know what we are missing out on once we quit (anything). We realise it after a year or XYZ time - \u0026ldquo;Oh, I missed out on this. Had I stayed longer, it would have worked\u0026rdquo;. Also, this sounds like a relationship advice, right? Maybe it is then! You have a relationship with your city, your company, your friends, your partners, your parents and for nerds like me - keyboard, monitors, shoes, badminton gear and so on. I imagine myself without them and I know how important they are. I feel we should practice visualisation more often in life, it just helps\u0026hellip; Just like it helped me push my typing speed to 135 from 120 wpm.\nFelt like sharing this, so I did! Thank you for reading everyone, as always - I\u0026rsquo;m grateful for all the love you\u0026rsquo;ve given me in life.\n","permalink":"https://krshrimali.github.io/posts/2024/03/what-typing-taught-me-in-life/","summary":"\u003cp\u003eSo this happened today, I was attempting a typing speed test on \u003ca href=\"https://monkeytype.com\"\u003emonkeytype\u003c/a\u003e and quickly realised a few things that kind of related in my personal life. Thought I\u0026rsquo;ll share with y\u0026rsquo;all and I\u0026rsquo;m confident you all will relate!\u003c/p\u003e\n\u003cp\u003eFor context, I\u0026rsquo;ve been at around 120 wpm (avg) to 135 wpm (max) in a 30 seconds speed test for quite some time now (maybe more than a year at least). And very honestly, I\u0026rsquo;m happy about it, I don\u0026rsquo;t wwant to grow my typing speed further - in fact I never planned it before as well. I used to just type as fast as my fingers would allow, and turns out it was good enough for me to do my work. Right now, my goal is more towards sustaining my wrists and fingers along with work, as it definitely pains a lot if your positioning is not right.\u003c/p\u003e","title":"What typing taught me in life?"},{"content":"Hi everyone! Been a long time, thought I should talk about an ongoing project I\u0026rsquo;m working on.\nIntroduction - What are we trying to do here? Lemme start with some context and disclaimer first:\nThis was a part of an interview process in one of the amazing startups, and I wanted to extend it to an end-to-end project (kinda out of scope from the requirement of that interview process). I won\u0026rsquo;t be naming the startup here to help keep them anonymous and candid for their future candidates.\nAlright, so here it begins. Imagine having a documentation website, and you would like to finetune a ChatBot Model to help answer your questions. That\u0026rsquo;s it, that\u0026rsquo;s the problem. Let\u0026rsquo;s break it down on why this is an interesting problem to solve:\nFor learning?\nIt involves a lot of data preprocessing and cleaning. You\u0026rsquo;ll also have to automate the data scraping and cleaning process. It\u0026rsquo;s probably safe to assume that generally all documentations are hosted somewhere on GitHub or any other platform, but to make it more complex - let\u0026rsquo;s not use that assumption. We\u0026rsquo;ll have to scrape the data from the website, and clean it to make it ready for the model to consume. Automating this process and by just using a single documentation link is a good challenge to solve. A ChatBot Model takes an input of prompt and answer (kind of an instruction dataset format) and we\u0026rsquo;ll have to convert the documentation into this format. This is not as straight forward as it sounds, specially when you want to have minimal human intervention. More on this later. Finetuning an LLM Model: Always good to learn how to finetune an LLM model, because training it from scratch is never going to be feasible for all of us (this includes the constraint on lack of resources, motivation and time, oh and also need). The choice we make for which model to use is going to teach us a lot. Just in the exploring phase, we\u0026rsquo;ll be going through multiple models that exist and will try setting up priorities on the \u0026ldquo;What works for us\u0026rdquo; question. For the product?\nVery honestly, I don\u0026rsquo;t think there is a huge product goal here. It\u0026rsquo;s more of a learning project than a product project. But, if I were to think of a product goal, it would be to have a ChatBot Model that can answer questions from the documentation website. This can be used for customer support, or for internal documentation search, or for any other use case where you think a ChatBot can be helpful. In addition to this, I do think that this can be extended to something that is not just a documentation but a lot of text on websites.\nFor fun?\nThat\u0026rsquo;s why I\u0026rsquo;m using Rust here. I know this question is going to come up quite a lot, so lemme answer this first.\nWHY RUST?\nI honestly would love to see the performance of Rust in the data preprocessing and cleaning phase. It would be nice to see where does the data engineering ecosystem stand w.r.t Rust. We\u0026rsquo;ve all (probably?) used beautifulsoup in Python to scrap data from websites, and data cleaning/preprocessing libraries, but I\u0026rsquo;m excited to see where does Rust stand in this. I\u0026rsquo;ve been using Rust for some time now, and am pretty comfortable in exploring the ecosystem with this project. Writing a ChatBot interface would be fun in Rust. Finetuning a model in Rust\u0026hellip; hmm, I\u0026rsquo;m honestly not sure what\u0026rsquo;s the state of Rust bindings for PyTorch C++ API - so it\u0026rsquo;s something I\u0026rsquo;m very much looking forward to. I\u0026rsquo;m honestly nervous about this part, but let\u0026rsquo;s see. Using a single language for the whole end to end flow would make it easier for me to automate everything including testing and workflows. As always, I just want to point out that language you use isn\u0026rsquo;t the topmost criteria of any project IMO. First should be, whether it got delivered to the users and if they are happy with it. So, yes, please feel free to port it to some other language of your choice and try out the ecosystem for yourself to learn and grow your confidence :)\nPlan I\u0026rsquo;ve been live streaming my work on this project on my YouTube channel here: https://youtube.com/c/kushashwaraviShrimali. Please feel free to check it out if you\u0026rsquo;re interested.\nPick up a sample documentation website that we can use for this project. I\u0026rsquo;m picking up PyTorch docs website, it\u0026rsquo;s well maintained, and the format is tricky enough to automate things for other websites in the future. Input to this flow is going to be a simple documentation website link (of the index page). a. We\u0026rsquo;ll have to fetch all the hyperlinks that are \u0026ldquo;internal\u0026rdquo; to the documentation website, and then fetch the content of those hyperlinks. b. We\u0026rsquo;ll have to skip fetching content for \u0026ldquo;external\u0026rdquo; hyperlinks for now to avoid a lot of unrelated data (not really unrelated, but we can enable it easily if someone wants it). c. Output of this step should be: we have all the data in the text format from HTML webpages. Convert this data into the format of ChatBot model. Finetune the model by passing that data into a model of our choice. Create a ChatBot interface to deploy the model (more like an API end-point for now). Deploy the model to the ChatBot interface, or update the API end-point. The very tricky part is, Step 3 here. Ideally, a ChatBot model takes input of the following format:\nprompt: \u0026#34;What is the function of `torch.nn.functional.relu`?\u0026#34; answer: \u0026#34;Applies the rectified linear unit function element-wise.\u0026#34; context: \u0026lt;optional\u0026gt; ... // other properties The names (of the keys) can differ, but if you look at the format, it\u0026rsquo;s more like an instruction dataset. We\u0026rsquo;ll have to convert the documentation into this format.\nQuestion: How would you convert a given \u0026ldquo;text\u0026rdquo; into the format above?\nLet\u0026rsquo;s just say, your task is to convert \u0026ldquo;this\u0026rdquo; blog that you\u0026rsquo;re reading right now (thank you for being here btw ‚ù§Ô∏è) into a prompt-answer format (can leave context for now for simplicity).\nI\u0026rsquo;m going to come back with another blog to discuss my solution, but for now, leaving this blog at this to let y\u0026rsquo;all ponder over this and come up with some interesting approaches.\nYouTube! Sorry for the shameless plug here, I mean not really shameless since it\u0026rsquo;s still my blog haha, but\u0026hellip; I don\u0026rsquo;t want to miss out on telling you that I\u0026rsquo;ve been live streaming my work on this project on my YouTube channel here: https://youtube.com/c/kushashwaraviShrimali. By the time I\u0026rsquo;m writing this, there have been 2 videos out:\nLet\u0026rsquo;s finetune a model for a ChatBot Fetching dataset for finetuning a model for a ChatBot There are more videos coming, and I hope that they help you out with honest development views. We all fail, none of this happens in 5 minutes, 10 minutes and so on. I wanted to be candid with whatever I develop, and I hope that if nothing - it at least helps y\u0026rsquo;all with some motivation to keep going on your projects.\nI am grateful to you all for the love and support you\u0026rsquo;ve shown me, and I hope that I can give back to the community in some way or the other. I\u0026rsquo;m always open to feedback, and I hope that I can help you out with your projects in some way or the other. Feel free to reach out to me on Twitter.\nDiscord! Yep, we also have an active discord channel, so please feel free to join and chat with all of us there. Link: https://discord.gg/nh2KuAX3V8.\n","permalink":"https://krshrimali.github.io/posts/2024/02/building-a-chatbot-from-your-documentation-website-docsgpt/","summary":"\u003cp\u003eHi everyone! Been a long time, thought I should talk about an ongoing project I\u0026rsquo;m working on.\u003c/p\u003e\n\u003ch2 id=\"introduction---what-are-we-trying-to-do-here\"\u003eIntroduction - What are we trying to do here?\u003c/h2\u003e\n\u003cp\u003eLemme start with some context and disclaimer first:\u003c/p\u003e\n\u003cblockquote\u003e\n\u003cp\u003eThis was a part of an interview process in one of the amazing startups, and I wanted to extend it to an end-to-end project (kinda out of scope from the requirement of that interview process). I won\u0026rsquo;t be naming the startup here to help keep them anonymous and candid for their future candidates.\u003c/p\u003e","title":"Building a ChatBot from your Documentation Website | DocsGPT"},{"content":"Alright everyone, we are back. Just FYI, we\u0026rsquo;ve had a blog on introduction to DocsGPT before: https://krshrimali.github.io/posts/2024/02/building-a-chatbot-from-your-documentation-website-docsgpt/. This is a follow up blog where we\u0026rsquo;ll discuss data scraping and preprocessing to be able to finetune our model for ChatBot use-case.\nQuick recap?\nInput is going to be a single link to documentation page (index page). Need to fetch data for \u0026ldquo;all the internal pages\u0026rdquo;. Preprocess (and/or clean) and transform the data to be able to finetune the model. Finetune the model and use it for ChatBot use-case. In this blog, we\u0026rsquo;ll be covering the first two above, and the rest will be covered in the next blog(s).\nData Scraping The problem is simple, let\u0026rsquo;s just define our function and it\u0026rsquo;s return type. It kinda sets the tone.\npub fn fetch_data(input_link: \u0026amp;str) -\u0026gt; Result\u0026lt;Vec\u0026lt;String\u0026gt;, Box\u0026lt;dyn std::error::Error\u0026gt;\u0026gt; { // input_link is the link to the index page of the documentation concerned } Alright, so as it might be clear already, we\u0026rsquo;ll be passing link to the index page of the documentation. For our use-case of PyTorch documentation, we\u0026rsquo;ll use: https://pytorch.org/docs/stable/index.html. The return type is Vec\u0026lt;String\u0026gt; where each string is the content of the internal page.\nThis is an example function though, we\u0026rsquo;ll be moving to another format as we go ahead. But for now, let\u0026rsquo;s just keep it simple. There are 2 problems we\u0026rsquo;re trying to solve here:\nFetch \u0026ldquo;all the internal pages\u0026rdquo; through the index page. This includes going through all the hyperlinks in each page starting from the index page, and fetching the content of each page. We\u0026rsquo;ll include any external page (that is not internal to the documentation, for simplicity). Getting content of the page To be able to solve these problems, we\u0026rsquo;ll use soup crate in Rust: https://docs.rs/soup/latest/soup/. For those who are used to using beautifulsoup in Python, it\u0026rsquo;s pretty similar.\nLet\u0026rsquo;s start with the first problem. We\u0026rsquo;ll define a function extract_all_hyperlinks which will take the index page link and return all the internal links. (the function won\u0026rsquo;t be recursive)\nNote that, to be able to do this, we\u0026rsquo;ll just make sure to get the content of the index page first.\npub fn fetch_raw_html(main_html_link: String) -\u0026gt; Result\u0026lt;String, std::io::Error\u0026gt; { let output = reqwest::blocking::get(main_html_link).expect(\u0026#34;Failed to fetch the URL\u0026#34;); let output_text = output.text().expect(\u0026#34;Failed to read the response text\u0026#34;); Ok(output_text.to_string()) } fn main() { let main_html_link: String = \u0026#34;https://pytorch.org/docs/stable/index.html\u0026#34;.to_string(); let raw_data: Result\u0026lt;String, std::io::Error\u0026gt; = fetch_raw_html(main_html_link.clone()); env_logger::init(); let mut all_links: Vec\u0026lt;String\u0026gt; = Vec::new(); let mut all_text: Vec\u0026lt;String\u0026gt; = Vec::new(); let if_succeeded = extract_all_hyperlinks(raw_data, \u0026amp;mut all_links, \u0026amp;mut all_text); } If you\u0026rsquo;re wondering, why fetch raw HTML separately and then extract hyperlinks from the content? Rust is naturally a functional language, and following the practice to keep a single function limited to one single action, we\u0026rsquo;re doing this. Will help testing efforts in the future.\nAlright, so - so far, we got the raw HTML content (used reqwest crate for fetching the content), and then we\u0026rsquo;re passing it to extract_all_hyperlinks function. The function will look something like this:\npub fn extract_all_hyperlinks( raw_data: Result\u0026lt;String, std::io::Error\u0026gt;, all_links: \u0026amp;mut Vec\u0026lt;String\u0026gt;, all_text: \u0026amp;mut Vec\u0026lt;String\u0026gt;, ) -\u0026gt; Result\u0026lt;(), std::io::Error\u0026gt; { match raw_data { Ok(content) =\u0026gt; { let soup = Soup::new(\u0026amp;content); let text = soup.text(); all_text.push(text); let all_tags_a_href = soup.tag(\u0026#34;a\u0026#34;).find_all(); all_tags_a_href.enumerate().for_each(|(_, tag)| { let href = tag.get(\u0026#34;href\u0026#34;); match href { Some(href) =\u0026gt; { // NOTE: Can you think of a better way to differentiate b/w internal and external links? if href.starts_with(\u0026#34;http\u0026#34;) || href.starts_with(\u0026#39;_\u0026#39;) || href.starts_with(\u0026#39;#\u0026#39;) || href.starts_with(\u0026#39;.\u0026#39;) { log::info!(\u0026#34;Skipping: {:?}\u0026#34;, href); return; } all_links.push(href); } None =\u0026gt; log::warn!(\u0026#34;No href attribute found\u0026#34;), } }); Ok(()) } Err(e) =\u0026gt; { log::error!(\u0026#34;Error reading file: {:?}\u0026#34;, e,); Err(e) } } } What are we doing here?\nWe got the content from fetch_raw_html, if valid we\u0026rsquo;ll create a Soup object from it and get the text. Once we have the text, we\u0026rsquo;ll get all the a tags and their href attributes. We\u0026rsquo;ll skip the external links (for simplicity) and add the internal links to all_links vector. I\u0026rsquo;m open to ideas on differentiating b/w external and internal links. For now, anything that starts with http, _, #, or . is considered external (please note that https automatically comes in this because it\u0026rsquo;s a superset of http). We\u0026rsquo;ll also add the text to all_text vector. It\u0026rsquo;s comparatively easier to test these functions now, thanks to the original idea of keeping their jobs separate.\nAlright, now we have the hyperlinks, but we haven\u0026rsquo;t fetched all the content of those hyperlinks and their hyperlinks and their hyperlinks\u0026hellip; (nested hyperlinks xD).\nIt\u0026rsquo;s easy if you think about it:\nIterate through all the hyperlinks, for each link - get the data and store it. ^^ Hmm, sounds very easy, duh? Well, we are missing one part. What if the link is repeated? Which is quite possible in any documentation. Let\u0026rsquo;s revise our list above:\nIterate through all the hyperlinks, for each link - get the data and store it. If the link is already in a HashSet, skip it. Otherwise, add it to the HashSet and store the data. Here\u0026rsquo;s where we\u0026rsquo;ll just re-use our functions defined above:\nfn main() { // ... let mut visited_set: HashSet\u0026lt;String\u0026gt; = HashSet::new(); for link in all_links.iter() { if visited_set.contains(link) { log::info!(\u0026#34;Skipping link because it\u0026#39;s alr visited: {:?}\u0026#34;, link); continue; } // NOTE: Guess what we are doing here? let modified_link = Path::new(\u0026amp;main_html_link).parent().unwrap().join(link); let content = fetch_raw_html(modified_link.display().to_string()); let success = extract_all_hyperlinks(content, \u0026amp;mut nested_links, \u0026amp;mut all_text); visited_set.insert(link.clone()); if success.is_ok() { log::info!(\u0026#34;Success for link: {:?}\u0026#34;, link); log::debug!(\u0026#34;Nested links: {:?}\u0026#34;, nested_links); } else { log::info!(\u0026#34;Failed for link: {:?}\u0026#34;, link); } } // ... } If you saw the code above, there is a note which says:\n// NOTE: Guess what we are doing here?\nAlright, so for any internal link, which would look like this: /docs/stable/tensors.html, we\u0026rsquo;ll just join it with the parent link and fetch the content. This is a very simple way to do it, and it\u0026rsquo;s not the best way to do it. But for now, it\u0026rsquo;s good enough. Do note that, the parent link comes from the initial link the user passes to this flow, which was: https://pytorch.org/docs/stable/index.html in this case. So we\u0026rsquo;ll just replace /index.html with /docs/stable/tensors.html and fetch the content.\nOnce this is done, the next task would be to understand what data we got - how we can transform it - what kind of cleaning methods are needed etc. That\u0026rsquo;s going to be a good exercise for the next blog. For now, I think, this is a good start! Right? :)\nYouTube! As always, a shameless plug, I do have a YouTube channel guys! Please consider referring to the channel if there\u0026rsquo;s anything that interests you there. Here\u0026rsquo;s the link: https://www.youtube.com/c/kushashwaraviShrimali.\nDiscord! Oh, and yes, we do have a discord channel. Please consider joining the channel for any discussions, or if you need any help. Here\u0026rsquo;s the link: https://discord.gg/nh2KuAX3V8.\nAlright then ü§ù, I\u0026rsquo;ll see you soon in the next blog. Thank you all, bye! ‚ù§Ô∏è\n","permalink":"https://krshrimali.github.io/posts/2024/02/data-scrapping-for-chatbot-model-in-rust-docsgpt-part-2/","summary":"\u003cp\u003eAlright everyone, we are back. Just FYI, we\u0026rsquo;ve had a blog on introduction to DocsGPT before: \u003ca href=\"https://krshrimali.github.io/posts/2024/02/building-a-chatbot-from-your-documentation-website-docsgpt/\"\u003ehttps://krshrimali.github.io/posts/2024/02/building-a-chatbot-from-your-documentation-website-docsgpt/\u003c/a\u003e. This is a follow up blog where we\u0026rsquo;ll discuss data scraping and preprocessing to be able to finetune our model for ChatBot use-case.\u003c/p\u003e\n\u003cp\u003eQuick recap?\u003c/p\u003e\n\u003col\u003e\n\u003cli\u003eInput is going to be a \u003cem\u003esingle link to documentation page (index page)\u003c/em\u003e.\u003c/li\u003e\n\u003cli\u003eNeed to fetch data for \u0026ldquo;all the internal pages\u0026rdquo;.\u003c/li\u003e\n\u003cli\u003ePreprocess (and/or clean) and transform the data to be able to finetune the model.\u003c/li\u003e\n\u003cli\u003eFinetune the model and use it for ChatBot use-case.\u003c/li\u003e\n\u003c/ol\u003e\n\u003cp\u003eIn this blog, we\u0026rsquo;ll be covering the first two above, and the rest will be covered in the next blog(s).\u003c/p\u003e","title":"Data Scrapping for ChatBot Model in Rust | DocsGPT | Part-2"},{"content":"Hi everyone! Happy to be back, in case y\u0026rsquo;all have been wondering - where have you been Kush? Well, a lot of things have been happening on my YouTube channel and Twitch Channel - buffetcodes. But in any case, I wanted to write something out - so here it is.\nDisclaimer: this is more on a personal level rather than on an educational level, I\u0026rsquo;m sorry to disappoint if you were expecting a detailed tech blog today. I\u0026rsquo;ll come up with something soon. :)\nAlright, so\u0026hellip; this thought came into my mind, probably it\u0026rsquo;s just me but how easily do we forget good things, people and moments? You\u0026rsquo;ll see people talking about their down moments, but why have we stopped appreciating and recognising the ups we have had in the past? While reading this, let\u0026rsquo;s all think about the good moments you had in the past, and think about the people who contributed to your journey (irrespective of their presence in your life right now), maybe close your eyes - speak out loud - or write it down - do whatever that suits you, but let\u0026rsquo;s do it together.\nMake sure to include very small wins as well when you think about the moments! :) A win is a win. I\u0026rsquo;ll write things down as I think, please note that I chose to not name people/companies to keep their privacy and my peace üòâ\nI interned at NTU Singapore, immensely grateful to the friends who contributed significantly in the journey. Also thankful to my mentors, colleagues at NTU and professor as well. My elder brother and parents supported significantly in this internship, as it wasn\u0026rsquo;t fully funded. I remember my brother and my parents telling me, \u0026ldquo;You go, we\u0026rsquo;ll take care of the money part\u0026rdquo;. Going to Singapore changed the course of my life\u0026hellip; Because of the time I spent in Singapore, I realised I absolutely love low level programming, I didn\u0026rsquo;t know it all though (I still don\u0026rsquo;t :)) but it fascinated me. I started writing blogs for PyTorch C++ API. Doing that while in the institute, it kept me focused on my goal. I decided not to sit for on-campus placements, there are many reasons to it - probably good for another blog? Many developers helped me in the journey while writing blogs, they weren\u0026rsquo;t just a \u0026ldquo;technical content\u0026rdquo; writing gig, each blog took me a month to write and reproduce results - each blog was a project to me. I travelled to the US, many may not know this but when my VISA got rejected (first try) - I talked to a few professors of my institute, and talking to them comforted me. I got the VISA eventually, but I was never nervous - even if I would have \u0026ldquo;failed\u0026rdquo; to not get a VISA to the US, I would have happily worked for any company who would give me a job at that time, and would have tried creating an impact. I travelled back to India because of COVID pandemic, and worked for a few startups. No matter how the experience was, I had my learnings - and have fortunately not committed the same mistakes again. Started my YouTube channel after hitting the rock bottom of my emotional state. Streaming my projects, talking about things openly, helped me a lot. Continued contributing to Open Source, Got laid off once (no shame in admitting this) - ran 7 kms after that üèÉ- and got another job within a month - very grateful to the God and family. Shifted to Bangalore, worked on a lot of projects, streamed a lot, did a lot of running and started playing badminton üè∏. Okay, now once all this is done, lemme ask you a simple question - How many times did you express your gratitude to the people who helped in your journey? Do you still value them like the way you should have\u0026hellip;? Not all, maybe, I\u0026rsquo;ll leave that decision to you, but think about it.\nI\u0026rsquo;ll be back with another blog after this, meanwhile\u0026hellip; let\u0026rsquo;s all take some time - and express our gratitude to the people who were in the journey and who deserve knowing that you\u0026rsquo;re doing GREAT! ‚ù§Ô∏è\nUntil next time,\nNamaste üôè Kushashwa Ravi Shrimali\n","permalink":"https://krshrimali.github.io/posts/2024/01/bring-back-the-old-times-celebrate-your-wins/","summary":"\u003cp\u003eHi everyone! Happy to be back, in case y\u0026rsquo;all have been wondering - where have you been Kush? Well, a lot of things have been happening on my \u003ca href=\"https://youtube.com/c/kushashwaraviShrimali\"\u003eYouTube channel\u003c/a\u003e and \u003ca href=\"https://twitch.tv/buffetcodes\"\u003eTwitch Channel - buffetcodes\u003c/a\u003e. But in any case, I wanted to write something out - so here it is.\u003c/p\u003e\n\u003cp\u003eDisclaimer: this is more on a personal level rather than on an educational level, I\u0026rsquo;m sorry to disappoint if you were expecting a detailed tech blog today. I\u0026rsquo;ll come up with something soon. :)\u003c/p\u003e","title":"Bring back the old times - celebrate your wins!"},{"content":"Okay, I think I should just accept this, while at work, it will be very hard for me to go and actually code something. And it makes sense, honestly at this stage - I feel like giving 50% of the time to reading + thinking, and 20% to implementing + 30% on iterating and feedback. So you\u0026rsquo;ll see more links about reading etc. over here.\nDesign a basic search engine (Google or Bing) | System Design Interview Prep Design a Payment System - System Design Interview That would be it, to be honest - not a lot. I\u0026rsquo;ve started reading through fasterthanlime\u0026rsquo;s videos on Silly Fast Fresh Deploys, I\u0026rsquo;ll share progress on it soon once I\u0026rsquo;ve some learnings.\nTODOs:\nExplore HVM: https://github.com/HigherOrderCO/HVM, looks interesting at least. Read through: Specifying and Verifying Higher-order Rust Iterators. Record a video for YouTube! Stabilizing async fn in traits in 2023 Alright, thank you all for reading. :) See y\u0026rsquo;all tomorrow!\n","permalink":"https://krshrimali.github.io/posts/2023/05/daily-update-3rd-may-2023/","summary":"\u003cp\u003eOkay, I think I should just accept this, while at work, it will be very hard for me to go and actually code something. And it makes sense, honestly at this stage - I feel like giving 50% of the time to reading + thinking, and 20% to implementing + 30% on iterating and feedback. So you\u0026rsquo;ll see more links about reading etc. over here.\u003c/p\u003e\n\u003col\u003e\n\u003cli\u003e\u003ca href=\"https://www.youtube.com/watch?v=0LTXCcVRQi0\u0026amp;list=LL\u0026amp;index=7\"\u003eDesign a basic search engine (Google or Bing) | System Design Interview Prep\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"https://www.youtube.com/watch?v=olfaBgJrUBI\u0026amp;list=LL\u0026amp;index=6\"\u003eDesign a Payment System - System Design Interview\u003c/a\u003e\u003c/li\u003e\n\u003c/ol\u003e\n\u003cp\u003eThat would be it, to be honest - not a lot. I\u0026rsquo;ve started reading through fasterthanlime\u0026rsquo;s videos on Silly Fast Fresh Deploys, I\u0026rsquo;ll share progress on it soon once I\u0026rsquo;ve some learnings.\u003c/p\u003e","title":"Daily Update: 3rd May 2023"},{"content":"Today was more like a reading/learning day. Here are the things I\u0026rsquo;ve been watching:\nWhich Database Model to choose?: Why this? Going to be useful for the project I\u0026rsquo;ve been working on. Thinking on directions of storing in-memory, using key-value model but let\u0026rsquo;s see. I made every sentry page 300 ms faster (intermediate) anthony explains #540 Why this? Always enjoy watching Anthony. Came on my YT recommendation, thought I\u0026rsquo;ll watch. Looks like a CDN config fix, also learnt a few things on watching the type of network calls (cached or not) from this video. How does async rust work Still reading, not done yet. Will update my learnings when done. Update NeoVim Config to use ruff-lsp instead of pyright for most except hover Ruff has been there for long, boasts a lot of perf improvements as a linter. Good to see Ruff LSP growing. NeoVim config repository: https://github.com/krshrimali/nvim It\u0026rsquo;s of course going to take a lot of efforts to catch up to pyright in terms of capabilities, but I\u0026rsquo;m very positive. I love to see code actions finally in my neovim + python code. üéâ It\u0026rsquo;s written in Rust, for those who don\u0026rsquo;t know üòâ TODOs:\nExplore HVM: https://github.com/HigherOrderCO/HVM, looks interesting at least. Read through: Specifying and Verifying Higher-order Rust Iterators. Record a video for YouTube! Alright, thank you all for reading. :) See y\u0026rsquo;all tomorrow!\n","permalink":"https://krshrimali.github.io/posts/2023/05/daily-update-2nd-may-2023/","summary":"\u003cp\u003eToday was more like a reading/learning day. Here are the things I\u0026rsquo;ve been watching:\u003c/p\u003e\n\u003col\u003e\n\u003cli\u003e\u003ca href=\"https://www.youtube.com/watch?v=9mdadNspP_M\"\u003eWhich Database Model to choose?\u003c/a\u003e:\n\u003cul\u003e\n\u003cli\u003eWhy this?\n\u003cul\u003e\n\u003cli\u003eGoing to be useful for the \u003ca href=\"https://github.com/krshrimali/keystroke-store-rs\"\u003eproject\u003c/a\u003e I\u0026rsquo;ve been working on. Thinking on directions of storing in-memory, using key-value model but let\u0026rsquo;s see.\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"https://www.youtube.com/watch?v=FY7EQwT7QKU\"\u003eI made every sentry page 300 ms faster (intermediate) anthony explains \u003ccode\u003e#540\u003c/code\u003e\u003c/a\u003e\n\u003cul\u003e\n\u003cli\u003eWhy this?\n\u003cul\u003e\n\u003cli\u003eAlways enjoy watching Anthony. Came on my YT recommendation, thought I\u0026rsquo;ll watch. Looks like a CDN config fix, also learnt a few things on watching the type of network calls (cached or not) from this video.\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"https://bertptrs.nl/2023/04/27/how-does-async-rust-work.html\"\u003eHow does async rust work\u003c/a\u003e\n\u003cul\u003e\n\u003cli\u003eStill reading, not done yet. Will update my learnings when done.\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003eUpdate NeoVim Config to use ruff-lsp instead of pyright for most except hover\n\u003cul\u003e\n\u003cli\u003eRuff has been there for long, boasts a lot of perf improvements as a linter. Good to see Ruff LSP growing.\u003c/li\u003e\n\u003cli\u003eNeoVim config repository: \u003ca href=\"https://github.com/krshrimali/nvim\"\u003ehttps://github.com/krshrimali/nvim\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003eIt\u0026rsquo;s of course going to take a lot of efforts to catch up to pyright in terms of capabilities, but I\u0026rsquo;m very positive.\u003c/li\u003e\n\u003cli\u003eI love to see code actions finally in my neovim + python code. üéâ\u003c/li\u003e\n\u003cli\u003eIt\u0026rsquo;s written in Rust, for those who don\u0026rsquo;t know üòâ\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003c/ol\u003e\n\u003cp\u003eTODOs:\u003c/p\u003e","title":"Daily Update: 2nd May 2023"},{"content":"Interesting day, lots of design stuff for the project we\u0026rsquo;ve been working on.\nDesigning flow for the word combinator service This is actually tricky, as we are figuring out how we\u0026rsquo;ll figure out the keystrokes into words/phrases (I\u0026rsquo;d like to call them entities at one point of time). The algorithm is close, and I\u0026rsquo;m particularly hopeful about it: It considers some edge cases of mouse events, sitting idle, and gives user the control what they would like to choose. More on this very soon! Boring nerdy stuff: Exploring fzf I wanted to have fzf previews height changed to 100%, finally changed my zsh config for that. It looks much better now. Then figured out that I\u0026rsquo;m using outdated formula for fzf, and I\u0026rsquo;ll end up going through fzf release notes to see what has changed over the last few releases. It\u0026rsquo;s usually a good practice I believe to go through the release notes, and be aware of the tools you use. Work day tomorrow, so doing some early readings. Of course can\u0026rsquo;t share details, but yep, going to be more occupied now. A lot of TODOs from yesterday day before yesterday are left ü•∫, I\u0026rsquo;ll move them to tomorrow, yay! Procrastination, let\u0026rsquo;s go üòÜüéâ\nTODOs:\nExplore HVM: https://github.com/HigherOrderCO/HVM, looks interesting at least. Read through: Specifying and Verifying Higher-order Rust Iterators. Read through: How does async rust work Record a video for YouTube! Alright, thank you all for reading. :) See y\u0026rsquo;all tomorrow!\n","permalink":"https://krshrimali.github.io/posts/2023/05/daily-update-1st-may-2023/","summary":"\u003cp\u003eInteresting day, lots of design stuff for the \u003ca href=\"https://github.com/krshrimali/keystroke-store-rs\"\u003eproject we\u0026rsquo;ve been working on\u003c/a\u003e.\u003c/p\u003e\n\u003col\u003e\n\u003cli\u003eDesigning flow for the word combinator service\n\u003cul\u003e\n\u003cli\u003eThis is actually tricky, as we are figuring out how we\u0026rsquo;ll figure out the keystrokes into words/phrases (I\u0026rsquo;d like to call them \u003cem\u003eentities\u003c/em\u003e at one point of time).\u003c/li\u003e\n\u003cli\u003eThe algorithm is close, and I\u0026rsquo;m particularly hopeful about it:\n\u003cul\u003e\n\u003cli\u003eIt considers some edge cases of mouse events, sitting idle, and gives user the control what they would like to choose.\u003c/li\u003e\n\u003cli\u003eMore on this very soon!\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003eBoring nerdy stuff: Exploring fzf\n\u003cul\u003e\n\u003cli\u003eI wanted to have fzf previews height changed to 100%, finally changed my zsh config for that. It looks much better now.\u003c/li\u003e\n\u003cli\u003eThen figured out that I\u0026rsquo;m using outdated formula for fzf, and I\u0026rsquo;ll end up going through fzf release notes to see what has changed over the last few releases.\n\u003cul\u003e\n\u003cli\u003eIt\u0026rsquo;s usually a good practice I believe to go through the release notes, and be aware of the tools you use.\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003eWork day tomorrow, so doing some early readings.\n\u003cul\u003e\n\u003cli\u003eOf course can\u0026rsquo;t share details, but yep, going to be more occupied now.\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003c/ol\u003e\n\u003cp\u003eA lot of TODOs from \u003cdel\u003eyesterday\u003c/del\u003e day before yesterday are left ü•∫, I\u0026rsquo;ll move them to tomorrow, yay! Procrastination, let\u0026rsquo;s go üòÜüéâ\u003c/p\u003e","title":"Daily Update: 1st May 2023"},{"content":"Lovely day, had my friend imsrbh come over and we were talking about the project I wanted to do. He had some ideas and of course some experience with Kafka, so he helped me setup Kafka and docker on my machine, and that\u0026rsquo;s how it all started.\nSetting up Kafka and Docker on my machine: Refer to this blog for Kafka setup. We verified by running a sample producer-consumer app in Python and it worked. Starting to implement the core logic of getting key strokes in Rust I\u0026rsquo;m using rdev library for this, it was decent. Was wondering how I can convert the keystrokes to strings, apparently it was a Rust Enum. Fortunately, they had a feature serialize which would help me do just that. Now it\u0026rsquo;s quite smooth. Implementing Kafka Producer-Consumer in Rust Kafka has a client in Rust, cargo add kafka will help. Wrote Kafka Producer and Consumer referring to their official documentation. It works flawlessly. We were sending the keys as \u0026amp;[u8] and had to convert these to strings back at consumer (need to proper error checking there) Producer repo: https://github.com/krshrimali/keystroke-store-rs Consumer repo: https://github.com/krshrimali/keystroke-consumer-rs Quick learning on why we didn\u0026rsquo;t go for Redis (mostly because we wanted to learn Kafka in Rust lol, can try Redis one day but I like reliability of Kafka): https://stackoverflow.com/a/37993809 A lot of TODOs from yesterday are left, I\u0026rsquo;ll move them to tomorrow, yay! Procrastination, let\u0026rsquo;s go üòÜüéâ\nTODOs:\nExplore HVM: https://github.com/HigherOrderCO/HVM, looks interesting at least. Read through: Specifying and Verifying Higher-order Rust Iterators. Read through: How does async rust work Record a video for YouTube! Alright, thank you all for reading. :) See y\u0026rsquo;all tomorrow!\n","permalink":"https://krshrimali.github.io/posts/2023/04/daily-update-30th-april-2023/","summary":"\u003cp\u003eLovely day, had my friend \u003ca href=\"https://github.com/imsrbh\"\u003eimsrbh\u003c/a\u003e come over and we were talking about the project I wanted to do. He had some ideas and of course some experience with Kafka, so he helped me setup Kafka and docker on my machine, and that\u0026rsquo;s how it all started.\u003c/p\u003e\n\u003col\u003e\n\u003cli\u003eSetting up Kafka and Docker on my machine:\n\u003cul\u003e\n\u003cli\u003eRefer to \u003ca href=\"https://medium.com/@fengliplatform/kafka-broker-setup-using-docker-image-33f7a8081a07#:~:text=1%20Setup%20Kafka%20cluster%20on%20Windows%20laptop%201,instance%20...%205%201.5%20Create%20Kafka%20topic%20\"\u003ethis blog\u003c/a\u003e for Kafka setup.\u003c/li\u003e\n\u003cli\u003eWe verified by running a sample producer-consumer app in Python and it worked.\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"https://github.com/krshrimali/keystroke-store-rs\"\u003eStarting to implement the core logic of getting key strokes in Rust\u003c/a\u003e\n\u003cul\u003e\n\u003cli\u003eI\u0026rsquo;m using \u003ccode\u003erdev\u003c/code\u003e library for this, it was decent.\u003c/li\u003e\n\u003cli\u003eWas wondering how I can convert the keystrokes to strings, apparently it was a Rust Enum.\u003c/li\u003e\n\u003cli\u003eFortunately, they had a feature \u003ccode\u003eserialize\u003c/code\u003e which would help me do just that. Now it\u0026rsquo;s quite smooth.\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003eImplementing Kafka Producer-Consumer in Rust\n\u003cul\u003e\n\u003cli\u003eKafka has a client in Rust, \u003ccode\u003ecargo add kafka\u003c/code\u003e will help.\u003c/li\u003e\n\u003cli\u003eWrote Kafka Producer and Consumer referring to their official documentation.\u003c/li\u003e\n\u003cli\u003eIt works flawlessly.\u003c/li\u003e\n\u003cli\u003eWe were sending the keys as \u003ccode\u003e\u0026amp;[u8]\u003c/code\u003e and had to convert these to strings back at consumer (need to proper error checking there)\u003c/li\u003e\n\u003cli\u003eProducer repo: \u003ca href=\"https://github.com/krshrimali/keystroke-store-rs\"\u003ehttps://github.com/krshrimali/keystroke-store-rs\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003eConsumer repo: \u003ca href=\"https://github.com/krshrimali/keystroke-consumer-rs\"\u003ehttps://github.com/krshrimali/keystroke-consumer-rs\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003eQuick learning on why we didn\u0026rsquo;t go for Redis (mostly because we wanted to learn Kafka in Rust lol, can try Redis one day but I like reliability of Kafka): \u003ca href=\"https://stackoverflow.com/a/37993809\"\u003ehttps://stackoverflow.com/a/37993809\u003c/a\u003e\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003c/ol\u003e\n\u003cp\u003eA lot of TODOs from yesterday are left, I\u0026rsquo;ll move them to tomorrow, yay! Procrastination, let\u0026rsquo;s go üòÜüéâ\u003c/p\u003e","title":"Daily Update: 30th April 2023"},{"content":"Alright, we are back. It\u0026rsquo;s Saturday, so I spent a lot of time cleaning up my flat and my setup. Bengaluru is quite a dusty city, so yep, gotta clean regularly. Anyways, in terms of learning, my goals of today were to read some source code and papers/official documentation. It\u0026rsquo;s been some time since I didn\u0026rsquo;t do any Rust, so today I\u0026rsquo;ll just touch upon that as well.\nRust - building a UI to plot a sensor in real time Why this? I\u0026rsquo;ve used Iced library quite a bit so far, so wanted to understand how other UI libraries look like. I\u0026rsquo;m not going for the \u0026ldquo;famous\u0026rdquo; libraries here, just wanted to experience any other library. Plus, this video seemed to be decent and technically focused enough on the implementation. Setting up PostgreSQL server on my system: Why this? For me to start working on some projects of my own, it\u0026rsquo;s important to setup stage for them. Notes: I used this blog by digital ocean as reference. Micro Project: Store all the key presses in a server Notes: Since I type a lot, everyone does, the DB will have to be good enough. I wanted to see how I can enable indexing and searching, this will help give me an idea. No UI stuff right now. Maybe one day. NeoVim Config: I was randomly trying a keymap, and I just remembered I had \u0026lt;leader\u0026gt;nf for SnipRun. And oh, wow - it finally made sense why it could be useful. Selecting text, code block, displaying output in the command window or as virtual text (different highlight for errors) and even in a vertical split terminal, amazing stuff! Something my nvim-autorunner was trying to do, cool stuff. Oh, and on another note: I watched a movie (after a long long time). Had to find time to relax a bit.\n(New section: TODOs for tomorrow)\nTODOs:\nExplore HVM: https://github.com/HigherOrderCO/HVM, looks interesting at least. Read through: Specifying and Verifying Higher-order Rust Iterators. Read through: How does async rust work Continue working on the micro project. Record a video for YouTube! Alright, thank you all for reading. :) See y\u0026rsquo;all tomorrow!\n","permalink":"https://krshrimali.github.io/posts/2023/04/daily-update-29th-april-2023/","summary":"\u003cp\u003eAlright, we are back. It\u0026rsquo;s Saturday, so I spent a lot of time cleaning up my flat and my setup. Bengaluru is quite a dusty city, so yep, gotta clean regularly. Anyways, in terms of learning, my goals of today were to read some source code and papers/official documentation. It\u0026rsquo;s been some time since I didn\u0026rsquo;t do any Rust, so today I\u0026rsquo;ll just touch upon that as well.\u003c/p\u003e\n\u003col\u003e\n\u003cli\u003e\u003ca href=\"https://www.youtube.com/watch?v=zUvHkkkrmIY\"\u003eRust - building a UI to plot a sensor in real time\u003c/a\u003e\n\u003cul\u003e\n\u003cli\u003eWhy this?\n\u003cul\u003e\n\u003cli\u003eI\u0026rsquo;ve used Iced library quite a bit so far, so wanted to understand how other UI libraries look like.\u003c/li\u003e\n\u003cli\u003eI\u0026rsquo;m not going for the \u0026ldquo;famous\u0026rdquo; libraries here, just wanted to experience any other library. Plus, this video seemed to be decent and technically focused enough on the implementation.\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003eSetting up PostgreSQL server on my system:\n\u003cul\u003e\n\u003cli\u003eWhy this?\n\u003cul\u003e\n\u003cli\u003eFor me to start working on some projects of my own, it\u0026rsquo;s important to setup stage for them.\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003eNotes:\n\u003cul\u003e\n\u003cli\u003eI used \u003ca href=\"https://www.digitalocean.com/community/tutorials/how-to-install-postgresql-on-ubuntu-20-04-quickstart\"\u003ethis blog by digital ocean\u003c/a\u003e as reference.\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003eMicro Project: Store all the key presses in a server\n\u003cul\u003e\n\u003cli\u003eNotes:\n\u003cul\u003e\n\u003cli\u003eSince I type a lot, everyone does, the DB will have to be good enough.\u003c/li\u003e\n\u003cli\u003eI wanted to see how I can enable indexing and searching, this will help give me an idea.\u003c/li\u003e\n\u003cli\u003eNo UI stuff right now. Maybe one day.\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003eNeoVim Config:\n\u003cul\u003e\n\u003cli\u003eI was randomly trying a keymap, and I just remembered I had \u003ccode\u003e\u0026lt;leader\u0026gt;nf\u003c/code\u003e for \u003ccode\u003eSnipRun\u003c/code\u003e. And oh, wow - it finally made sense why it could be useful.\u003c/li\u003e\n\u003cli\u003eSelecting text, code block, displaying output in the command window or as virtual text (different highlight for errors) and even in a vertical split terminal, amazing stuff!\u003c/li\u003e\n\u003cli\u003eSomething my \u003ca href=\"https://github.com/krshrimali/nvim-autorunner\"\u003envim-autorunner\u003c/a\u003e was trying to do, cool stuff.\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003c/ol\u003e\n\u003cp\u003eOh, and on another note: I watched a movie (after a long long time). Had to find time to relax a bit.\u003c/p\u003e","title":"Daily Update: 29th April 2023"},{"content":"Started my day with talking to my family, and then kicking off work after getting ready. (Note: these blogs are mostly around my personal learning updates, so I\u0026rsquo;ll mostly miss anything that I do at work as that\u0026rsquo;s confidential)\nWhat is OSI Model | Real World Examples Why this? The answer to: \u0026ldquo;How does the communication work from client to server\u0026rdquo; is very important in backend. I wanted to see if I missed anything from my graduate studies, but nothing much. This was more like a refresher. Notes: Each operating system has their own implementation of TCP protocol. Linux has it\u0026rsquo;s own for example, and it\u0026rsquo;s open sourced. Can take a look at TCP\u0026rsquo;s source code in Linux source code to understand how checksum correction works. Cloud load balancers: L4 Load Balancer (operates at TCP Level), L7 Load Balnacer (operates at Application Protocol Layer - HTTP/HTTPS) Postgres: Architectural Fundamentals Why this? Going through database design and architectural fundamentals is helpful. I plan to explore CockroachDB soon, but wanted to get some idea about the ones I\u0026rsquo;ve used so far (Postgres for example) Notes: Uses client/server model. PostgreSQL session consists following processes: Server: manages DB files, accepts connections to the DB from client apps, performs DB actions on behalf of clients. DB server process is called postgres Client (frontend): app that wants to perform DB operations. Client/server could be on diff hosts: communicate over TCP/IP n/w connection postgres can handle multiple connections from clients: Forks a new process for each connection The new process and client directly communicate No intervention of postgres server Supervisor server process is always running (daemon process), waiting for client connections Client and child server process can come and go PostgreSQL System Architecture Why this? Just wanted to explore another blog on the same topic to see if I missed anything. Tbh, nothing much. Notes: PostgreSQL: process-per-transaction model postgres server process: Managed by postmaster, central coordinating process Responsibilities: Initializing, terminating the server Handling connection requests from the new clients Recovery Run background processes Shared Memory: Reserved for DB caching and Transactional Log Caching Shared Disk Buffer Shared Tables Uses same set of tables to host multiple client data (TODO: how?) Backend Processes: Client interacts with backend processes (submits queries and receiving queries result) Multiple backend servers executing queries concurrently Each backend server: will handle only a single query at a time access data from main memory buffer pool (placed in shared memory) Thank you! :) See y\u0026rsquo;all in the next blog.\n","permalink":"https://krshrimali.github.io/posts/2023/04/daily-update-28th-april-2023/","summary":"\u003cp\u003eStarted my day with talking to my family, and then kicking off work after getting ready. (Note: these blogs are mostly around my personal learning updates, so I\u0026rsquo;ll mostly miss anything that I do at work as that\u0026rsquo;s confidential)\u003c/p\u003e\n\u003col\u003e\n\u003cli\u003e\u003ca href=\"https://www.youtube.com/watch?v=0y6FtKsg6J4\"\u003eWhat is OSI Model | Real World Examples\u003c/a\u003e\n\u003cul\u003e\n\u003cli\u003eWhy this?\n\u003cul\u003e\n\u003cli\u003eThe answer to: \u0026ldquo;How does the communication work from client to server\u0026rdquo; is very important in backend. I wanted to see if I missed anything from my graduate studies, but nothing much. This was more like a refresher.\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003eNotes:\n\u003cul\u003e\n\u003cli\u003eEach operating system has their own implementation of TCP protocol. Linux has it\u0026rsquo;s own for example, and it\u0026rsquo;s open sourced.\u003c/li\u003e\n\u003cli\u003eCan take a look at TCP\u0026rsquo;s source code in Linux source code to understand how checksum correction works.\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003eCloud load balancers: L4 Load Balancer (operates at TCP Level), L7 Load Balnacer (operates at Application Protocol Layer - HTTP/HTTPS)\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"https://www.postgresql.org/docs/current/tutorial-arch.html\"\u003ePostgres: Architectural Fundamentals\u003c/a\u003e\n\u003cul\u003e\n\u003cli\u003eWhy this?\n\u003cul\u003e\n\u003cli\u003eGoing through database design and architectural fundamentals is helpful. I plan to explore CockroachDB soon, but wanted to get some idea about the ones I\u0026rsquo;ve used so far (Postgres for example)\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003eNotes:\n\u003cul\u003e\n\u003cli\u003eUses client/server model.\u003c/li\u003e\n\u003cli\u003ePostgreSQL session consists following processes:\n\u003cul\u003e\n\u003cli\u003eServer: manages DB files, accepts connections to the DB from client apps, performs DB actions on behalf of clients. DB server process is called postgres\u003c/li\u003e\n\u003cli\u003eClient (frontend): app that wants to perform DB operations.\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003eClient/server could be on diff hosts: communicate over TCP/IP n/w connection\u003c/li\u003e\n\u003cli\u003epostgres can handle multiple connections from clients:\n\u003cul\u003e\n\u003cli\u003eForks a new process for each connection\n\u003cul\u003e\n\u003cli\u003eThe new process and client directly communicate\u003c/li\u003e\n\u003cli\u003eNo intervention of postgres server\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003eSupervisor server process is always running (daemon process), waiting for client connections\n\u003cul\u003e\n\u003cli\u003eClient and child server process can come and go\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"https://www.geeksforgeeks.org/postgresql-system-architecture/\"\u003ePostgreSQL System Architecture\u003c/a\u003e\n\u003cul\u003e\n\u003cli\u003eWhy this?\n\u003cul\u003e\n\u003cli\u003eJust wanted to explore another blog on the same topic to see if I missed anything. Tbh, nothing much.\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003eNotes:\n\u003cul\u003e\n\u003cli\u003ePostgreSQL: process-per-transaction model\u003c/li\u003e\n\u003cli\u003epostgres server process:\n\u003cul\u003e\n\u003cli\u003eManaged by postmaster, central coordinating process\u003c/li\u003e\n\u003cli\u003eResponsibilities:\n\u003cul\u003e\n\u003cli\u003eInitializing, terminating the server\u003c/li\u003e\n\u003cli\u003eHandling connection requests from the new clients\u003c/li\u003e\n\u003cli\u003eRecovery\u003c/li\u003e\n\u003cli\u003eRun background processes\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003eShared Memory:\n\u003cul\u003e\n\u003cli\u003eReserved for DB caching and Transactional Log Caching\u003c/li\u003e\n\u003cli\u003eShared Disk Buffer\u003c/li\u003e\n\u003cli\u003eShared Tables\n\u003cul\u003e\n\u003cli\u003eUses same set of tables to host multiple client data (TODO: how?)\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003eBackend Processes:\n\u003cul\u003e\n\u003cli\u003eClient interacts with backend processes (submits queries and receiving queries result)\u003c/li\u003e\n\u003cli\u003eMultiple backend servers executing queries concurrently\u003c/li\u003e\n\u003cli\u003eEach backend server:\n\u003cul\u003e\n\u003cli\u003ewill handle only a single query at a time\u003c/li\u003e\n\u003cli\u003eaccess data from main memory buffer pool (placed in shared memory)\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003c/ol\u003e\n\u003cp\u003eThank you! :) See y\u0026rsquo;all in the next blog.\u003c/p\u003e","title":"Daily Update: 28th April 2023"},{"content":"Hi everyone!! I\u0026rsquo;ve an update, on what I\u0026rsquo;ve been up to - and I\u0026rsquo;m excited. It\u0026rsquo;s a rusty update üòâ. As always, I would love to tell you a story (this will help set some context), but if you want to skip and go directly to the update, please scroll to: # THE Update section.\nThe Story I\u0026rsquo;ve been through, possibly, the best and the worst time of my life. I use these contrasting words because everything went wrong - and that\u0026rsquo;s where you get an opportunity to shine. I think I fairly enjoyed resisting the pain and fighting through the odds, hopefully you\u0026rsquo;ll learn about it very soon on my blog (just need the courage to say it out loud :)).\nAnyways, just when the life hit extreme low, I had two options:\nCry about it, not for a day, but keep crying about it. Accept, and do something that I enjoy. I chose a mix of both 1 and 2. Cried for a day or two, but then accepted and started diverting my energy to the things I imagined myself doing. One of those, which I can share here, was finishing up my dream side projects. To name a few:\nCompleting the book: Writing Interpreter in Go. Do some leetcode to gain confidence. Release the next version of C++ File Manager. Write an App in Rust and more\u0026hellip; The 4th point you\u0026rsquo;re looking at, is exactly what we are going to cover today.\nTHE Update Here it comes! For those who (very kindly) follow my YouTube channel, know already that I\u0026rsquo;ve been learning Rust for a while. And while I was at it, I found it a good time to start building something. Pop! OS, the best Linux distribution I\u0026rsquo;ve ever come across, announced their new Cosmic Desktop Environment a while back (it\u0026rsquo;s a work in progress). I like reading source codes, and have been following what they were doing with Cosmic and remembered that they were using Iced Library for the development (GUI). I thought to give it a try, and the best place to start with, was the YouTube Monitoring App which I was building earlier with Python and Electron JS. Porting the idea to Rust was comparatively easier than to think of another project.\nBefore I proceed, feel free to check out the repository here. It would be already outdated for this blog, but nonetheless - I hope you like the work.\nSo the journey took off, started with going through the examples, and the styling example. Seeing how theme is switched, was mesmerizing.\nThat\u0026rsquo;s where I started, taking the first steps, just replicating the styling example to see if I can reproduce and understand what they were doing. As the time passed by, things started to take a good look:\nTheme switching was enabled. Each user had a card (but not aligned üò¢). The ideal stage would be to see the YouTube API collect details, create the JSON file, and show it to the user. I knew I was not even close to the final goal, but why give up?\nAnyways, my next target was to figure out having a grid - multiple rows and multiple columns. From what I know, there isn\u0026rsquo;t a grid like implementation within Iced, so I had to use Column and Row classes from Iced. It was tricky, if I have to be very honest, but finally worked out. Had to ask a couple of questions on the discord channel of Iced, they were very kind and quick to help me with my queries.\nI also had to choose an image for each user, as an avatar was necessary. Once added, I knew it will start looking better. A header and a footer were also required, to give the user some info about what they are seeing on the screen. However, when the grid wasn\u0026rsquo;t full, the image wouldn\u0026rsquo;t align and that was an issue to resolve for the next stage.\nAdding the avatar was very interesting, I wanted a link to work (for the avatar). But for now, I thought I\u0026rsquo;ll suffice to using an image path to see if it really works (spoiler: it did üòÑ):\nfn profile_pic\u0026lt;\u0026#39;a\u0026gt;(width: u16) -\u0026gt; Container\u0026lt;\u0026#39;a, Message\u0026gt; { container( // This should go away once we unify resource loading on native // platforms if cfg!(target_arch = \u0026#34;wasm32\u0026#34;) { image(\u0026#34;profile_images/Noddy.jpeg\u0026#34;) } else { image(format!( \u0026#34;{}/profile_images/Noddy.jpeg\u0026#34;, env!(\u0026#34;CARGO_MANIFEST_DIR\u0026#34;) )) } .height(Length::Units(width)) .width(Length::Units(width)), ) .width(Length::Fill) .center_x() } As I knew things were starting to take place, the next important step was to fix the alignment, and enable JSON parsing. Let me show you the issue we had with alignment:\nThe alignment issue was fixed with using align_items and also avoiding width(Length::Fill) with the user cards. We\u0026rsquo;ll discuss these details in the next blog, for now, I just shared to give you a hint.\nFor JSON parsing, to start with, I created a sample JSON file, and used serde crate to parse JSON file. The way a JSON can be deserialized into a JSON class, was amazing to see - very intuitive.\npub fn read_json(file_path: \u0026amp;str) -\u0026gt; Result\u0026lt;YTCreator, Box\u0026lt;dyn Error\u0026gt;\u0026gt; { let file = File::open(file_path)?; let reader = BufReader::new(file); // Read the JSON contents of the file as an instance of `YTCreator`. let u: YTCreator = serde_json::from_reader(reader)?; if u.size() \u0026gt; MAX_EXPECTED_ITEMS { Ok(u.slice_to(MAX_EXPECTED_ITEMS)) } else { Ok(u) } } The implementation of YTCreator had the fields that were useful for the user:\n#[derive(Deserialize, Debug, Default, Clone)] pub struct YTCreator { names: Vec\u0026lt;String\u0026gt;, avatar_links: Vec\u0026lt;String\u0026gt;, descriptions: Vec\u0026lt;String\u0026gt;, is_live_status: Vec\u0026lt;String\u0026gt;, subscribers: Vec\u0026lt;String\u0026gt; } If you have not noticed already, focus on the u.slice_to(MAX_EXPECTED_ITEMS) expression. While this isn\u0026rsquo;t really useful right now (more on this later), but it was a good practice to implement. This basically answers: \u0026ldquo;What if there is more data then the grid limit?\u0026rdquo;. Let\u0026rsquo;s say the JSON file contains the data for 13 users, but our grid could only have 12 users\u0026rsquo; data -\u0026gt; in this case, .slice_to will make sure that we only use maximum of 12 users on the grid:\nfn slice_to(\u0026amp;self, count_items: usize) -\u0026gt; YTCreator { let mut new_obj = YTCreator { names: Vec::new(), avatar_links: Vec::new(), descriptions: Vec::new(), is_live_status: Vec::new(), subscribers: Vec::new(), }; new_obj.names = self.names.get(0..count_items).unwrap().to_vec(); new_obj.avatar_links = self.avatar_links.get(0..count_items).unwrap().to_vec(); new_obj.descriptions = self.descriptions.get(0..count_items).unwrap().to_vec(); new_obj.is_live_status = self.is_live_status.get(0..count_items).unwrap().to_vec(); new_obj.subscribers = self.subscribers.get(0..count_items).unwrap().to_vec(); new_obj } The implementation of slice_to method is fairly simple, just creating another YTCreator object but with stripped count.\nOne of the very important step was to enable passing a link for the image/avatar. This was made possible using reqwest crate, getting the image in a form of bytes array and then converting it to an image::Handle.\nfn profile_pic\u0026lt;\u0026#39;a\u0026gt;(width: u16, link: String) -\u0026gt; Container\u0026lt;\u0026#39;a, Message\u0026gt; { let img_obj = reqwest::blocking::get(link).ok(); let img_bytes = match img_obj { Some(bytes) =\u0026gt; { bytes.bytes().ok() }, None =\u0026gt; None }.unwrap(); let out_img: image::Handle = image::Handle::from_memory(img_bytes.to_vec()); container( image(out_img) .height(Length::Units(width)) .width(Length::Units(width)), ) .width(Length::Fill) .center_x() } While I was very happy that:\nThe alignment was fixed Image links were working JSON parsing was done correctly BUT there was a small but significantly major mistake I made. Let\u0026rsquo;s keep it for later though, keep reading ‚ù§Ô∏è\nStarting from here, you\u0026rsquo;ll understand how important development is. In this stage, the UI won\u0026rsquo;t change BUT we\u0026rsquo;ll be significantly improving the performance.\nSo, while I was very happy about the project progress so far, I thought I\u0026rsquo;ll ask my friend to open it up on his system. He had Windows, but fortunately he also had WSL. Thanks to him, he didn\u0026rsquo;t give up on trying to run GUI on his WSL - and when finally the app loaded, it was LAGGING. \u0026#x1f622;\nI was surprised, because on my Mac - it was working fine. I opened up my Linux system, and guess what? It lagged there as well. I immediately knew I was doing something wrong. Now, as developers, we generally try to get to the bottleneck of the issue, and I had an instinct about it.\nfn view(\u0026amp;self) -\u0026gt; iced::Element\u0026lt;\u0026#39;_, Self::Message\u0026gt; { let choose_theme = ...; let content = container(column![choose_theme].spacing(20).padding(20).max_width(600)) .width(Length::Fill) .center_x(); let footer = ...; let title_header = ...; let all_cards = render_cards::create_list_of_cards(\u0026amp;self.json_obj); let binding = render_cards::ListOfCards::default(); // RELEVANT... let first_row = render_cards::create_row(all_cards.get(0).unwrap_or(\u0026amp;binding)); let second_row = render_cards::create_row(all_cards.get(1).unwrap_or(\u0026amp;binding)); let third_row = render_cards::create_row(all_cards.get(2).unwrap_or(\u0026amp;binding)); container(column![ content, horizontal_rule(10), title_header, horizontal_rule(10), first_row, second_row, third_row, horizontal_rule(10), footer, horizontal_rule(10), ]) .height(Length::Shrink) .into() } The view method above was running every time I switched the theme (whenever anything changed), and this was the problem. Let me show you the create_row method, which will give you an idea why it\u0026rsquo;s slow:\npub fn create_row(cards: \u0026amp;ListOfCards) -\u0026gt; Row\u0026lt;\u0026#39;static, Message\u0026gt; { Row::with_children( cards .cards .iter() .map(|each_card| { container( row![ column![create_card(each_card)].spacing(50).padding(20), column![profile_pic(130, each_card.avatar_link.to_owned())] .width(Length::Units(130)) .height(Length::Units(150)) .padding(20) ] .align_items(iced::Alignment::End) .height(Length::Fill), ) .width(Length::Fill) .center_y() .style(theme::Container::Box) .into() }) .collect(), ) } Just observe, that I was calling profile_pic method with the avatar link - which means that every time the theme is switched, the images will be fetched again. Wow! That\u0026rsquo;s the only bottleneck? Nope!\nSurprise! There is one more, we were actually reading the JSON file in the view function as well. Which was bad! So we finally have 2 bottlenecks for the performance:\nDownloading avatar link to an image handle. Parsing and deserializing the JSON file to YTCreator struct. (Also thanks to the iced community on discord, who helped with the first point above. They were really helpful.)\nSolving both bottlenecks above was intuitive to me. We could parse the JSON file as well as download the avatars when the app is created, for now. Later on, we\u0026rsquo;ll do it when the user has changed inputs for the YT users they want to monitor. Here is how I solved it:\nfn new() -\u0026gt; YTMonitor { let json_obj = render_cards::get_json_data(); let image_handles = render_cards::get_all_avatars(\u0026amp;json_obj); // Because dark as default is cool :D YTMonitor { theme: Theme::Dark, json_obj, loaded_photos: image_handles, } } As you can see above^, we have json_obj, loaded_photos as two added fields to the YTMonitor struct. We parsed the JSON file as well as downloaded all the avatars (we already know the avatar links from the JSON file). Later on, I would just use these as self.json_obj and self.image_handles in the view function, so no more performance degradation!\nfn view(\u0026amp;self) -\u0026gt; iced::Element\u0026lt;\u0026#39;_, Self::Message\u0026gt; { // ... let all_cards = render_cards::create_list_of_cards(\u0026amp;self.json_obj); let binding = render_cards::ListOfCards::default(); let all_photos = self.loaded_photos.to_owned(); let first_row = render_cards::create_row(all_cards.get(0).unwrap_or(\u0026amp;binding), \u0026amp;all_photos, 0); let second_row = render_cards::create_row(all_cards.get(1).unwrap_or(\u0026amp;binding), \u0026amp;all_photos, 4); let third_row = render_cards::create_row(all_cards.get(2).unwrap_or(\u0026amp;binding), \u0026amp;all_photos, 8); } As you can see above, I am passing the all_photos variable to create_row this time (along with the offset). The profile_pic method just takes the image handle now:\npub fn profile_pic\u0026lt;\u0026#39;a\u0026gt;(width: u16, img_handle: image::Handle) -\u0026gt; Container\u0026lt;\u0026#39;a, Message\u0026gt; { container( image(img_handle) .height(Length::Units(width)) .width(Length::Units(width)), ) .width(Length::Fill) .center_x() } ^^ This makes it much faster now! And yes, it did solve the problem. While doing this, of course, I learnt a lot of Rust, and Iced. So many things to learn, and I\u0026rsquo;m already enjoying the challenges.\nI know this doesn\u0026rsquo;t look the best yet, there are so many good apps out there built on the top of Iced, but I\u0026rsquo;m getting started. Once this is done, I know at least - I - would love it. ‚ù§Ô∏è\nHere is how the app looks like, right now (featuring my friend Mohit Wankhade üòÉ):\nWe aren\u0026rsquo;t done yet by the way, I have to cover some macros I built, but I\u0026rsquo;ll keep it for my next blog. Until then, I hope you continue grinding whatever you love, and make the best out of the time you have. Take care, and thank you for being here. üíó\n","permalink":"https://krshrimali.github.io/posts/2022/12/i-started-building-an-app-using-rust-and-here-is-how-it-went.../","summary":"\u003cp\u003eHi everyone!! I\u0026rsquo;ve an update, on what I\u0026rsquo;ve been up to - and I\u0026rsquo;m excited. It\u0026rsquo;s a \u003cem\u003erusty\u003c/em\u003e update üòâ. As always, I would love to tell you a story (this will help set some context), but if you want to skip and go directly to the update, please scroll to: \u003ccode\u003e# THE Update\u003c/code\u003e section.\u003c/p\u003e\n\u003ch2 id=\"the-story\"\u003eThe Story\u003c/h2\u003e\n\u003cp\u003eI\u0026rsquo;ve been through, possibly, the best and the worst time of my life. I use these contrasting words because everything went wrong - and that\u0026rsquo;s where you get an opportunity to shine. I think I fairly enjoyed resisting the pain and fighting through the odds, hopefully you\u0026rsquo;ll learn about it very soon on my blog (just need the courage to say it out loud :)).\u003c/p\u003e","title":"I started building an app using Rust and here is how it went..."},{"content":"Hi everyone! Sorry for missing out on publishing individual blogs for 19th, 20th and 21st November, but it has been very hectic for me, and I decided to give leetcode a break and explore System Design (a lot). Turns out, I had some idea about it already, one of the advantages of working with startups (CareAI/Dukaan). But in any case, quite a few interesting things I learnt. I can\u0026rsquo;t share all the links, as it will be just better to share the playlists or YT channels I referred:\nSystem Design Playlist from Gaurav Sen - I didn\u0026rsquo;t watch every single video, but about Consistent Hashing and Database Sharing were quite helpful. Design Twitter - System Design Interview from NeetCode - NeetCode is one of my favorite content creators in the tech on YT, always on point, no nonsense. System Design Interview: Design Amazon Prime Video from Exponent System Design Interview Channel - Not active anymore, but whatever videos he has are really good. SQL vs NoSQL or MySQL vs MongoDB from Academind - I got this question from NeetCode\u0026rsquo;s video on his design choice to use SQL, so this got me interested (NoSQL vs SQL). Reading about Kafka, Apache Hadoop MapReduce vs Apache Spark was also helpful. I intend to use them more to understand their implementation details. Personally, using and managing load balancers for horizontal scaling is also interesting.\nI\u0026rsquo;ll have more to share in the coming days, until then - take care! :)\n","permalink":"https://krshrimali.github.io/posts/2022/11/daily-update-19th-20th-21st-november-2022-day-5-to-day-7-inclusive/","summary":"\u003cp\u003eHi everyone! Sorry for missing out on publishing individual blogs for 19th, 20th and 21st November, but it has been very hectic for me, and I decided to give leetcode a break and explore System Design (a lot). Turns out, I had some idea about it already, one of the advantages of working with startups (CareAI/Dukaan). But in any case, quite a few interesting things I learnt. I can\u0026rsquo;t share all the links, as it will be just better to share the playlists or YT channels I referred:\u003c/p\u003e","title":"Daily Update: 19th 20th 21st November 2022 - Day 5 to Day 7 (inclusive)"},{"content":"Hi everyone, Day 5 of this series of daily updates. Started my day, fairly early but was feeling unwell. In any case, I did make some progress with my learning of the Interpreter. One of motivation to learn about Interpreter is to learn the whole process, from the code we write to the object files / executables. Of course, in this goal, my next step would be to read the \u0026ldquo;Write a compiler in Go\u0026rdquo; book. But one step at a time, right?\nProjects/Learning:\nCompleted Chapter 1 (Lexing) of the book \u0026ldquo;Write an Interpreter in Go\u0026rdquo;. Very detailed, and I love how the author included REPL in the chapter 1. Even though it\u0026rsquo;s not a lot of code, it just makes it so intuitive to run line by line and get the tokens. Started reading Chapter 2 (Parser). Fortunately, I do have some knowledge about grammars, BNF and EBNF forms, and using parser generators. But I agree with the author there, you\u0026rsquo;ll learn a lot if you create the parser from scratch. So I\u0026rsquo;m looking forward to it. Leetcode:\nRevisit most of the problems I\u0026rsquo;ve solved so far. Implement them again in C++ and Rust.\nThank you! :)\n","permalink":"https://krshrimali.github.io/posts/2022/11/daily-update-18th-november-2022-day-5/","summary":"\u003cp\u003eHi everyone, Day 5 of this series of daily updates. Started my day, fairly early but was feeling unwell. In any case, I did make some progress with my learning of the Interpreter. One of motivation to learn about Interpreter is to learn the whole process, from the code we write to the object files / executables. Of course, in this goal, my next step would be to read the \u0026ldquo;Write a compiler in Go\u0026rdquo; book. But one step at a time, right?\u003c/p\u003e","title":"Daily Update: 18th November 2022 - Day 5"},{"content":"Hi everyone, Day 4 of this series of daily updates. I don\u0026rsquo;t have a lot to share today, just one of those days where I spent a lot of time going through whatever I\u0026rsquo;ve done already. I take some time to look at the things I learned so far, and see if I could be more intuitive back then. I\u0026rsquo;m going to continue doing the same for the next day, though this time for the problems I\u0026rsquo;ve solved on Leetcode.\nProjects/Learning:\nStudied a bit about MapReduce algorithm. Surprisingly, there isn\u0026rsquo;t enough about it, except blogs like this from IBM sharing the same old definitions and examples. Spent some time going through possibilities of handwritten recognition using Stable Diffusion, I think it\u0026rsquo;s an interesting area to explore. Autoencoders can do a good job here, and with the amount of data we can generate for this task - it could be huge. Thank you! :)\n","permalink":"https://krshrimali.github.io/posts/2022/11/daily-update-17th-november-2022-day-4/","summary":"\u003cp\u003eHi everyone, Day 4 of this series of daily updates. I don\u0026rsquo;t have a lot to share today, just one of those days where I spent a lot of time going through whatever I\u0026rsquo;ve done already. I take some time to look at the things I learned so far, and see if I could be more intuitive back then. I\u0026rsquo;m going to continue doing the same for the next day, though this time for the problems I\u0026rsquo;ve solved on Leetcode.\u003c/p\u003e","title":"Daily Update: 17th November 2022 - Day 4"},{"content":"Hi everyone, Day 3 of this series of daily updates. Woke up fairly early (~6 AM), and started working. Though it seemed productive to me, but you won\u0026rsquo;t see a lot of content here today - just one of those days where I didn\u0026rsquo;t do a lot of different things, I guess. :)\nProjects/Learning:\nI made my rust-leetcode repository public today. It currently contains 12 solved problems, you\u0026rsquo;ll see huge difference between the first few codes to the last few codes. I\u0026rsquo;ve been learning, please don\u0026rsquo;t refer it for learning. I\u0026rsquo;m just documenting it there. Autogenerate README once a new file is added to my Rust Leetcode repository: I first started with writing a Python script to auto-generate the README.md file, from the modules registered in the src/lib.rs. The script is here. Since I wanted to explore how it can be done in Rust, I ported it to Rust as well. The script is here. Took some break from doing leetcode today, had a lot of work which I unfortunately can\u0026rsquo;t mention here.\n","permalink":"https://krshrimali.github.io/posts/2022/11/daily-update-16th-november-2022-day-3/","summary":"\u003cp\u003eHi everyone, Day 3 of this series of daily updates. Woke up fairly early (~6 AM), and started working. Though it seemed productive to me, but you won\u0026rsquo;t see a lot of content here today - just one of those days where I didn\u0026rsquo;t do a lot of different things, I guess. :)\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003eProjects/Learning\u003c/strong\u003e:\u003c/p\u003e\n\u003col\u003e\n\u003cli\u003eI made my \u003ca href=\"https://github.com/krshrimali/rust-leetcode/\"\u003erust-leetcode\u003c/a\u003e repository public today. It currently contains 12 solved problems, you\u0026rsquo;ll see huge difference between the first few codes to the last few codes. I\u0026rsquo;ve been learning, please don\u0026rsquo;t refer it for learning. I\u0026rsquo;m just documenting it there.\u003c/li\u003e\n\u003cli\u003eAutogenerate README once a new file is added to my Rust Leetcode repository:\u003c/li\u003e\n\u003c/ol\u003e\n\u003cul\u003e\n\u003cli\u003eI first started with writing a Python script to auto-generate the \u003ccode\u003eREADME.md\u003c/code\u003e file, from the modules registered in the \u003ccode\u003esrc/lib.rs\u003c/code\u003e. The script is \u003ca href=\"https://github.com/krshrimali/rust-leetcode/blob/main/update_readme.py\"\u003ehere\u003c/a\u003e.\u003c/li\u003e\n\u003cli\u003eSince I wanted to explore how it can be done in Rust, I ported it to Rust as well. The script is \u003ca href=\"https://github.com/krshrimali/rust-leetcode/blob/main/src/main.rs\"\u003ehere\u003c/a\u003e.\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eTook some break from doing leetcode today, had a lot of work which I unfortunately can\u0026rsquo;t mention here.\u003c/p\u003e","title":"Daily Update: 16th November 2022 - Day 3"},{"content":"Hi everyone, Day 2 of this series of daily updates. Started my day earlier than yesterday, at around 9 AM (work).\nLearning:\nWatched this nice video from Jason on The Right Way to Write C++ in 2022: I like when he advocated on using different compilers, or at least verifying that it works for all (and if not, then you\u0026rsquo;re aware). Interest: take warnings seriously. Need to start doing it, I\u0026rsquo;ve been following this with Rust, but C++ never raised Rust-like \u0026ldquo;good enough\u0026rdquo; warnings. Might as well start doing it now. Code coverage analysis: hmm, it used to stay at around 87 to 90% for the libraries I worked with. I agree, most of it should be tested, but as the library grows longer, you can\u0026rsquo;t block PRs on coverage (specially by the community). But it\u0026rsquo;s underrated, I\u0026rsquo;ve seen developers ignore it until they face the wrath of weird bugs. 100% testing coverage, will always be difficult, but if the goal is kept in that direction, it might reach to 95-100%. Never heard of fuzz testing before\u0026hellip;after reading about it, just another word of what we have done before. Never got a name to it though. ;) Ship with Hardening Enabled: Again, no idea about this. I\u0026rsquo;ll have to read about it. Marked as a TODO. Missed watching Edward\u0026rsquo;s stream on TorchDynamo, I woke up late. :/ Going to re-watch the stream now. I\u0026rsquo;ll share the notes in a separate blog once I\u0026rsquo;m done, I could only watch some of it today. I\u0026rsquo;ve started learning Kernel Fusion, how does it work and why it can help (and when it can not). This is going to be a long road, hopefully I\u0026rsquo;ll be able to share something concrete soon. Leetcode Problems:\nLongest substring without repeating characters Clone graph Note: this was done in C++, Leetcode didn\u0026rsquo;t have Rust added as the supported language for this. Product of Array Except Self I\u0026rsquo;m still learning Rust, so it takes some time for me to write the best Rust versions of the codes above, but yeah - doing it the best way I can. I don\u0026rsquo;t want to rush completing tens of problems in a day, to me - that doesn\u0026rsquo;t make sense. Ending Words Felt better as I am slowly collecting myself together. It\u0026rsquo;s going to take some time though, but I aim to be consistent with the things I love doing.\n","permalink":"https://krshrimali.github.io/posts/2022/11/daily-update-15th-november-2022-day-2/","summary":"\u003cp\u003eHi everyone, Day 2 of this series of daily updates. Started my day earlier than yesterday, at around 9 AM (work).\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003eLearning\u003c/strong\u003e:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eWatched this nice video from Jason on \u003ca href=\"https://www.youtube.com/watch?v=q7Gv4J3FyYE\"\u003eThe Right Way to Write C++ in 2022\u003c/a\u003e:\n\u003cul\u003e\n\u003cli\u003eI like when he advocated on using different compilers, or at least verifying that it works for all (and if not, then you\u0026rsquo;re aware).\u003c/li\u003e\n\u003cli\u003eInterest: take warnings seriously. Need to start doing it, I\u0026rsquo;ve been following this with Rust, but C++ never raised Rust-like \u0026ldquo;good enough\u0026rdquo; warnings. Might as well start doing it now.\u003c/li\u003e\n\u003cli\u003eCode coverage analysis: hmm, it used to stay at around 87 to 90% for the libraries I worked with. I agree, most of it should be tested, but as the library grows longer, you can\u0026rsquo;t block PRs on coverage (specially by the community). But it\u0026rsquo;s underrated, I\u0026rsquo;ve seen developers ignore it until they face the wrath of weird bugs. 100% testing coverage, will always be difficult, but if the goal is kept in that direction, it might reach to 95-100%.\u003c/li\u003e\n\u003cli\u003eNever heard of fuzz testing before\u0026hellip;after reading about it, just another word of what we have done before. Never got a name to it though. ;)\u003c/li\u003e\n\u003cli\u003eShip with Hardening Enabled: Again, no idea about this. I\u0026rsquo;ll have to read about it. Marked as a TODO.\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003eMissed watching Edward\u0026rsquo;s stream on TorchDynamo, I woke up late. :/ Going to re-watch the stream now. I\u0026rsquo;ll share the notes in a separate blog once I\u0026rsquo;m done, I could only watch \u003cem\u003esome\u003c/em\u003e of it today.\u003c/li\u003e\n\u003cli\u003eI\u0026rsquo;ve started learning Kernel Fusion, how does it work and why it can help (and when it can not). This is going to be a long road, hopefully I\u0026rsquo;ll be able to share something concrete soon.\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003e\u003cstrong\u003eLeetcode Problems\u003c/strong\u003e:\u003c/p\u003e","title":"Daily Update: 15th November 2022 - Day 2"},{"content":"Hi everyone, I\u0026rsquo;m back to - writing daily update blogs. Just sharing what I did today, and documenting my progress. Now of course it won\u0026rsquo;t contain everything I do, but yep - everything related to learning and things I\u0026rsquo;m passionate about.\nReading\nJust In Time Compilation (Wiki): Wiki: https://en.wikipedia.org/wiki/Just-in-time_compilation Honestly, I wanted to start with a few papers on it, but Wikipedia does give enough background and sets some context. Leetcode problems\nFor some context, I started doing these problems to brush up my problem solving skills. The following problems were solved in both Rust and C++. I\u0026rsquo;ll share the rust codes in a few days.\nK Closest Points to the origin Jump Game Jump Game - II Miscellaneous\nSpent some time fixing my colorscheme in neovim changing because of tmux :/ I was wondering what went wrong. Finally, I found this: neovim losing colorscheme when in tmux. I\u0026rsquo;ve updated my dotfiles with the fix in case anyone is still using it. Ending words\nStill starting off with the momentum. Going to take some time, but I hope to continue doing it.\n","permalink":"https://krshrimali.github.io/posts/2022/11/daily-update-14th-november-2022-day-1/","summary":"\u003cp\u003eHi everyone, I\u0026rsquo;m back to - writing daily update blogs. Just sharing what I did today, and documenting my progress. Now of course it won\u0026rsquo;t contain everything I do, but yep - everything related to learning and things I\u0026rsquo;m passionate about.\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003eReading\u003c/strong\u003e\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eJust In Time Compilation (Wiki):\n\u003cul\u003e\n\u003cli\u003eWiki: \u003ca href=\"https://en.wikipedia.org/wiki/Just-in-time_compilation\"\u003ehttps://en.wikipedia.org/wiki/Just-in-time_compilation\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003eHonestly, I wanted to start with a few papers on it, but Wikipedia does give enough background and sets some context.\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003e\u003cstrong\u003eLeetcode problems\u003c/strong\u003e\u003c/p\u003e","title":"Daily Update: 14th November 2022 - Day 1"},{"content":"Hi everyone! Over the last few months, I\u0026rsquo;ve received a lot of queries regarding applying to research internships. And I wanted to answer them shortly, in this blog, with a disclaimer to begin with.\nDISCLAIMER\nI did only one research internship (NTU Singapore, ROSE Labs), and there are people who are more qualified than me to answer this, but I\u0026rsquo;ll make my best attempt.\nHere is how I will suggest to apply:\nAssumptions You are interested in Research. You are not doing a research internship because you think getting into an industry is really tough, and emailing a professor will be easier. You know what field excites you, not necessarily \u0026ldquo;your passion\u0026rdquo; - but still your interests. You read papers in some frequency. (can be anything, I am too much a developer to comment on this üòÑ) You have your work hosted somewhere, for others to see and judge! ;) Process Gather information:\nFind the research papers that interested you the most. Take a note of the authors, and co-authors. Take a note of the institutes that funded this research. Take a note of their timezone. Does the author work in a lab? Take a note of that lab. Find the institutes that offer research internships (preferably paid). Not all of them do, but most of the times - professors have some funds. Can be outside India. Prefer institutes with good labs for your field over the best institutes with absolutely no active labs for your interests. Review\nCheck if the professors/institutes have openings in their labs? Most of the times, their home page will tell you about the status. Sometimes, Post Docs are also active in interviewing research interns, so it doesn\u0026rsquo;t have to be all about professors/institutes. What is the field of interest of the person? Is it something that aligns with you (you may not know in the first glance, but if you are excited about what they do\u0026hellip; then the answer is yes!)? Note that: for labs, a lot of times they pair with industries and other academic institutions Reach Out\nAssuming that the Review process is followed thoroughly\nEmail the profs/post docs/researchers: Try scheduling emails in their mornings, worst case: their time zones. Mention the research you\u0026rsquo;ve done about their work. Mention any relevant papers you\u0026rsquo;ve written (or read from their work, and share any feedback -\u0026gt; only honest feedback, please don\u0026rsquo;t write random things). Talk to them about how you can contribute, if you don\u0026rsquo;t have an idea on this - let them know about what you\u0026rsquo;ve done so far. Keep it brief. Wait for a few weeks. Gentle ping, in case they haven\u0026rsquo;t gotten back to you. If they didn\u0026rsquo;t, it\u0026rsquo;s okay.‚ù§Ô∏è There is a lot to explore, keep doing your work! A suggestion This process changes over time, depends on the labs, I tried my best to share the generalized version of it. However, remember that if you are honest to the work you do -\u0026gt; people will eventually recognize your talent, and you\u0026rsquo;ll get the opportunities you ever needed. Have patience, and enjoy the work.\nHope it helps you all. Thank you!\n","permalink":"https://krshrimali.github.io/posts/2022/09/applying-for-research-internships-universities/","summary":"\u003cp\u003eHi everyone! Over the last few months, I\u0026rsquo;ve received a lot of queries regarding applying to research internships. And I wanted to answer them shortly, in this blog, with a disclaimer to begin with.\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003eDISCLAIMER\u003c/strong\u003e\u003c/p\u003e\n\u003cp\u003eI did only one research internship (NTU Singapore, ROSE Labs), and there are people who are more qualified than me to answer this, but I\u0026rsquo;ll make my best attempt.\u003c/p\u003e\n\u003cp\u003eHere is how I will suggest to apply:\u003c/p\u003e","title":"Applying for Research Internships (Universities)"},{"content":"Hi everyone! In this blog, I will be discussing the algorithm used in Bismuth to find the closest relative window to be focused for focusWindowByDirection event. If you haven\u0026rsquo;t read the previous blog, make sure to give it a read here.\nRecap from the previous blog Let\u0026rsquo;s start with a quick recap though, in the previous blog, we discussed:\nfocusWindowByDirection requires the following information:\ndirection (from the user) - can be one of: right, left, top/up, bottom/down. activeWindow (from the current session) - this is needed since focusWindowByDirection event is a relative event to your current focused window. Neighbor window candidates (neighborCandidates) to your current window (activeWindow) and the given direction (direction). // declaration std::vector\u0026lt;Window\u0026gt; Engine::getNeighborCandidates(const FocusDirection \u0026amp;direction, const Window \u0026amp;basisWindow); // use std::vector\u0026lt;Window\u0026gt; neighborCandidates = getNeighborCandidates(direction, basisWindow); From these neighbor candidates (neighborCandidates), we will now find the closest relative window corner. To me, it was tricky to understand at first, so we\u0026rsquo;ll be discussing this in detail over in the later sections. Once we know the closest relative window corner, we\u0026rsquo;ll try to find the window which satisfies the corner condition. If there were multiple found, we\u0026rsquo;ll return the first one based on the time-stamp (last used) Understanding the scenario I want to start off with a visual, took me some time to draw it, but in case it doesn\u0026rsquo;t look good, I\u0026rsquo;m sorry! My drawing teacher in the high school tried his best, but\u0026hellip;\nAbove image is visual of a tiling window layout where there are in total 5 windows opened (just for imagination, no sane person would open these many windows on a 24 inch monitor\u0026hellip; xD): A, B, C, D, E, where as mentioned in the figure above, E is the active window and we are trying to focus UP. A few notes to take from the figure:\nA, B, C, D windows are of same height and width w and h. We\u0026rsquo;ll use this information later. E window is the active window with width: 2 * w and height: h. We are trying to focus UP. Getting Closest Relative Window Corner In the previous blog, we had covered getNeighborCandidates, the output here would be windows: A, B, C, D. The order will not matter here for understanding, so don\u0026rsquo;t worry about that.\nThe next steps in the code include:\nint closestRelativeWindowCorner = getClosestRelativeWindowCorner(direction, neighborCandidates); auto closestWindows = getClosestRelativeWindow(direction, neighborCandidates, getClosestRelativeWindow); return most_recently_used(closestWindows); I didn\u0026rsquo;t add comments here, because we\u0026rsquo;ll be going through these 2 magic functions below. Let\u0026rsquo;s start with getClosestRelativeWindowCorner. The source code for the definition of this function is:\nint Engine::getClosestRelativeWindowCorner(const Engine::FocusDirection \u0026amp;direction, const std::vector\u0026lt;Window\u0026gt; \u0026amp;neighbors) { return std::reduce(neighbors.cbegin(), neighbors.cend(), /* initial value */ direction == Engine::FocusDirection::Up || direction == Engine::FocusDirection::Left ? 0 : INT_MAX, [\u0026amp;](int prevValue, const Window \u0026amp;window) { switch (direction) { case Engine::FocusDirection::Up: return std::max(window.geometry().bottom(), prevValue); case Engine::FocusDirection::Down: return std::min(window.geometry().y(), prevValue); case Engine::FocusDirection::Left: return std::max(window.geometry().right(), prevValue); case Engine::FocusDirection::Right: return std::min(window.geometry().x(), prevValue); } }); } Don\u0026rsquo;t worry about the code if it confuses you, keep in mind that we have the direction as Engine::FocusDirection::Up, and neighbors as {A, B, C, D}. This function gets you the closest window corner relative to the active window or the basis window. How would you do that? Well, it will depend on the direction.\nIf the direction is Up or Down \u0026ndash;\u0026gt; you should compare the y coordinate. If the direction is Left or Right \u0026ndash;\u0026gt; you should compare the x coordinate.\nNow remember the mathematics lectures you had way back in the high school, if you wanna focus up, which vertex do you really care about? Keep your focus on the window C and E for once, the comparison should definitely be with the bottom right\u0026rsquo;s y coordinate, right? That\u0026rsquo;s what we do here:\ncase Engine::FocusDirection::Up: return std::max(window.geometry().bottom(), prevValue); A quick look at bottom() source code in qrect.h file:\nQ_DECL_CONSTEXPR inline QRect::bottom() const noexcept { return y2; } Where y2 is the bottom right\u0026rsquo;s y coordinate. Since we are going up, and anything above the basis window should have y value \u0026lt; basis window\u0026rsquo;s y value. (The top left of any screen is considered to be (0, 0) in this blog). Hence we set the initial value as 0. If we had to go down, we\u0026rsquo;ll set it to INT_MAX as for anything below the basis window, we\u0026rsquo;ll use std::min and hence INT_MAX will fade away with each neighbor window.\nAnyways, enough of theory, so what will be the output of this function for our scenario? Well, this function will give us y_C + h (which is equal to y_D + h, so any of them is fine). Now, we\u0026rsquo;ll go ahead to the next function.\nGetting Closest Relative Window std::vector\u0026lt;Window\u0026gt; getClosestRelativeWindow(const Engine::FocusDirection \u0026amp;direction, const std::vector\u0026lt;Window\u0026gt; \u0026amp;windowArray, const int \u0026amp;closestPoint) { std::vector\u0026lt;Window\u0026gt; result; std::copy_if(windowArray.cbegin(), windowArray.cend(), result.begin(), [\u0026amp;](const Window \u0026amp;window) { switch (direction) { case Engine::FocusDirection::Up: return window.geometry().bottom() \u0026gt; closestPoint - 5; case Engine::FocusDirection::Down: return window.geometry().y() \u0026lt; closestPoint + 5; case Engine::FocusDirection::Left: return window.geometry().right() \u0026gt; closestPoint - 5; case Engine::FocusDirection::Right: return window.geometry().x() \u0026lt; closestPoint + 5; } }); return result; } Again, remember, we have direction as Engine::FocusDirection::Up, windowArray as {A, B, C, D}, and closestPoint as y_C + h value.\nThis function only exists to give you all the windows which are close enough to the closestPoint. The output out of this function will be windows C, D (reminder: E is the basis or active window).\nSome will wonder why do we have two functions: getClosestRelativeWindowCorner, and getClosestRelativeWindow? And why this -5, +5? Unfortunately, it\u0026rsquo;s possible that some windows aren\u0026rsquo;t tiled properly, see this issue. I\u0026rsquo;ve attached the screenshot: (credits to the author)\nHence we can\u0026rsquo;t be too strict here. I personally believe this number +/- 5 should be tinkered better and not hard-coded, but that\u0026rsquo;s for later.\nSo from A, B, C, D being the neighbor candidates, we have C, D as the final closest windows to the basis window (E). Now which one to choose? That\u0026rsquo;s where we\u0026rsquo;ll have to store the timestamps for each window. And this timestamp should record the last time it was used or accessed. We just get the most recently used out of these windows, and I\u0026rsquo;ll be discussing in the future blogs. I think we discussed a lot today. So that should be it\u0026hellip;\nAcknowledgement I don\u0026rsquo;t want to shy away from thanking the main maintainer of Bismuth, gikari who has worked pro-actively on Bismuth. Of course, the credits should also go to krohnkite for the hard work they put in.\nIn case anyone has a feedback or suggestion, please leave a comment on this blog. I wish everyone good health and success. Thanks for reading \u0026lt;3\n","permalink":"https://krshrimali.github.io/posts/2022/07/porting-a-tiling-window-manager-extenstion-to-c-bismuth-part-2-getting-closest-relative-window/","summary":"\u003cp\u003eHi everyone! In this blog, I will be discussing the algorithm used in Bismuth to find the closest relative window to be focused for \u003ccode\u003efocusWindowByDirection\u003c/code\u003e event. If you haven\u0026rsquo;t read the previous blog, make sure to give it a read \u003ca href=\"https://krshrimali.github.io/posts/2022/07/porting-a-tiling-window-manager-extenstion-to-c-bismuth-part-1/\"\u003ehere\u003c/a\u003e.\u003c/p\u003e\n\u003ch2 id=\"recap-from-the-previous-blog\"\u003eRecap from the previous blog\u003c/h2\u003e\n\u003cp\u003eLet\u0026rsquo;s start with a quick recap though, in the previous blog, we discussed:\u003c/p\u003e\n\u003cp\u003e\u003ccode\u003efocusWindowByDirection\u003c/code\u003e requires the following information:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\u003ccode\u003edirection\u003c/code\u003e (from the user) - can be one of: \u003ccode\u003eright, left, top/up, bottom/down\u003c/code\u003e.\u003c/li\u003e\n\u003cli\u003e\u003ccode\u003eactiveWindow\u003c/code\u003e (from the current session) - this is needed since \u003ccode\u003efocusWindowByDirection\u003c/code\u003e event is a \u003cem\u003erelative\u003c/em\u003e event to your current focused window.\u003c/li\u003e\n\u003cli\u003eNeighbor window candidates (\u003ccode\u003eneighborCandidates\u003c/code\u003e) to your current window (\u003ccode\u003eactiveWindow\u003c/code\u003e) and the given direction (\u003ccode\u003edirection\u003c/code\u003e).\u003c/li\u003e\n\u003c/ul\u003e\n\u003cdiv class=\"highlight\"\u003e\u003cpre tabindex=\"0\" style=\"color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;\"\u003e\u003ccode class=\"language-cpp\" data-lang=\"cpp\"\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#75715e\"\u003e// declaration\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#75715e\"\u003e\u003c/span\u003estd\u003cspan style=\"color:#f92672\"\u003e::\u003c/span\u003evector\u003cspan style=\"color:#f92672\"\u003e\u0026lt;\u003c/span\u003eWindow\u003cspan style=\"color:#f92672\"\u003e\u0026gt;\u003c/span\u003e Engine\u003cspan style=\"color:#f92672\"\u003e::\u003c/span\u003egetNeighborCandidates(\u003cspan style=\"color:#66d9ef\"\u003econst\u003c/span\u003e FocusDirection \u003cspan style=\"color:#f92672\"\u003e\u0026amp;\u003c/span\u003edirection, \u003cspan style=\"color:#66d9ef\"\u003econst\u003c/span\u003e Window \u003cspan style=\"color:#f92672\"\u003e\u0026amp;\u003c/span\u003ebasisWindow);\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#75715e\"\u003e// use\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#75715e\"\u003e\u003c/span\u003estd\u003cspan style=\"color:#f92672\"\u003e::\u003c/span\u003evector\u003cspan style=\"color:#f92672\"\u003e\u0026lt;\u003c/span\u003eWindow\u003cspan style=\"color:#f92672\"\u003e\u0026gt;\u003c/span\u003e neighborCandidates \u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e getNeighborCandidates(direction, basisWindow);\n\u003c/span\u003e\u003c/span\u003e\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003cul\u003e\n\u003cli\u003eFrom these neighbor candidates (\u003ccode\u003eneighborCandidates\u003c/code\u003e), we will now find the closest relative window corner. To me, it was tricky to understand at first, so we\u0026rsquo;ll be discussing this in detail over in the later sections.\u003c/li\u003e\n\u003cli\u003eOnce we know the closest relative window corner, we\u0026rsquo;ll try to find the window which satisfies the corner condition.\u003c/li\u003e\n\u003cli\u003eIf there were multiple found, we\u0026rsquo;ll return the first one based on the time-stamp (last used)\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch2 id=\"understanding-the-scenario\"\u003eUnderstanding the scenario\u003c/h2\u003e\n\u003cp\u003eI want to start off with a visual, took me some time to draw it, but in case it doesn\u0026rsquo;t look good, I\u0026rsquo;m sorry! My drawing teacher in the high school tried his best, but\u0026hellip;\u003c/p\u003e","title":"Porting a Tiling Window Manager Extenstion to C++ (Bismuth): Part-2 (getting closest relative window)"},{"content":"Hi everyone! I understand it\u0026rsquo;s been a long time, and I\u0026rsquo;m so excited to be writing this blog today. In today\u0026rsquo;s blog, I wanted to talk about my journey (so far) on contributing to Bismuth (a KDE\u0026rsquo;s Tiling Window Manager Extension), mainly how and why I started, and where I am right now.\nThe Story: Why KDE Plasma and Why Bismuth? For the last few months (close to a year), I\u0026rsquo;ve been using Pop OS (a linux distribution by System 76) which had this amazing automatic tiling window extension called pop-shell, and it was close to what I always needed:\nTiling. A desktop environment. Ability to configure keyboard shortcuts. Ability to turn-off tiling to floating. An option to launch specific windows as floating windows. (example: Steam) An active community to seek help or suggestions from. Open-Sourced! Now some would say that there is a possibility to install tiling window managers on desktop environments (I\u0026rsquo;m aware i3 on KDE Plasma), but that just felt\u0026hellip; odd for some reason. So I stuck with Pop OS, until this happened, oh and also this. The second issue where there was a lag while dragging windows, was unfortunately not a Pop OS bug but was mostly related to upstream (mutter if I\u0026rsquo;m not wrong). And when they say that it only happened with NVIDIA drivers, I knew that it\u0026rsquo;s something that will probably take some time to resolve (I would rather prefer not to get into the details here).\nThat\u0026rsquo;s when I decided to explore KDE Plasma. KDE Plasma 5.25 was just announced, and oh man - it seemed to have impressed a lot of people out there. However, what impressed me the most was that it had no such issues with NVIDIA drivers, at least no lag while dragging windows. I also liked their zoom accessibility feature, much much much better than what GNOME had. Needless to say, that I had decided to stick to KDE Plasma after that.\nJust to give some context, I use multiple monitors and while people happily survive without a tiling window manager, I was the opposite - I felt the need of tiling, specially when I started streaming or sharing my work with others. And then I saw this video: \u0026ldquo;TILING comes to KDE Kwin? ;)\u0026rdquo;! I was wow-ed (is that a word BTW?). I was introduced to this amazing KDE Tiling Window Manager Extension named Bismuth (https://github.com/Bismuth-Forge/bismuth/). I didn\u0026rsquo;t waste any time in installing and setting up the extension on my machine\u0026hellip;\nThe Motivation: Why contribute? Of course, with great power comes great responsibility, and in the Linux ecosystem, with more users, comes more bugs. The same happened with Bismuth, lots of users started trying it, and it had good amount of issues, interestingly, less were bugs and more were about features. However, I got stuck with one of the most important feature I needed, and it was \u0026ldquo;Move window to the next/previous screen/monitor\u0026rdquo; with a keyboard-shortcut. Now, do note that Bismuth did promise that it comes with the feature, so it was a bug. And as any other user would do, I thought of raising an issue but there was one already: here. I regularly move my windows from one screen to another with keyboard shortcuts, and with this bug, I started facing issues. But as they say, in open-source, the community is everything. A guy with username: benemorius came up with a solution, and even though it took me some time to get it working, but it was eventually fixed. I started realizing how much I love this process, but more than that - I wanted to dive into the source code, and understand how it works. That was the time I realized I will look at the issues, found many opened, but since the maintainer of the library had a goal of porting it from typescript to C++, which meant that new features were essentially blocked till then (unless and until they are small with respect to the number of lines of code).\nThat was the time I realized that I should stop complaining, and instead start helping. I found an opened issue here and left a comment. The maintainer was very kind to respond, and guide through the process. And that\u0026rsquo;s where I started contributing to Bismuth.\nTo a lot of people, and even to me, porting looks like an onboarding task, you have got things baked in for yourself, all you have to do is port it to another language. This was different though, I realized that I might have to write my own code at some places (https://github.com/Bismuth-Forge/bismuth/issues/335#issuecomment-1159993392), it was a re-write from ground up.\nOne thing I missed writing so far, was how much I loved developing tools and libraries. It\u0026rsquo;s something that comes naturally to me, and Bismuth seemed to be an amazing place to continue my passion.\nContributions For those who might be unaware, I did all my contributions (so far) live on my Twitch channel: buffetcodes, and have uploaded all the recordings on my YouTube channel. There is a playlist if you are interested.\nHonestly speaking, I had no clue when I started that how the journey will be, how easy/difficult it will be! To me, it was just fun. I don\u0026rsquo;t know if it was easy, or if it was difficult, it was just something very fun to do! Plus, came with a lot of learning. So far, when I\u0026rsquo;m writing this blog, I\u0026rsquo;ve 2 opened PRs:\nC++ Port: focusWindowByDirection C++ Port: ThreeColumn layout To give you a glimpse, here is how the code looks when you do press a keyboard shortcut to focus window to your left/right/up/down:\nvoid Engine::focusWindowByDirection(FocusDirection direction) { auto windowsToChoseFrom = m_windows.visibleWindowsOn(activeSurface()); if (windowsToChoseFrom.empty()) { return; } // If there is no current window, select the first one. auto activeWindow = m_windows.activeWindow(); if (!activeWindow.has_value()) { activeWindow = windowsToChoseFrom.front(); } auto window = windowNeighbor(direction, activeWindow.value()); if (window.has_value()) { window-\u0026gt;activate(); } } Let\u0026rsquo;s consider that you are trying to focus right from your current window, so the parameter direction will have a value of FocusDirection::Right (it\u0026rsquo;s an enum). The current state of Bismuth only allowed you to move right/left/top/bottom on the current screen, that means if you want to move to the next monitor - you can\u0026rsquo;t use the same keyboard shortcuts. Hence the first line in the body of the function:\n// This gets all the visible windows (not hidden) on the active screen/montior/surface auto windowsToChoseFrom = m_windows.visible(activeSurface()); Now of-course there is a possibility that you have no windows visible on the surface, in that case it will just return (which is what the next 3 lines do):\n// Early return if no window is visible on the current surface if (windowsToChoseFrom.empty()) { return; } Okay, now comes the serious part. Whenever you think of focusWindowByDirection, there are 2 possibilities (apart from those listed above):\nYou have an active window, that means your mouse is already focused on a window on the current screen. You don\u0026rsquo;t have an active window, that means your focus can be on the panel, or maybe on an icon or wherever except a window on the current surface. These two cases need to be handled, and that\u0026rsquo;s what the next few lines do:\n// If there is no current window, select the first one. auto activeWindow = m_windows.activeWindow(); if (!activeWindow.has_value()) { activeWindow = windowsToChoseFrom.front(); } Once the extension knows what activeWindow is, it\u0026rsquo;s comparatively easier to figure out which window to focus on (only if it\u0026rsquo;s possible). Time to talk about the function that does the magic.\n/* This function returns the closest window (if any) from the current window for the given direction */ std::optional\u0026lt;Window\u0026gt; Engine::windowNeighbor(Engine::FocusDirection direction, const Window \u0026amp;basisWindow); Above is the declaration of the function, and hopefully the comment describes it well. You will definitely need to know the window relative to which you\u0026rsquo;ll return the output window, and the direction is a must. Note the return type, std::optional\u0026lt;Window\u0026gt;. As I said, it is possible that there is a window to the right, it\u0026rsquo;s also possible that there are no more windows to the right direction. Hence std::optional there.\nLet me quickly talk about the algorithm that Bismuth follows for this feature:\nGet all possible candidates in the neighborhood of the active window for the given direction: If the direction is right, you need to know how many tiled windows are on the right to the active window. These neighbor candidates can also be on the top-right if the given direction is right. Get the closest relative window from the candidates selected in the step-1. If there are multiple windows from step-2, return the window which was used recently (this means that we need each window to have \u0026ldquo;the time it was last used\u0026rdquo; as a meta-data). I\u0026rsquo;ll be diving deep into the code for each of these steps in my next blog.\nDo note that I\u0026rsquo;m not doing this full-time, so this will obviously look slow to a lot of people, but I see a motivation behind doing this. I also believe that it is worth to mention benemorius\u0026rsquo;s work on Bismuth, where has fixed a lot of the issues + added new features to Bismuth, and that is amazing! Shoutout to him on what he has been doing for the community.\nThank you for reading this blog, and if you are interested, feel free to check out Bismuth. üòÑ‚ù§Ô∏è\n","permalink":"https://krshrimali.github.io/posts/2022/07/porting-a-tiling-window-manager-extenstion-to-c-bismuth-part-1/","summary":"\u003cp\u003eHi everyone! I understand it\u0026rsquo;s been a long time, and I\u0026rsquo;m so excited to be writing this blog today. In today\u0026rsquo;s blog, I wanted to talk about my journey (so far) on contributing to \u003ca href=\"https://github.com/Bismuth-Forge/bismuth/\"\u003eBismuth (a KDE\u0026rsquo;s Tiling Window Manager Extension)\u003c/a\u003e, mainly how and why I started, and where I am right now.\u003c/p\u003e\n\u003ch2 id=\"the-story-why-kde-plasma-and-why-bismuth\"\u003eThe Story: Why KDE Plasma and Why Bismuth?\u003c/h2\u003e\n\u003cp\u003eFor the last few months (close to a year), I\u0026rsquo;ve been using Pop OS (a linux distribution by System 76) which had this amazing automatic tiling window extension called \u003ca href=\"https://github.com/pop-os/shell\"\u003e\u003ccode\u003epop-shell\u003c/code\u003e\u003c/a\u003e, and it was close to what I always needed:\u003c/p\u003e","title":"Porting a Tiling Window Manager Extenstion to C++ (Bismuth): Part-1"},{"content":"Chapter 8: Common Collections These are my notes from the chapter-8 of rust book. Please scroll down to the bottom (Note) section if you are curious about what this is.\n8.1: Storing Lists of Values with Vectors Vec\u0026lt;T\u0026gt; collection type discussed, aka vector: * By default contiguous. * All values should be of same type.\n// Creation let v: Vec\u0026lt;i32\u0026gt; = Vec::new(); // vec! macro for convenience // default integer type is i32 let v = vec![1, 2, 3]; // Modifying let mut v = Vec::new(); // Rust infers the type from the elements pushed here v.push(5); v.push(6); // ... // Dropping // a vector is freed, when it goes out of scope { let v = vec![1, 2, 3, 4]; // ... } // \u0026lt;-- v goes out of scope here, and hence memory is freed as well // Reading Elements of Vectors let v = vec![1, 2, 3, 4, 5]; // First way: let third: \u0026amp;i32 = \u0026amp;v[2]; println!(\u0026#34;The third element is: {}\u0026#34;, third); // Second way: match v.get(2) { Some(num) =\u0026gt; println!(\u0026#34;The third element is: {}\u0026#34;, num), None =\u0026gt; println!(\u0026#34;There is no third element.\u0026#34;), } .get(\u0026amp;index) method allows you to handle out of range errors.\nWhen a program has a valid reference, the borrow checker enforces the ownership and borrowing rules to ensure this reference and any other references to the contents of the vector remain valid.\nFollowing example will fail:\nlet mut v = vec![1, 2, 3, 4, 5]; let first = \u0026amp;v[0]; v.push(6); println!(\u0026#34;The first element is: {}\u0026#34;, first); Adding an element to a vector may require the old memory chunk to be transferred to a new space, causing old memory disallocation for the object v. Accessing \u0026amp;v[0] can thus be dangerous, and hence the borrowing rules prevent programs from ending up in that situation. // Iterating over the values in a vector let v = vec![100, 32, 57]; // get immutable references to each element in a vector v for i in \u0026amp;v { println!(\u0026#34;{}\u0026#34;, i); } // get mutable references to each element in a mutable veector v let mut v = vec![100, 32, 57]; for i in \u0026amp;mut v { *i += 50; } You can use an enum to store multiple type values in a vector, but indirectly, this is how:\nenum SpreadsheetCell { Int(i32), Float(f64), Text(String), } let row = vec![ SpreadsheetCell::Int(3), SpreadsheetCell::Text(String::from(\u0026#34;blue\u0026#34;)), SpreadsheetCell::Float(10.12), ]; Note how row vector still has same types (of enum SpreadsheetCell) but can hold values of multiple types through enums.\n8.2: Storing UTF-8 Encoded Text with Strings In Rust, strings are implemented as a collection of bytes + some methods to provide useful functionality when those bytes are interpreted as text.\n(both types listed below are UTF-8 encoded)\nRust has only one string type in the core language: string slice str (usually seen in borrowed form: \u0026amp;str). Rust\u0026rsquo;s standard library provides String type: (not coded into the core language): It\u0026rsquo;s growbable, mutable, and owned UTF-8 encoded string type. There are other string types included in Rust\u0026rsquo;s standard library: OsString, OsStr, CString, CStr\u0026quot;. Types ending with Stringrefer to owned variants, while types ending withStr` refer to borrowed variants. (not discussed in the book/chapter)\n// Creating a new string let mut s = String::new(); // If you have some initial data, use to_string method // this is available on any type that implements the Display trait // Following three methods are valid // First let data = \u0026#34;initial contents\u0026#34;; // a string literal let s = data.to_string(); // Second let s = \u0026#34;initial conents\u0026#34;.to_string(); // Third let s = String::from(\u0026#34;initial contents\u0026#34;); // Updating a string let mut s = String::from(\u0026#34;foo\u0026#34;); s.push_str(\u0026#34;bar\u0026#34;); // s is now \u0026#34;foobar\u0026#34; // Appending to a string // using push_str and push let mut s = String::from(\u0026#34;foo\u0026#34;); s.push_str(\u0026#34;bar\u0026#34;); let mut s1 = String::from(\u0026#34;foo\u0026#34;); let s2 = \u0026#34;bar\u0026#34;; // We don\u0026#39;t take ownership of s2 here // To ensure that we can still use s2 even after appending contents to s1 s1.push_str(s2); println!(\u0026#34;s2 is {}\u0026#34;, s2); println!(\u0026#34;s1 is {}\u0026#34;, s1); // Use push to append single character to the String let mut s = String::from(\u0026#34;lo\u0026#34;); s.push(\u0026#39;l\u0026#39;); // now \u0026#34;lol\u0026#34; // Concatenation with + operator or the format! macro let s1 = String::from(\u0026#34;Hello, \u0026#34;); let s2 = String::from(\u0026#34;world!\u0026#34;); let s3 = s1 + \u0026amp;s2; Consider the following script:\nlet s1 = String::from(\u0026#34;foo\u0026#34;); let s2 = String::from(\u0026#34; bar\u0026#34;); // The operation takes ownership of s1 here let s3 = s1 + \u0026amp;s2; // You can not use s1 after the operation above println!(\u0026#34;s3 is: {}\u0026#34;, s3); Following is the error you\u0026rsquo;ll get:\nerror[E0382]: borrow of moved value: `s1` --\u0026gt; src/main.rs:5:27 | 2 | let s1 = String::from(\u0026#34;foo\u0026#34;); | -- move occurs because `s1` has type `String`, which does not implement the `Copy` trait 3 | let s2 = String::from(\u0026#34; bar\u0026#34;); 4 | let s3 = s1 + \u0026amp;s2; | -- value moved here 5 | println!(\u0026#34;s1 is: {}\u0026#34;, s1); | ^^ value borrowed here after move For more information about this error, try `rustc --explain E0382` So what\u0026rsquo;s happening? The + operator uses an add method whose signature looks like this for our inputs:\nfn add(self, s: \u0026amp;str) -\u0026gt; String { // ... } A few notes about s parameter:\nIt\u0026rsquo;s taken by reference. Means you add reference of the second string to the firstt string. You can not add 2 String values together, only \u0026amp;str to a String. (Rust compiler, in our case, coerces the \u0026amp;String argument into a \u0026amp;str, more on deref coercion later). Note about self parameter:\nadd method takes ownership of self (it doesn\u0026rsquo;t have an \u0026amp;). The point above implies, s1 will be moved to the add call and no longer be valid after that. let s3 = s1 + \u0026amp;s2; The above statement actually:\nMoves s1 into the add call / takes ownership of s1 (making it invalid after that) Appends a copy of the contents of s2. Returns ownership of the result. (this process is more efficient than copying).\n// Concatenating multiple strings let s1 = String::from(\u0026#34;tic\u0026#34;); let s2 = String::from(\u0026#34;tac\u0026#34;); let s2 = String::from(\u0026#34;toe\u0026#34;); let s = s1 + \u0026#34;-\u0026#34; + \u0026amp;s2 + \u0026#34;-\u0026#34; + \u0026amp;s3; // You can also do: // Uses reference for all parameters, so no ownership of any here... let s = format!(\u0026#34;{}-{}-{}\u0026#34;, s1, s2, s3); Talking about Indexing into Strings:\nThe following will give an error.\n// Indexing into Strings // Not like other languages (C++/Python): let s1 = String::from(\u0026#34;hello\u0026#34;); let h = s1[0]; Error:\nerror[E0277]: the type `String` cannot be indexed by `{integer}` --\u0026gt; src/main.rs:3:13 | 3 | let h = s1[0]; | ^^^^^ `String` cannot be indexed by `{integer}` | = help: the trait `Index\u0026lt;{integer}\u0026gt;` is not implemented for `String` Rust strings don\u0026rsquo;t support indexing. To understand, let\u0026rsquo;s understand how memory storage works for strings in Rust.\nA String is a wrapper over a Vec\u0026lt;u8\u0026gt;. Consider:\n// Hola is 4 bytes long (each of the chars take 1 byte when encoded in UTF-8) and len will be 4 let hello = String::from(\u0026#34;Hola\u0026#34;); // Now consider: let hello = String::from(\u0026#34;–ó–¥—Ä–∞–≤—Å—Ç–≤—É–π—Ç–µ\u0026#34;); // Number of bytes stored for hello: 24 // Each unicode scalar value in the string above takes 2 bytes of storage. Hence, indexing into string\u0026rsquo;s bytes will not always correlate to a valid Unicode scalar value. Hence, no indexing support for strings in Rust.\nA final reason Rust doesn‚Äôt allow us to index into a String to get a character is that indexing operations are expected to always take constant time (O(1)). But it isn‚Äôt possible to guarantee that performance with a String, because Rust would have to walk through the contents from the beginning to the index to determine how many valid characters there were.\nSlicing strings:\nlet hello = \u0026#34;–ó–¥—Ä–∞–≤—Å—Ç–≤—É–π—Ç–µ\u0026#34;; // This will give you first 4 bytes of hello string let s = \u0026amp;hello[0..4]; // In hello, every character is of 2 bytes, so s will have –ó–¥ With the same hello string, what will be the output if we used let s = \u0026amp;hello[0..1];? Following error will be raised:\nthread \u0026#39;main\u0026#39; panicked at \u0026#39;byte index 1 is not a char boundary; it is inside \u0026#39;–ó\u0026#39; (bytes 0..2) of `–ó–¥—Ä–∞–≤—Å—Ç–≤—É–π—Ç–µ`\u0026#39;, src/main.rs:3:13 Iterating over strings: use the chars method.\nfor c in \u0026#34;‡§®‡§Æ‡§∏‡•ç‡§§‡•á\u0026#34;.chars() { println!(\u0026#34;{}\u0026#34;, c); } // Output: // ‡§® // ‡§Æ // ‡§∏ // ‡•ç // ‡§§ // ‡•á // bytes method will return each raw byte for b in \u0026#34;‡§®‡§Æ‡§∏‡•ç‡§§‡•á\u0026#34;.bytes() { println!(\u0026#34;{}\u0026#34;, b); } // Output: // All 18 bytes that made up this string // 224 // 164 // 168 // 224 // 164 // 174 // 224 // 164 // 184 // 224 // 165 // 141 // 224 // 164 // 164 // 224 // 165 // 135 Note: valid unicode scalar values maybe made up of more than 1 byte, like above.\nNote NOTE\nThese are just my notes, or things I write down while/after reading the chapters/blogs or going through resources. I like sharing them, for everyone\u0026rsquo;s and also my memory. At no point I say or mean that these should be preferred or read \u0026ldquo;over\u0026rdquo; the original resource mentioned. But as always, I\u0026rsquo;m open for feedback and/or suggestions, so feel free to comment here on the blog (just be nice, is all I ask for).\n","permalink":"https://krshrimali.github.io/posts/2022/01/common-collections-vector-and-strings-in-rust-notes/","summary":"\u003ch2 id=\"chapter-8-common-collections\"\u003eChapter 8: Common Collections\u003c/h2\u003e\n\u003cp\u003eThese are my notes from the \u003ca href=\"https://doc.rust-lang.org/book/ch08-00-common-collections.html\"\u003echapter-8\u003c/a\u003e of \u003ca href=\"https://doc.rust-lang.org/book\"\u003erust book\u003c/a\u003e. Please scroll down to the bottom (\u003ccode\u003eNote\u003c/code\u003e) section if you are curious about what this is.\u003c/p\u003e\n\u003ch3 id=\"81-storing-lists-of-values-with-vectors\"\u003e8.1: Storing Lists of Values with Vectors\u003c/h3\u003e\n\u003cp\u003e\u003ccode\u003eVec\u0026lt;T\u0026gt;\u003c/code\u003e collection type discussed, aka vector:\n* By default contiguous.\n* All values should be of same type.\u003c/p\u003e\n\u003cdiv class=\"highlight\"\u003e\u003cpre tabindex=\"0\" style=\"color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;\"\u003e\u003ccode class=\"language-rust\" data-lang=\"rust\"\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#75715e\"\u003e// Creation\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#75715e\"\u003e\u003c/span\u003e\u003cspan style=\"color:#66d9ef\"\u003elet\u003c/span\u003e v: Vec\u003cspan style=\"color:#f92672\"\u003e\u0026lt;\u003c/span\u003e\u003cspan style=\"color:#66d9ef\"\u003ei32\u003c/span\u003e\u003cspan style=\"color:#f92672\"\u003e\u0026gt;\u003c/span\u003e \u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e Vec::new();\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#75715e\"\u003e// vec! macro for convenience\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#75715e\"\u003e// default integer type is i32\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#75715e\"\u003e\u003c/span\u003e\u003cspan style=\"color:#66d9ef\"\u003elet\u003c/span\u003e v \u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e vec![\u003cspan style=\"color:#ae81ff\"\u003e1\u003c/span\u003e, \u003cspan style=\"color:#ae81ff\"\u003e2\u003c/span\u003e, \u003cspan style=\"color:#ae81ff\"\u003e3\u003c/span\u003e];\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#75715e\"\u003e// Modifying\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#75715e\"\u003e\u003c/span\u003e\u003cspan style=\"color:#66d9ef\"\u003elet\u003c/span\u003e \u003cspan style=\"color:#66d9ef\"\u003emut\u003c/span\u003e v \u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e Vec::new();\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#75715e\"\u003e// Rust infers the type from the elements pushed here\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#75715e\"\u003e\u003c/span\u003ev.push(\u003cspan style=\"color:#ae81ff\"\u003e5\u003c/span\u003e);\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003ev.push(\u003cspan style=\"color:#ae81ff\"\u003e6\u003c/span\u003e);\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#75715e\"\u003e// ...\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#75715e\"\u003e\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#75715e\"\u003e// Dropping\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#75715e\"\u003e// a vector is freed, when it goes out of scope\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#75715e\"\u003e\u003c/span\u003e{\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e    \u003cspan style=\"color:#66d9ef\"\u003elet\u003c/span\u003e v \u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e vec![\u003cspan style=\"color:#ae81ff\"\u003e1\u003c/span\u003e, \u003cspan style=\"color:#ae81ff\"\u003e2\u003c/span\u003e, \u003cspan style=\"color:#ae81ff\"\u003e3\u003c/span\u003e, \u003cspan style=\"color:#ae81ff\"\u003e4\u003c/span\u003e];\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e    \u003cspan style=\"color:#75715e\"\u003e// ...\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#75715e\"\u003e\u003c/span\u003e} \u003cspan style=\"color:#75715e\"\u003e// \u0026lt;-- v goes out of scope here, and hence memory is freed as well\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#75715e\"\u003e\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#75715e\"\u003e// Reading Elements of Vectors\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#75715e\"\u003e\u003c/span\u003e\u003cspan style=\"color:#66d9ef\"\u003elet\u003c/span\u003e v \u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e vec![\u003cspan style=\"color:#ae81ff\"\u003e1\u003c/span\u003e, \u003cspan style=\"color:#ae81ff\"\u003e2\u003c/span\u003e, \u003cspan style=\"color:#ae81ff\"\u003e3\u003c/span\u003e, \u003cspan style=\"color:#ae81ff\"\u003e4\u003c/span\u003e, \u003cspan style=\"color:#ae81ff\"\u003e5\u003c/span\u003e];\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#75715e\"\u003e// First way:\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#75715e\"\u003e\u003c/span\u003e\u003cspan style=\"color:#66d9ef\"\u003elet\u003c/span\u003e third: \u003cspan style=\"color:#66d9ef\"\u003e\u0026amp;\u003c/span\u003e\u003cspan style=\"color:#66d9ef\"\u003ei32\u003c/span\u003e \u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e \u003cspan style=\"color:#f92672\"\u003e\u0026amp;\u003c/span\u003ev[\u003cspan style=\"color:#ae81ff\"\u003e2\u003c/span\u003e];\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003eprintln!(\u003cspan style=\"color:#e6db74\"\u003e\u0026#34;The third element is: \u003c/span\u003e\u003cspan style=\"color:#e6db74\"\u003e{}\u003c/span\u003e\u003cspan style=\"color:#e6db74\"\u003e\u0026#34;\u003c/span\u003e, third);\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#75715e\"\u003e// Second way:\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#75715e\"\u003e\u003c/span\u003e\u003cspan style=\"color:#66d9ef\"\u003ematch\u003c/span\u003e v.get(\u003cspan style=\"color:#ae81ff\"\u003e2\u003c/span\u003e) {\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e    Some(num) \u003cspan style=\"color:#f92672\"\u003e=\u0026gt;\u003c/span\u003e println!(\u003cspan style=\"color:#e6db74\"\u003e\u0026#34;The third element is: \u003c/span\u003e\u003cspan style=\"color:#e6db74\"\u003e{}\u003c/span\u003e\u003cspan style=\"color:#e6db74\"\u003e\u0026#34;\u003c/span\u003e, num),\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e    None \u003cspan style=\"color:#f92672\"\u003e=\u0026gt;\u003c/span\u003e println!(\u003cspan style=\"color:#e6db74\"\u003e\u0026#34;There is no third element.\u0026#34;\u003c/span\u003e),\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e}\n\u003c/span\u003e\u003c/span\u003e\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003cp\u003e\u003ccode\u003e.get(\u0026amp;index)\u003c/code\u003e method allows you to handle out of range errors.\u003c/p\u003e","title":"Common Collections (Vector and Strings) in Rust [Notes]"},{"content":"Chapter 8: Common Collections (Hash Maps) In the previous blog, I shared my notes on strings and vectors in Rust, and in this post we\u0026rsquo;ll cover Hash Maps. I personally have found their use in competitive programming, a lot, but hopefully as we move on, we\u0026rsquo;ll see lots of use-cases in real-life problems.\nHash Maps Hash Maps: HashMap\u0026lt;K, V\u0026gt;\nYou can\u0026rsquo;t access using indices, but through keys. Hash Maps store data on heap. Hash Maps are homogenous (all keys must have same type, and all values must have same type). Use std::collections::HashMap to bring HashMap to scope. Creating a New Hash Map\nFirst Way:\nuse std::collections::HashMap; let mut scores = HashMap::new(); // Now insert key, val pair scores.insert(\u0026#34;Kush\u0026#34;, 3); scores.insert(\u0026#34;Kushashwa\u0026#34;, 10); Second Way: In case you have keys and values stored in two different vectors, and want to generate a hashmap from them, use collect().\nuse std::collections::HashMap; let teams = vec![String::from(\u0026#34;Blue\u0026#34;), String::from(\u0026#34;Yellow\u0026#34;)]; let initial_scores = vec![10, 50]; // Types will be infered by Rust from the data in the vectors let mut scores: HashMap\u0026lt;_, _\u0026gt; = teams.into_iter().zip(initial_scores.into_iter()).collect(); Why HashMap\u0026lt;_, _\u0026gt;? collect() method can store values into different datastructures, and we need to specify the type of scores. For the types of keys and values, Rust can infer the types itself - hence we specify _.\nI was curious what into_iter() does, and here is a very interesting answer to into_iter() vs iter() on Stackoverflow. Someone mentioned this blog post in the comments, I\u0026rsquo;ve this as a TODO - but from the looks of it, it might be useful.\nIf you are curious what teams.into_iter().zip(initial_scores.into_iter()) do? Great, check this:\nfn main() { let teams = vec![String::from(\u0026#34;Blue\u0026#34;), String::from(\u0026#34;Yellow\u0026#34;)]; let initial_scores = vec![10, 50]; let tuple_output = teams.into_iter().zip(initial_scores.into_iter()); for item in tuple_output { println!(\u0026#34;{}, {}\u0026#34; ,item.0, item.1); } } Outputs:\nBlue, 10 Yellow, 50 So clearly, creating a vector of tuples (of a String and Integer). Maybe try passing a vector of different length to .zip() function? ;)\nHash Maps and Ownership Ownership is always the center of discussion when it comes to Rust. When you create a hashmap, and insert String objects, i32 values, the ownership behaves differently:\nFor types that implement Copy trait, like i32, the values will be copied to the hashmap. For String values (owned values), the values are moved into the hashmap and the hashmap will be the owner. See the following example and corresponding error:\nuse std::collections::HashMap; fn main() { let str1 = String::from(\u0026#34;Kush\u0026#34;); let str2 = String::from(\u0026#34;Name\u0026#34;); let mut map = HashMap::new(); map.insert(str1, str2); println!(\u0026#34;{}\u0026#34;, str1); } | 4 | let str1 = String::from(\u0026#34;Kush\u0026#34;); | ---- move occurs because `str1` has type `String`, which does not implement the `Copy` trait ... 8 | map.insert(str1, str2); | ---- value moved here 9 | 10 | println!(\u0026#34;{}\u0026#34;, str1); | ^^^^ value borrowed here after move As you can see, value was moved when map.insert(str1, str2) was called. One important point, that I\u0026rsquo;ll just take from the book:\nIf we insert references to values into the hash map, the values won‚Äôt be moved into the hash map. The values that the references point to must be valid for at least as long as the hash map is valid.\n(This is covered in detail in Chapter 10, so let\u0026rsquo;s save it for later).\nAccessing Values in a Hash Map Use \u0026lt;HashMap\u0026gt;.get(\u0026lt;Key\u0026gt;) function to get the value corresponding to the key. Please note that it returns Option\u0026lt;\u0026amp;V\u0026gt; where V is the type of the value.\nuse std::collections::HashMap; fn main() { let str1 = String::from(\u0026#34;Kush\u0026#34;); let str2 = String::from(\u0026#34;Name\u0026#34;); let mut map = HashMap::new(); map.insert(str1, str2); let val = map.get(\u0026amp;String::from(\u0026#34;Kush\u0026#34;)); println!(\u0026#34;{}\u0026#34;, val.unwrap()); } Outputs: Name.\nSince map.get(\u0026amp;String::from(\u0026quot;Kush\u0026quot;)) returns Option\u0026lt;\u0026amp;String\u0026gt; object, we need to unwrap() it in order to print it. Here is an interesting post on why using unwrap() is not the best idea, and what other options we have. I used unwrap() here, since I knew it is not None.\nIterating through a hash map is also easy in Rust:\n// Use the same code as above for (key, value) in \u0026amp;map { println!(\u0026#34;{}: {}\u0026#34;, key, value); } // Output: // Kush: Name Updating a Hash Map Note that you can only have one value corresponding to a key. Let\u0026rsquo;s consider our options:\nOverwriting a value: insert() twice, and the new value will replace the old value.\nOnly insert if key has no value:\nUse entry(\u0026lt;Key\u0026gt;), it returns an Enum: Empty (represents a value that might or might not exist). Call or_insert on the enum, which inserts a value if key doesn\u0026rsquo;t exist. or_insert returns a mutable reference to the value present, or to the new value inserted. use std::collections::HashMap; fn main() { let mut scores = HashMap::new(); scores.insert(String::from(\u0026#34;Blue\u0026#34;), 10); scores.entry(String::from(\u0026#34;Yellow\u0026#34;)).or_insert(50); scores.entry(String::from(\u0026#34;Blue\u0026#34;)).or_insert(50); println!(\u0026#34;{:?}\u0026#34;, scores); } // Outputs // {\u0026#34;Blue\u0026#34;: 10, \u0026#34;Yellow\u0026#34;: 50} Updating a value based on the old value: Comments in the code should help explain the example.\nuse std::collections::HashMap; fn main() { let text = \u0026#34;hello world wonderful world\u0026#34;; let mut map = HashMap::new(); // \u0026#34;hello\u0026#34;, \u0026#34;world\u0026#34;, \u0026#34;wonderful\u0026#34;, \u0026#34;world\u0026#34; - values of word for word in text.split_whitespace() { // if the map has an entry for word, return the count of it // else, insert 0 and increase the counter by 1 // note that count is mutable reference // so it needs to be de-referenced (in order to use it) let count = map.entry(word).or_insert(0); *count += 1; } println!(\u0026#34;{:?}\u0026#34;, map); } // Outputs: // {\u0026#34;world\u0026#34;: 2, \u0026#34;wonderful\u0026#34;: 1, \u0026#34;hello\u0026#34;: 1} Hashing Functions By default, Hash Maps in Rust use SipHash hashing function, but you can use your own as well - it should implement BuildHasher trait. More on this is discussed in Chapter 10.\nThat\u0026rsquo;s it for Chapter 8, thank you for reading. :)\nNote These are just my notes, or things I write down while/after reading the chapters/blogs or going through resources. I like sharing them, for everyone\u0026rsquo;s and also my memory. At no point I say or mean that these should be preferred or read \u0026ldquo;over\u0026rdquo; the original resource mentioned. But as always, I\u0026rsquo;m open for feedback and/or suggestions, so feel free to comment here on the blog (just be nice, is all I ask for).\n","permalink":"https://krshrimali.github.io/posts/2022/01/common-collections-vector-and-strings-in-rust-notes/","summary":"\u003ch2 id=\"chapter-8-common-collections-hash-maps\"\u003eChapter 8: Common Collections (Hash Maps)\u003c/h2\u003e\n\u003cp\u003eIn the \u003ca href=\"https://krshrimali.github.io/posts/2022/01/common-collections-vector-and-strings-in-rust-notes/\"\u003eprevious blog\u003c/a\u003e, I shared my notes on strings and vectors in Rust, and in this post we\u0026rsquo;ll cover Hash Maps. I personally have found their use in competitive programming, a lot, but hopefully as we move on, we\u0026rsquo;ll see lots of use-cases in real-life problems.\u003c/p\u003e\n\u003ch2 id=\"hash-maps\"\u003eHash Maps\u003c/h2\u003e\n\u003cp\u003eHash Maps: \u003ccode\u003eHashMap\u0026lt;K, V\u0026gt;\u003c/code\u003e\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eYou can\u0026rsquo;t access using indices, but through keys.\u003c/li\u003e\n\u003cli\u003eHash Maps store data on heap.\u003c/li\u003e\n\u003cli\u003eHash Maps are homogenous (all keys must have same type, and all values must have same type).\u003c/li\u003e\n\u003cli\u003eUse \u003ccode\u003estd::collections::HashMap\u003c/code\u003e to bring \u003ccode\u003eHashMap\u003c/code\u003e to scope.\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003e\u003cstrong\u003eCreating a New Hash Map\u003c/strong\u003e\u003c/p\u003e","title":"Common Collections (Vector and Strings) in Rust [Notes]"},{"content":"Hi everyone, so this week has been comparatively more productive in terms of learning as well as work! I\u0026rsquo;m happy, so why not share with everyone as well? \u0026#x1f389;\nPyTorch: (ft. Quansight and Facebook)\nStarted working on porting index_add to structured kernels, see the PR I made on my forked repo here, and on upstream here. This included adding an out= variant to the op. Refining the way it\u0026rsquo;s registered in PyTorch. I am thinking to pass defalut value while registering, but it will be a BC breaking change. Finally got to use ghstack, thanks to Yukio (Quansight). Revised derivatives yaml file. Personally I feel that this one needs opinions from the Facebook team, and a lot of changes might be rejected (which is okay, at the end of the day - everything we do should be good for the library). My PR here is more like a prototype for everyone to get a chance to review, as well as comment on what they feel. Structured Kernel porting PR for baddbmm, bmm has been merged. Yay! \u0026#x1f389; \u0026#x2764;\u0026#xfe0f; PR. Took a walkthrough of lots of autogenerated code in PyTorch, to understand how ops are registered. Listened to Ed\u0026rsquo;s podcast - episode on NVIDIA GPUs. A sweet little introduction to NVIDIA GPUs. Extras:\nContinued reading history of JIT, but couldn\u0026rsquo;t complete it. :sad: Will have to continue reading the next week, find the link here. Effective Modern C++: I finished reading Item 14 of Chapter 3, but couldn\u0026rsquo;t write notes about it. This blog post, I\u0026rsquo;m planning to discuss the intermediate generated code (closer to assembly), so let\u0026rsquo;s see how that goes the next week. Learnt a bit about compilers, started reading a book on it. Wrote a shell script in case I distro hop a lot, will link on this later once I\u0026rsquo;ve uploaded it. Started going through NumPy\u0026rsquo;s source code, just something I\u0026rsquo;m doing for learning. Reading about memory overlaps in C. BuffetCodes organization was dissolved recently, and the repositories were transferred. Thanks to my friend (Himanshu Singh) on working with me on this, it was a great experience. I\u0026rsquo;m continuously thinking if writing this is anyway helpful for anyone else, so yes - please do let me know if you like reading these. However, I\u0026rsquo;m enjoying learning and I think that\u0026rsquo;s all that matters. I plan to start new projects very soon, and hopefully that should go good.\n","permalink":"https://krshrimali.github.io/posts/2021/10/weekly-progress-report-03-10-2021-2/","summary":"\u003cp\u003eHi everyone, so this week has been comparatively more productive in terms of learning as well as work! I\u0026rsquo;m happy, so why not share with everyone as well? \u0026#x1f389;\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003ePyTorch\u003c/strong\u003e: (ft. Quansight and Facebook)\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eStarted working on porting \u003ccode\u003eindex_add\u003c/code\u003e to structured kernels, see the PR I made on my forked repo \u003ca href=\"https://github.com/krshrimali/pytorch/pull/14\"\u003ehere\u003c/a\u003e, and on upstream \u003ca href=\"https://github.com/pytorch/pytorch/pull/65993\"\u003ehere\u003c/a\u003e.\n\u003cul\u003e\n\u003cli\u003eThis included adding an \u003ccode\u003eout=\u003c/code\u003e variant to the op.\u003c/li\u003e\n\u003cli\u003eRefining the way it\u0026rsquo;s registered in PyTorch. I am thinking to pass defalut value while registering, but it will be a BC breaking change.\u003c/li\u003e\n\u003cli\u003eFinally got to use \u003ca href=\"https://github.com/ezyang/ghstack.git\"\u003eghstack\u003c/a\u003e, thanks to \u003ca href=\"https://github.com/ysiraichi\"\u003eYukio (Quansight)\u003c/a\u003e.\u003c/li\u003e\n\u003cli\u003eRevised derivatives yaml file. Personally I feel that this one needs opinions from the Facebook team, and a lot of changes might be rejected (which is okay, at the end of the day - everything we do should be \u003cem\u003egood\u003c/em\u003e for the library).\u003c/li\u003e\n\u003cli\u003eMy PR \u003ca href=\"https://github.com/pytorch/pytorch/pull/65993\"\u003ehere\u003c/a\u003e is more like a prototype for everyone to get a chance to review, as well as comment on what they feel.\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003eStructured Kernel porting PR for \u003ccode\u003ebaddbmm, bmm\u003c/code\u003e has been merged. Yay! \u0026#x1f389; \u0026#x2764;\u0026#xfe0f; \u003ca href=\"https://github.com/pytorch/pytorch/pull/65993\"\u003ePR\u003c/a\u003e.\u003c/li\u003e\n\u003cli\u003eTook a walkthrough of lots of autogenerated code in PyTorch, to understand how ops are registered.\u003c/li\u003e\n\u003cli\u003eListened to \u003ca href=\"https://open.spotify.com/show/6UzHKeiy368jKfQMKKvJY5\"\u003eEd\u0026rsquo;s podcast\u003c/a\u003e - episode on NVIDIA GPUs. A sweet little introduction to NVIDIA GPUs.\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003e\u003cstrong\u003eExtras\u003c/strong\u003e:\u003c/p\u003e","title":"Weekly Progress Report: 03-10-2021, 2"},{"content":"Hi everyone, before I go ahead and share my progress, I wanted to quickly talk about what this blog is about.\nI am highly passionate with the idea of high performance computing, optimizing deep learning applications, and solving real world problems using deep learning, computer vision, and speech processing. While I\u0026rsquo;m on this path - I would like to document this somewhere. And while I\u0026rsquo;m doing that, why not share it publicly?\nSo here it is, the first weekly update blog on what I\u0026rsquo;ve been upto:\nPyTorch: (ft. Quansight and Facebook)\nFollowed up on batch_norm PR. Merged \u0026#x1f389; Structured Kernel porting for baddbmm, bmm PR. Lots of interesting discussions, and should be approved after final round of review. Will have to follow up on this coming week. Structured Kernel porting (WIP) for nonzero PR. Putting this on hold, as there is a related PR on this - and it makes more sense to let that one go first, and then start working on this. Finished the max_poolNd PR, the PR is approved and will be merged after some time. While working on structured kernel for baddbmm, bmm PR, we needed to differentiate between in-place and out calls, interestingly - the autogenerated set_output function was the answer. A good discussion on this is here. Took a look at implementation of ArrayRef class, find the source code here. Extras:\nThe concept behind JIT is very interesting, and I wanted to read more about it. Started with this article on \u0026ldquo;A Brief History of Just-In-Time\u0026rdquo;. I plan to write a blog on this once I\u0026rsquo;m done reading. Effective Modern C++: I continued reading through Item 12 and Item 13 of Chapter 3: Declaring Overriding Functions override Prefer const_iterators to iterators A very interesting blog on vmap and pmap in Jax is here. Read half of it, will continue reading in the next week. An interesting watch on When do I use a union in C or C++, instead of a struct?. I like watching such videos when I\u0026rsquo;m a little tired from work. Blog theme: Worked on a new theme inspired by this theme. Re-organized the blog, fixed all the links (wherever it\u0026rsquo;s shared on the internet), in-blog images and cover images as well. Check the milestone here. With this blog, the milestone for the blog will be closed. \u0026#x1f389; Started reading tutorial on PyTorch\u0026rsquo;s official docs about Returning a Dispatched Operator in C++. Half way through, will continue reading in the next week. Contribution to pystiche, attempting to suppress deprecation warnings produced by pystiche module. PR, a few review comments to address otherwise should look good. Yes, and apart from all of this, I did won a few games of CSGO with my friends, had a few health issues to catch up with (\u0026#x1f622;), updated my discord channel with blog announcements, helped a few friends with their doubts, and well, yeah that\u0026rsquo;s it.\nI hope to perform better once I feel good (w.r.t health). Thank you for reading.\n","permalink":"https://krshrimali.github.io/posts/2021/09/weekly-progress-report-26-09-2021-1/","summary":"\u003cp\u003eHi everyone, before I go ahead and share my progress, I wanted to quickly talk about what this blog is about.\u003c/p\u003e\n\u003cp\u003eI am highly passionate with the idea of high performance computing, optimizing deep learning applications, and solving real world problems using deep learning, computer vision, and speech processing. While I\u0026rsquo;m on this path - I would like to document this somewhere. And while I\u0026rsquo;m doing that, why not share it publicly?\u003c/p\u003e","title":"Weekly Progress Report: 26-09-2021, 1"},{"content":" NOTE\nMy notes on Chapter 3, Item 13 of Effective Modern C++ written by Scott Meyers.\nSome (or even all) of the text can be similar to what you see in the book, as these are notes: I\u0026rsquo;ve tried not to be unnecessarily creative with my words. :)\nIn C++, iterators come handy to point at memory addresses of STL containers. For example,\n// C++11 std::vector\u0026lt;int\u0026gt; x {11, 9, 23, 6}; // begin() member function returns an iterator, which points to the first // memory address of the container x std::vector\u0026lt;int\u0026gt;::iterator it = x.begin(); While the general practice is to use const whenever possible, but programmers tend to use whenever it\u0026rsquo;s practical. const_iterators is particularly suggested when you want to use iterators, but you don\u0026rsquo;t need to modify what it points to.\nTake an example of the code snippet above, you have the iterator it pointing to the beginning position of the container, and if you want to insert a value before this, you just need to pass this iterator to x.insert() along with the value, right? You don\u0026rsquo;t need to modify the iterator. In such cases, const_iterator will be a better choice.\nBut, this is all for C++-11 and above. In C++-98, const_iterator wasn\u0026rsquo;t just ready:\n// C++-98 // Without using const_iterator // C++-98, initializing a vector wasn\u0026#39;t that trivial as it is now const int values[] = {11, 23, 6}; std::vector\u0026lt;int\u0026gt; x (values, values+3); // Want to insert 9 before 23 here std::vector\u0026lt;int\u0026gt;::iterator it = std::find(x.begin(), x.end(), 23); x.insert(it, 9); While this will work in C++-98, but since we don\u0026rsquo;t modify the iterator it, using const_iterators is a better choice:\n// C++-98: this won\u0026#39;t compile // Not using aliases here, C++-98 didn\u0026#39;t have the support for aliases // (was introduced in C++-11) // Using these typedefs makes it easier to reuse them typedef std::vector\u0026lt;int\u0026gt;::iterator it; typedef std::vector\u0026lt;int\u0026gt;::const_iterator c_it; // C++-98, initializing a vector wasn\u0026#39;t that trivial as it is now const int values[] = {11, 23, 6}; std::vector\u0026lt;int\u0026gt; x (values, values+3); c_it ci = std::find(static_cast\u0026lt;c_it\u0026gt;(x.begin()), static_cast\u0026lt;c_it\u0026gt;(x.end()), 23); x.insert(static_cast\u0026lt;it\u0026gt;(ci), 9); If you compile the code snippet above using C++-98 (if you are using g++, use: g++ \u0026lt;filename\u0026gt;.cpp -std=c++-98), you will hopefully get an error similar to:\nmain.cpp:10:14: error: no matching conversion for static_cast from \u0026#39;c_it\u0026#39; (aka \u0026#39;__wrap_iter\u0026lt;const int *\u0026gt;\u0026#39;) to \u0026#39;it\u0026#39; (aka \u0026#39;__wrap_iter\u0026lt;int *\u0026gt;\u0026#39;) x.insert(static_cast\u0026lt;it\u0026gt;(ci), 9); ^~~~~~~~~~~~~~~~~~~ As you can probably notice, it says \u0026ldquo;no matching coversion for static_cast from c_it to it\u0026rdquo;, that means in C++-98 there was no straight forward way to cast const_iterator to iterator. The author does point that there are some reall non-trivial ways to get around, but those are out of the scope of this blog and the book as well.\nA few observations in the snippet above:\nIn std::find we cast x.begin() iterators to const_iterator (c_it) because in C++-98, there was no simple way to get a const_iterator from a non-const container. The reason we cast our const_iterator (ci) back to iterator in the call x.insert() is: in C++-98, locations for insertions (and erasures) could only be specified by iterators, not const iterators. In fact, in C++-11, there is no simple way to convert const_iterator to iterator.\nHere comes C++-11 now, they introduced cbegin() which returns a const_iterator instead, similarly cend() member function exists.\n// In C++-11, initializing of a vector got easier std::vector\u0026lt;int\u0026gt; x {11, 23, 6}; // could have used auto here, but just typing the type for understanding // Notice the use of cbegin(), cend() instead of static_cast\u0026lt;c_it\u0026gt;(x.begin()), ... std::vector\u0026lt;int\u0026gt;::const_iterator ci = std::find(x.cbegin(), x.cend(), 23); x.insert(ci, 11); This will compile successfully, make sure to pass -std=c++11 flag while compiling if you are using g++ to make sure you are using C++-11.\nIt\u0026rsquo;s near this time, isn\u0026rsquo;t it?\nOnly place where C++-11 comes up a bit short is, when you want to use non-member functions (in case you are writing a generic code for your library, see example below):\n// This is a generic function which will find targetVal in a container // Use of decltype discussed before template\u0026lt;typename C, typename V\u0026gt; auto find_value(C\u0026amp; container, const V\u0026amp; targetVal) -\u0026gt; decltype(std::begin(container)) { // Notice the use of non-member cbegin and cend functions here // Note: member functions would have looked like: container.cbegin(), container.cend() auto it = std::find(std::cbegin(container), std::cend(container), targetVal); return it; } If you try to compile this using C++-11 -std=c++11 flag, this will fail with an error like:\nmain.cpp:6:25: error: no matching function for call to \u0026#39;begin\u0026#39; auto it = std::find(std::cbegin(container), std::cend(container), targetVal); main.cpp:6:49: error: no matching function for call to \u0026#39;end\u0026#39; auto it = std::find(std::cbegin(container), std::cend(container), targetVal); In C++-11, they failed to add cbegin, cend, rbegin, rend, crbegin, crend as member functions (but begin, end were added). This was later rectified in C++-14, so if you compile the code above using -std=c++14, this won\u0026rsquo;t fail anymore.\nIf you are using C++-11, you can get around by defining a cbegin() function:\ntemplate\u0026lt;class C\u0026gt; auto cbegin(const C\u0026amp; container) -\u0026gt; decltype(std::begin(container)) { return std::begin(container); } The reason using std::begin works here, because in C++-11, if you pass a reference to const version of the container to std::begin, it will return a const_iterator (notice that we pass const C\u0026amp; container in the arguments).\nThe whole gist is to use const_iterator whenever possible. C++-14 tidied up the usage, however C++-11 added required support and features.\nThank you for reading!\n","permalink":"https://krshrimali.github.io/posts/2021/09/prefer-const_iterators-to-iterators-notes/","summary":"\u003chr\u003e\n\u003cp\u003e\u003cstrong\u003eNOTE\u003c/strong\u003e\u003c/p\u003e\n\u003cp\u003eMy notes on Chapter 3, Item 13 of Effective Modern C++ written by Scott Meyers.\u003c/p\u003e\n\u003cp\u003eSome (or even all) of the text can be similar to what you see in the book, as these are notes: I\u0026rsquo;ve tried not to be unnecessarily creative with my words. :)\u003c/p\u003e\n\u003chr\u003e\n\u003cp\u003e\u003cimg loading=\"lazy\" src=\"https://raw.githubusercontent.com/krshrimali/blog/main/assets/cover-images/prefer-const-iterators-to-iterators.png\"\u003e\u003c/p\u003e\n\u003cp\u003eIn C++, iterators come handy to point at memory addresses of STL containers. For example,\u003c/p\u003e\n\u003cdiv class=\"highlight\"\u003e\u003cpre tabindex=\"0\" style=\"color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;\"\u003e\u003ccode class=\"language-cpp\" data-lang=\"cpp\"\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#75715e\"\u003e// C++11\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#75715e\"\u003e\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003estd\u003cspan style=\"color:#f92672\"\u003e::\u003c/span\u003evector\u003cspan style=\"color:#f92672\"\u003e\u0026lt;\u003c/span\u003e\u003cspan style=\"color:#66d9ef\"\u003eint\u003c/span\u003e\u003cspan style=\"color:#f92672\"\u003e\u0026gt;\u003c/span\u003e x {\u003cspan style=\"color:#ae81ff\"\u003e11\u003c/span\u003e, \u003cspan style=\"color:#ae81ff\"\u003e9\u003c/span\u003e, \u003cspan style=\"color:#ae81ff\"\u003e23\u003c/span\u003e, \u003cspan style=\"color:#ae81ff\"\u003e6\u003c/span\u003e};\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#75715e\"\u003e// begin() member function returns an iterator, which points to the first\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#75715e\"\u003e// memory address of the container x\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#75715e\"\u003e\u003c/span\u003estd\u003cspan style=\"color:#f92672\"\u003e::\u003c/span\u003evector\u003cspan style=\"color:#f92672\"\u003e\u0026lt;\u003c/span\u003e\u003cspan style=\"color:#66d9ef\"\u003eint\u003c/span\u003e\u003cspan style=\"color:#f92672\"\u003e\u0026gt;::\u003c/span\u003eiterator it \u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e x.begin();\n\u003c/span\u003e\u003c/span\u003e\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003cp\u003eWhile the general practice is to use \u003ccode\u003econst\u003c/code\u003e whenever possible, but programmers tend to use whenever it\u0026rsquo;s \u003cem\u003epractical\u003c/em\u003e. \u003ccode\u003econst_iterators\u003c/code\u003e is particularly suggested when you want to use iterators, but you don\u0026rsquo;t need to modify what it points to.\u003c/p\u003e","title":"Prefer const_iterators to iterators (Notes)"},{"content":" NOTE\nMy notes on Chapter 3, Item 12 of Effective Modern C++ written by Scott Meyers.\nSome (or even all) of the text can be similar to what you see in the book, as these are notes: I\u0026rsquo;ve tried not to be unnecessarily creative with my words. :)\nOverriding != Overloading\nExample of virtual function overriding:\n// Base class class Base { public: virtual void doWork(); // ... }; // Derived class from Base class Derived: public Base { public: // virtual is optional // this will \u0026#34;override\u0026#34; Base::doWork virtual void doWork(); // ... }; // This creates a \u0026#34;Base\u0026#34; class pointer to \u0026#34;Derived\u0026#34; class object std::unique_ptr\u0026lt;Base\u0026gt; upb = std::make_unique\u0026lt;Derived\u0026gt;(); // Derived doWork() function is invoked upb-\u0026gt;doWork(); This is how virtual function overriding allows to invoke a \u0026ldquo;derived class function\u0026rdquo; from a base class interface.\nRequirements for overriding:\nBase class function must be virtual. Base and derived function names must be identical (except for destructors). Parameter types of base and derived functions must be identical. Constness of both base and derived functions must be identical. The return types and exception specifications of the base and derived functions must be compatible. [from C++11] Both functions should have identical reference qualifiers (see below) Reference qualifiers were new to me, but the book gives a really good example:\n// Useful when you want to limit the calls to lvalues or rvalues only class Widget { public: // ... void doWork() \u0026amp;; // Only when *this is an lvalue void doWork() \u0026amp;\u0026amp;; // Only when *this is an rvalue // ... }; // Suppose there is a makeWidget() function that returns an instance of Widget class, this will be an rvalue here Widget makeWidget(); // lvalue Widget w; // this will call void doWork() \u0026amp;; w.doWork(); // this will call void doWork() \u0026amp;\u0026amp;; makeWidget().doWork(); In case the functions don\u0026rsquo;t have identical reference qualifiers, the derived class will still have the function, but it won\u0026rsquo;t override the base class function.\nBecause of a lot of conditions (see 6 requirements above), it\u0026rsquo;s easy to make mistakes or forget a few things while overriding a function. And the book states that it\u0026rsquo;s not worth expecting from compiler to report an error, because the code can still be valid - it\u0026rsquo;s just you expected it to override, but it didn\u0026rsquo;t.\nSee an example below:\nclass Base { public: virtual void mf1() const; virtual void mf2(int x); virtual void mf3() \u0026amp;; void mf4() const; }; class Derived: public Base { public: virtual void mf1(); virtual void mf2(unsigned int x); virtual void mf3() \u0026amp;\u0026amp;; void mf4() const; }; None of the functions in Derived class will override functions in the Base class. Why?\nmf1(): declared const in Base but not in Derived. mf2(): arg int in Base but unsigned int in Derived. mf3(): lvalue-qualified in Base but rvalue-qualified in Derived. mf4(): isn\u0026rsquo;t declared virtual in Base. To avoid worrying about human mistakes, it\u0026rsquo;s better to let the compiler know explicitly that these functions are expected to override. For that, declare the functions in Derived class override.\nclass Derived: public Base { public: virtual void mf1() override; virtual void mf2(unsigned int x) override; virtual void mf3() \u0026amp;\u0026amp; override; virtual void mf4() const override; }; Now, this won\u0026rsquo;t compile for sure no matter what compiler you are using.\nC++-11 introduced 2 contextual keywords: final (see note below), override. These keywords are reserved, but only in certain contexts.\nNote: In case you want to prevent a function in a base class to be overridden in derived classes, you can declare them final. final can also be declared for a class, in which case it won\u0026rsquo;t be allowed to be used a base class.\nFor override: In case your old (legacy) code uses override somewhere else, it\u0026rsquo;s fine - keep it! Only when it\u0026rsquo;s used at the end of a member function declaration, then it\u0026rsquo;s reserved.\nclass Sample { public: // legal void override(); } The chapter (item) ends with a brief about lvalue and rvalue reference member functions, which I would like to cover in another blog. For this blog, I think this should be good! :)\nThank you for reading.\n","permalink":"https://krshrimali.github.io/posts/2021/09/declaring-overriding-functions-override-notes/","summary":"\u003chr\u003e\n\u003cp\u003e\u003cstrong\u003eNOTE\u003c/strong\u003e\u003c/p\u003e\n\u003cp\u003eMy notes on Chapter 3, Item 12 of Effective Modern C++ written by Scott Meyers.\u003c/p\u003e\n\u003cp\u003eSome (or even all) of the text can be similar to what you see in the book, as these are notes: I\u0026rsquo;ve tried not to be unnecessarily creative with my words. :)\u003c/p\u003e\n\u003chr\u003e\n\u003cp\u003e\u003cimg loading=\"lazy\" src=\"https://raw.githubusercontent.com/krshrimali/blog/main/assets/cover-images/declaring-overriding-functions-override.png\"\u003e\u003c/p\u003e\n\u003cp\u003eOverriding != Overloading\u003c/p\u003e\n\u003cp\u003e\u003cem\u003eExample\u003c/em\u003e of virtual function overriding:\u003c/p\u003e\n\u003cdiv class=\"highlight\"\u003e\u003cpre tabindex=\"0\" style=\"color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;\"\u003e\u003ccode class=\"language-cpp\" data-lang=\"cpp\"\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#75715e\"\u003e// Base class\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#75715e\"\u003e\u003c/span\u003e\u003cspan style=\"color:#66d9ef\"\u003eclass\u003c/span\u003e \u003cspan style=\"color:#a6e22e\"\u003eBase\u003c/span\u003e {\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#66d9ef\"\u003epublic\u003c/span\u003e\u003cspan style=\"color:#f92672\"\u003e:\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e    \u003cspan style=\"color:#66d9ef\"\u003evirtual\u003c/span\u003e \u003cspan style=\"color:#66d9ef\"\u003evoid\u003c/span\u003e doWork();\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e    \u003cspan style=\"color:#75715e\"\u003e// ...\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#75715e\"\u003e\u003c/span\u003e};\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#75715e\"\u003e// Derived class from Base\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#75715e\"\u003e\u003c/span\u003e\u003cspan style=\"color:#66d9ef\"\u003eclass\u003c/span\u003e \u003cspan style=\"color:#a6e22e\"\u003eDerived\u003c/span\u003e\u003cspan style=\"color:#f92672\"\u003e:\u003c/span\u003e \u003cspan style=\"color:#66d9ef\"\u003epublic\u003c/span\u003e Base {\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#66d9ef\"\u003epublic\u003c/span\u003e\u003cspan style=\"color:#f92672\"\u003e:\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e    \u003cspan style=\"color:#75715e\"\u003e// virtual is optional\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#75715e\"\u003e\u003c/span\u003e    \u003cspan style=\"color:#75715e\"\u003e// this will \u0026#34;override\u0026#34; Base::doWork\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#75715e\"\u003e\u003c/span\u003e    \u003cspan style=\"color:#66d9ef\"\u003evirtual\u003c/span\u003e \u003cspan style=\"color:#66d9ef\"\u003evoid\u003c/span\u003e doWork();\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e    \u003cspan style=\"color:#75715e\"\u003e// ...\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#75715e\"\u003e\u003c/span\u003e};\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#75715e\"\u003e// This creates a \u0026#34;Base\u0026#34; class pointer to \u0026#34;Derived\u0026#34; class object\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#75715e\"\u003e\u003c/span\u003estd\u003cspan style=\"color:#f92672\"\u003e::\u003c/span\u003eunique_ptr\u003cspan style=\"color:#f92672\"\u003e\u0026lt;\u003c/span\u003eBase\u003cspan style=\"color:#f92672\"\u003e\u0026gt;\u003c/span\u003e upb \u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e std\u003cspan style=\"color:#f92672\"\u003e::\u003c/span\u003emake_unique\u003cspan style=\"color:#f92672\"\u003e\u0026lt;\u003c/span\u003eDerived\u003cspan style=\"color:#f92672\"\u003e\u0026gt;\u003c/span\u003e();\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#75715e\"\u003e// Derived doWork() function is invoked\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#75715e\"\u003e\u003c/span\u003eupb\u003cspan style=\"color:#f92672\"\u003e-\u0026gt;\u003c/span\u003edoWork();\n\u003c/span\u003e\u003c/span\u003e\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003cp\u003eThis is how virtual function overriding allows to invoke a \u0026ldquo;derived class function\u0026rdquo; from a base class interface.\u003c/p\u003e","title":"Declaring Overriding Functions override (Notes)"},{"content":"Prefer deleted functions to private undefined ones This item (11) in the chapter 3 focuses on:\nWhy and How to prevent users calling particular functions? C++-98 and C++-11 approach What\u0026rsquo;s the difference between deleting a function vs declaring a member function private (and not defining them)? NOTE\nThese are my notes on Chapter 3, Item 11 of Effective Modern C++ written by Scott Meyers.\nSome (or even all) of the text can be similar to what you see in the book, as these are notes: I\u0026rsquo;ve tried not to be unnecessarily creative with my words. :)\nWhen is it required to delete a function/not define a private member function? Problem:\nCases when you don\u0026rsquo;t want the client to call a particular function. Solution:\nJust don\u0026rsquo;t declare the function But\u0026hellip;doesn\u0026rsquo;t work always:\nCase: Special member functions generated by C++ automatically, discussed later). Examples considered in this blog:\nCopy Constructor Copy Assignment Operator C++-98 Approach:\nDeclare these functions private and don\u0026rsquo;t define them.\nExample:\nAll istream and ostream objects inherit (possibly) from basic_ios class in the C++ Standard Library. Copying these objects is undersirable. Why is copying objects of istream and ostream undesirable? [Also see this question on stackoverflow]\nistream object: represents stream of input values. some might have been read before. and some may be read later. If you copy istream object: Will that copy those values which have been read before? Or will also copy values which are to be read later? Hence, it\u0026rsquo;s just better to not allow copying istream or ostream objects.\nIn C++-98: Reminder (from above): All istream and ostream objects inherit from basic_ios class (possibly) in the C++ standard, and the basic_ios class in C++-98 looks something like this:\ntemplate \u0026lt;class charT, class traits = char_traits\u0026lt;T\u0026gt; \u0026gt; class basic_ios : public ios_base { public: // ... // Declaring the copy constructor and copy assignment operator private prohibits clients from calling them private: basic_ios(const basic_ios\u0026amp;); // not defined basic_ios\u0026amp; operator=(const basic_ios\u0026amp;); // not defined }; Note that:\nbasic_ios(const basic_ios\u0026amp;) is the copy constructor basic_ios\u0026amp; operator=(const basic_ios\u0026amp;) is the copy assignment operator (and both are private).\nHow does not defining these functions help?\nConsider a case where a friend class or member functions try accessing these functions, then linking will fail because of missing function definitions. In C++-11 In C++-11, the above can be done using = delete to mark the copy constructor and the copy assignment operator as deleted functions.\ntemplate \u0026lt;class charT, class traits = char_traits\u0026lt;charT\u0026gt; \u0026gt; class basic_ios : public ios_base { public: // ... basic_ios(const basic_ios\u0026amp; ) = delete; // deleted function basic_ios\u0026amp; operator=(const basic_ios\u0026amp;) = delete; // deleted function // ... }; It\u0026rsquo;s a convention to declare deleted functions public, but why? Better error messages.\nIn case you declare your deleted functions private, some compilers will probably complain about the function being private and can hide the error message of it not being usable (because it being deleted). Hence, it\u0026rsquo;s a good practice to make them public:\nFrom my experience though, Apple\u0026rsquo;s clang compiler (v 12.0.5) doesn\u0026rsquo;t complain about it being private, but then - it can vary from compiler to compiler, so better to play safe. Here is an example of how the error message looks like:\n#include \u0026lt;iostream\u0026gt; class Sample { public: Sample(int x) : x(x) { }; // Copy constructor has been deleted, so should not be callable Sample(const Sample\u0026amp;) = delete; void trying_copy_construct(Sample s) { // This function tries to use a copy constructor // This should fail Sample new_object(s); } private: int x; }; Compiling the above code fails with the following error:\nmain.cpp:10:16: error: call to deleted constructor of \u0026#39;Sample\u0026#39; Sample new_object(s); ^ ~ main.cpp:6:5: note: \u0026#39;Sample\u0026#39; has been explicitly marked deleted here Sample(const Sample\u0026amp;) = delete; Difference b/w using delete vs declaring private Note: There is more to it except the good practice reasoning.\nUsing a deleted function in a member function or by a friend class won\u0026rsquo;t even compile the code if it tries to copy basic_ios objects while declaring private will compile successfully but fail during link-time. Conventionally deleted functions are declared public, not private while the C++-98 way requires the functions to be declared private (see the section above for reasoning). Only member functions can be private, while any function can be deleted (so you can delete some overloads for your function, in case you don\u0026rsquo;t want it to accept certain type inputs). Let\u0026rsquo;s discuss the last point in detail:\nConsider the case where you have:\ntemplate\u0026lt;typename T\u0026gt; void processPointer(T* ptr); And:\nYou need a template that works with built-in pointers. You want to reject calls with void* and char* pointers (more on this later, these deserve special handling at times). Ideal way? Delete these instantiations (with void* or char* input pointers). template\u0026lt;\u0026gt; void processPointer\u0026lt;void\u0026gt;(void*) = delete; template\u0026lt;\u0026gt; void processPointer\u0026lt;char\u0026gt;(char*) = delete; But, with C++-98 way, it\u0026rsquo;s not possible within the class scope. That is, you can not give template specialization to your member function within the class scope (it should be done in the namespace scope):\nclass Sample { public: // ... template \u0026lt;typename T\u0026gt; void processPointer(T* ptr) { ... } private: // ... // This is template specialization inside the scope of the class - not allowed template \u0026lt;\u0026gt; void processPointer\u0026lt;void\u0026gt;(void*); // error }; While with the C++-11 way, you can delete function outside the class scope:\nclass Sample { public: // ... template \u0026lt;typename T\u0026gt; void processPointer(T* ptr) { ... } // ... }; template \u0026lt;\u0026gt; void Sample::processPointer\u0026lt;void\u0026gt;(void*) = delete; // in public scope, and deleted! I hope you liked this blog, thank you for reading! :)\n","permalink":"https://krshrimali.github.io/posts/2021/08/prefer-deleted-functions-to-private-undefined-ones-notes/","summary":"\u003ch2 id=\"prefer-deleted-functions-to-private-undefined-ones\"\u003ePrefer deleted functions to private undefined ones\u003c/h2\u003e\n\u003cp\u003eThis item (11) in the chapter 3 focuses on:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eWhy and How to prevent users calling particular functions?\u003c/li\u003e\n\u003cli\u003eC++-98 and C++-11 approach\u003c/li\u003e\n\u003cli\u003eWhat\u0026rsquo;s the difference between deleting a function vs declaring a member function private (and not defining them)?\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003e\u003cimg loading=\"lazy\" src=\"https://raw.githubusercontent.com/krshrimali/blog/main/assets/cover-images/Item-11-Notes.png\"\u003e\u003c/p\u003e\n\u003chr\u003e\n\u003cp\u003e\u003cstrong\u003eNOTE\u003c/strong\u003e\u003c/p\u003e\n\u003cp\u003eThese are my notes on Chapter 3, Item 11 of Effective Modern C++ written by Scott Meyers.\u003c/p\u003e\n\u003cp\u003eSome (or even all) of the text can be similar to what you see in the book, as these are notes: I\u0026rsquo;ve tried not to be unnecessarily creative with my words. :)\u003c/p\u003e","title":"Prefer Deleted Functions to Private Undefined Ones (Notes)"},{"content":"Scoped vs Unscoped Enums General rule: declaring a name inside curly braces is limited to that scope. Exception: C++-98 style Enums NOTE\nMy notes on Chapter 3, Item 10 of Effective Modern C++ written by Scott Meyers.\nSome (or even all) of the text can be similar to what you see in the book, as these are notes: I\u0026rsquo;ve tried not to be unnecessarily creative with my words. :)\n// You can\u0026#39;t declare black, white, red in the scope containing the enum Color enum Color { black, white, red; }; auto white = false; // error: white already declared in this scope Unscoped Enums have implicit type conversions for their enumerators. Enumerators can implicitly convert to integral types, and then to floating-point types. // Assume Color is declared like above Color c = red; // valid since Enumerator white is leaked to the scope Color is in if (c \u0026lt; 10) { // valid, implicit conversion // ... do something } else if (c \u0026lt; 10.5) { // also valid, implicit conversion // ... do something } The C++-98 Style Enums are termed as Unscoped Enums (because of leaking names).\nC++-11 Scoped Enums:\n// black, white, red are now scoped to Color Enum enum class Color { black, white, red; }; // This is now valid auto white = false; Separately, if you do: (consider Color Enum has already been declared)\nColor c = white; // error: no enumerator named \u0026#34;white\u0026#34; is in this scope Color c = Color::white; // valid auto c = Color::white; // valid Also referred as enum classes (because declared using enum class). Enumerators in scoped Enums are strongly typed (no implicit type conversion) // Assume Color is declared as above using enum class Color c = Color::red; if (c \u0026lt; 10.5) { // Error! can\u0026#39;t compare Color and double // do something... } Note: you can do explicit casting using cast. Note about enums in C++: * Every enum in C++ has an integral underlying type that is determined by compilers. * Compilers need to know the size of enum before using it.\nC++98 vs C++11 on Enums C++98:\nUnscoped enums can not be forward-declared. Hence only enums with definitions are supported. Allows compilers to choose underlying type for each enum prior to the enum being used. Drawbacks? Increase in compilation dependencies: wherever the enum is used, even if not affected by any addition in the enum, it will be recompiled (generally speaking, that is without any tweaks/optimizations). C++11:\nBoth unscoped and scoped enums can be forward-declared. Unscoped enums will need a few efforts though: /* For Scoped Enums */ // Default underlying type is int enum class Status; // Override it enum class Status: std::uint32_t; /* For Unscoped Enums */ // There is no underlying type for unscoped enum // You can manually specify though enum Status: sd::uint32_t; These specifications for underlying types can also go on enum\u0026rsquo;s definitions.\nUnscoped Enums over Scoped Enums? Imagine when you have a code like this:\n// Ordered as: name, email, reputation using UserInfo = std::tuple\u0026lt;std::string, std::string, std::size_t\u0026gt;; UserInfo uInfo; // This is not clear to the reader, you can\u0026#39;t always remember what 1st indexed field in UserInfo is auto val = std::get\u0026lt;1\u0026gt;(uInfo); Using the property of intrinsic conversion in unscoped enums, you can solve this:\nenum InfoFields { uName, uEmail, uReputation }; // UserInfo defined as above UserInfo uInfo; // Implicit conversion of int (default underlying type of enums) to std::size_t (that\u0026#39;s what std::get takes) auto val = std::get\u0026lt;uEmail\u0026gt;(uInfo); For scoped enums though, you\u0026rsquo;ll have to use static_cast\u0026lt;std::size_t\u0026gt;(InfoFields::uEmail) instead of just uEmail (for unscoped enums) passed to std::get, which is less readable. But\u0026hellip;\nThis can be redued by using a custom function which: * takes: enum * returns: corresponding std::size_t value\nstd::get is a template, and the value needs to be understood during compilation only, so the function should be a constexpr (more on this later in the series).\nTo generalize, let\u0026rsquo;s keep the enum\u0026rsquo;s underlying type (std::underlying_type type trait)\n// Using noexcept because we know there\u0026#39;ll be no exceptions raised template \u0026lt;typename E\u0026gt; constexpr typename std::underlying_type\u0026lt;E\u0026gt;::type toUType(E enumerator) noexcept { return static_cast\u0026lt;typename std::underlying_type\u0026lt;E\u0026gt;::type\u0026gt;(enumerator); } From the previous blog, we know that in C++14, we could have simplified by writing:\n// Using noexcept because we know there\u0026#39;ll be no exceptions raised template \u0026lt;typename E\u0026gt; constexpr std::underlying_type_t\u0026lt;E\u0026gt; toUType(E enumerator) noexcept { return static_cast\u0026lt;std::underlying_type_t\u0026lt;E\u0026gt;\u0026gt;(enumerator); } Could have used auto for return type in C++14:\n// Using noexcept because we know there\u0026#39;ll be no exceptions raised template \u0026lt;typename E\u0026gt; constexpr auto toUType(E enumerator) noexcept { return static_cast\u0026lt;std::underlying_type_t\u0026lt;E\u0026gt;\u0026gt;(enumerator); } Now this can be used as:\n// Reminder, InfoFields was defined as: enum InfoFields { uName, uEmail, uReputation }; // toUType is defined above auto val = std::get\u0026lt;toUType(InfoFields::uEmail)\u0026gt;(uInfo); Good Reads Forward Declaration: Stackoverflow: https://stackoverflow.com/questions/4757565/what-are-forward-declarations-in-c Are Unscoped Enums still helpful? Stackoverflow: https://stackoverflow.com/questions/27320603/are-unscoped-enumerations-still-useful Proposal for forward declaration to enums (accepted), dated 2008: http://www.open-std.org/jtc1/sc22/wg21/docs/papers/2008/n2764.pdf Forward Declaring an Enum in C++? Stackoverflow: https://stackoverflow.com/questions/71416/forward-declaring-an-enum-in-c Acknowledgement (Reviews) Thanks to Kshitij Kalambarkar for helping in reviewing the blog. It\u0026rsquo;s always helpful to get another set of eyes to what you write. :)\nThat\u0026rsquo;s it for this blog, thank you for reading everyone!\n","permalink":"https://krshrimali.github.io/posts/2021/08/prefer-scoped-enums-over-unscoped-enums-notes/","summary":"\u003ch2 id=\"scoped-vs-unscoped-enums\"\u003eScoped vs Unscoped Enums\u003c/h2\u003e\n\u003cul\u003e\n\u003cli\u003e\u003cstrong\u003eGeneral rule:\u003c/strong\u003e declaring a name inside curly braces is limited to that scope.\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eException:\u003c/strong\u003e C++-98 style Enums\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003e\u003cimg loading=\"lazy\" src=\"https://raw.githubusercontent.com/krshrimali/blog/main/assets/cover-images/Unscoped-Scoped-Enums.png\"\u003e\u003c/p\u003e\n\u003chr\u003e\n\u003cp\u003e\u003cstrong\u003eNOTE\u003c/strong\u003e\u003c/p\u003e\n\u003cp\u003eMy notes on Chapter 3, Item 10 of Effective Modern C++ written by Scott Meyers.\u003c/p\u003e\n\u003cp\u003eSome (or even all) of the text can be similar to what you see in the book, as these are notes: I\u0026rsquo;ve tried not to be unnecessarily creative with my words. :)\u003c/p\u003e","title":"Prefer Scoped Enums over Unscoped Enums (Notes)"},{"content":"Hi Everyone, today I want to talk about Union Find Problem. This is going to be a series covering:\nUnion Find Problem (this blog) Solutions to Union Find (1): Quick Find Solutions to Union Find (2): Quick Union Solutions to Union Find (3): Weighted Quick Union Applications of Union Find (perculation and more) Cool project using Union Find Solving some competitive programming questions using Union Find Each blog will try to cover very basic concepts behind the topic, and also what it\u0026rsquo;s all about.\nUnion Find Problem: Definition Let\u0026rsquo;s define the problem first. It\u0026rsquo;s a problem where you need to find whether two points/objects are in a connected relationship (defined below) or not in a defined environment (where you know the relationships).\nConnection Relationship is an equivalence relation, which means:\nIt\u0026rsquo;s reflexive: a ~ a (a is connected to itself) It\u0026rsquo;s symmetric: a ~ b iff b ~ a (a is connected to b iff b is connected to a OR if a is connected to b, b is also connected to a) It\u0026rsquo;s transitive: if a ~ b and b ~c then a ~ c (if a is connected to b, and b is connected to c, then a is connected to c) And by connected, we just mean that there is a path between the two objects. My thinking around this problem is mostly surrounded by the plot of dynamic connectivity, where you want to find if there is a connection between 2 objects in a graph. These objects can be friends (whether A and B are friends or not in a circle - here circle is the environment).\nUnion Find: Problem, why study it? It\u0026rsquo;s a name to a problem, but you must have encountered this in real life. Whether you are a friend to your ex, oh definitely not ;) (even if Union Find solution finds a connection, trust me - move on :P). Okay, on a serious note now:\nUnion Find Problem is seen in lots of applications:\nPerculation (example: if you pour water on the top of a tank having lots of cells/blocks - some are open, some are closed - will it reach the bottom?). I also see this as an application where you want to find if the leakage in a whole network of oil pipes will exit or if it will be blocked. Dynamic Connectivity: A very simple definition would be, whether there is a connection between two objects? You can see it\u0026rsquo;s application in social media, whether two objects (I know I should use humans but the whole internet objectifies you ;), hence objects ;)). Whether there is a connection between two places in a nation or not? Games (will be discussed later) and more\u0026hellip; Now it\u0026rsquo;s indeed a very interesting problem, and in this blog, I\u0026rsquo;ll show you a very basic implementation which I wrote before studying the algorithms which attempt to solve this problem.\nUnion Find breakdown: Union and Find NOTE\nAll codes are written here in C++ and code is available here: https://github.com/krshrimali/Algorithms-All-In-One/.\nBreaking it down to two functions, is really helpful:\n// Object is an arbitrary type for now, can be an int, can be a user defined type as well void union(Object a, Object b) { // This function will connect two objects, if: // * they exist // * there isn\u0026#39;t a connection already // ... } Similarly, the find function will try to find whether there is a connection between two objects:\n// Object is an arbitrary type for now, can be an int, can be a user defined type as well bool find(Object a, Object b) { // returns true if: // * both objects exist // * and they are connected // else returns false // ... } Let\u0026rsquo;s try to setup the environment first, and we need to answer these two questions first:\nWhat should be the objects? Where are these objects stored? I like thinking of this as a graph (environment) and points as objects. So let\u0026rsquo;s start implementing.\nImplementation: Modelling NOTE\nThis is a very basic implementation and first try presenting a naive solution to the problem, we\u0026rsquo;ll discuss better algorithms in next blogs.\nThe very first question you should ask yourself is, what data structures should be used for Graph and Point(s)? The way I\u0026rsquo;m thinking of solving this is:\nEach Point will have (x, y) coordinates. (so coordinates will be it\u0026rsquo;s property) Whenever two points are merged (union is called), the first point will append the second point in it\u0026rsquo;s list of connections. So each Point object will have a connection list. (std::vector?) Whenever find is called, that is - there is an attempt to find if there is a connection between two points? We just need to search if second point is there in the first point\u0026rsquo;s connection list. If it is, then there is a connection. And if not, then no connection. In python, I would have used a dict, so I went ahead with std::map in C++, will help me not duplicating points. So the Graph will be a std::map of Point, std::vector\u0026lt;Point\u0026gt;, which will look something like this:\n// This is how graph will look like, in imagination // Example: // a is connected to b, c, d // b is connected to a (Point a, {Point b, Point c, Point d}), (Point b, {Point a}), ...so on As you can see, there will be a list mapped to each Point, we call that list: connection list.\nNow, the Point can simply be a struct having int x, y as coordinates.\nImplementation: Skeleton Let\u0026rsquo;s create the skeleton now:\nstruct Point { // x and y are the coordinates for each point int x, y; }; // Graph will contain Points, helper functions: union and merge class Graph { private: std::map\u0026lt;Point, std::vector\u0026lt;Point\u0026gt;\u0026gt; graph; public: // We take references to avoid internal copies, const is used since we don\u0026#39;t want these functions // to modify these points in any way void union_(const Point\u0026amp; a, const Point\u0026amp; b) { // Use find utility function of std::map if (this-\u0026gt;graph.find(a) == this-\u0026gt;graph.end()) { // Not found // Means create an entry in the graph, and add b to the connection list of a this-\u0026gt;graph[a] = {b}; } else { // Found // Append b to the connection list of a this-\u0026gt;graph.at(a).push_back(b); } } // Are a and b connected? OR Is there a path b/w a and b? bool find_(const Point\u0026amp; a, const Point\u0026amp; b) { // First check if there is an Point a in the graph if (this-\u0026gt;graph.find(a) == this-\u0026gt;graph.end()) { std::cout \u0026lt;\u0026lt; \u0026#34;Not found\\n\u0026#34;; return false; } else { // Object found std::vector\u0026lt;Point\u0026gt; connection_list = this-\u0026gt;graph.at(a); // Now find if b exists in the connection list, if yes then there is a connection if (std::find(connection_list.begin(), connection_list.end(), b) != connection_list.end()) { // b found return true; } return false; } } }; // Usage int main() { Graph g_sample; struct Point p(2, 3); struct Point q(3, 3); struct Point r(4, 4); // Add a connection for (p, q) and (p, r), for testing g_sample.union_(p, q); g_sample.union_(p, r); std::cout \u0026lt;\u0026lt; \u0026#34;Are p and q connected? Answer: \u0026#34; \u0026lt;\u0026lt; sampleGraph.find_(p, q) \u0026lt;\u0026lt; \u0026#39;\\n\u0026#39;; // Expected: true std::cout \u0026lt;\u0026lt; \u0026#34;Are p and r connected? Answer: \u0026#34; \u0026lt;\u0026lt; sampleGraph.find_(p, r) \u0026lt;\u0026lt; \u0026#39;\\n\u0026#39;; // Expected; true std::cout \u0026lt;\u0026lt; \u0026#34;Are q and r connected? Answer: \u0026#34; \u0026lt;\u0026lt; sampleGraph.find_(q, r) \u0026lt;\u0026lt; \u0026#39;\\n\u0026#39;; // Expected: false } Now this is a great start, I won\u0026rsquo;t spend time explaining the code as the comments should help. In case you have queries, please feel free to open an issue here.\nBut this won\u0026rsquo;t compile. And the reason is, that when you are using std::find with user-defined types like Point, you need to define \u0026lt; operator or give it a comparator because it does some comparisons. Think of this like:\nThe compiler isn\u0026rsquo;t aware of how to do: Point(2, 3) \u0026lt; Point(3, 3)\nBecause for the compiler, both of these are an object. So we need to tell it explicitly, that hey! when you do \u0026lt; operation on Point objects, check their coordinates.\nFinal Implementation The final code can be found here. There are a few TODOs in the code mentioned, and in case you want to pick them up, please create a PR for the same. :)\nThe code will change with time, so I\u0026rsquo;ll refrain copy-pasting it here.\nHomework? Let\u0026rsquo;s do this before I release the next blog:\nAnalyze the algorithm used here, it\u0026rsquo;s time and space complexity. Address the TODOs in the code. In case you are able to do this before my next blog, kudos to you! You might as well help creating a PR, that will be great.\nThank you for reading this blog. I hope you liked it! :)\n","permalink":"https://krshrimali.github.io/posts/2021/08/union-find-problem-and-a-naive-implementation-c-/","summary":"\u003cp\u003eHi Everyone, today I want to talk about Union Find Problem. This is going to be a series covering:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eUnion Find Problem (this blog)\u003c/li\u003e\n\u003cli\u003eSolutions to Union Find (1): Quick Find\u003c/li\u003e\n\u003cli\u003eSolutions to Union Find (2): Quick Union\u003c/li\u003e\n\u003cli\u003eSolutions to Union Find (3): Weighted Quick Union\u003c/li\u003e\n\u003cli\u003eApplications of Union Find (perculation and more)\u003c/li\u003e\n\u003cli\u003eCool project using Union Find\u003c/li\u003e\n\u003cli\u003eSolving some competitive programming questions using Union Find\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003e\u003cimg loading=\"lazy\" src=\"https://raw.githubusercontent.com/krshrimali/blog/main/assets/cover-images/Union-Find-Intro.png\"\u003e\u003c/p\u003e\n\u003cp\u003eEach blog will try to cover very basic concepts behind the topic, and also what it\u0026rsquo;s all about.\u003c/p\u003e","title":"Union Find Problem, and a naive implementation (C++)"},{"content":"One solution to avoiding using long type names:\n// So C++98 like typedef std::unique_ptr\u0026lt;std::unordered_map\u0026lt;std::string, std::string\u0026gt;\u0026gt; UPtrMapSS; NOTE\nMy notes on Chapter 3, Item 9 of Effective Modern C++ written by Scott Meyers.\nSome (or even all) of the text can be similar to what you see in the book, as these are notes: I\u0026rsquo;ve tried not to be unnecessarily creative with my words. :)\nC++11 also offers alias declarations:\nusing UPtrMapSS = typedef std::unique_ptr\u0026lt;std::unordered_map\u0026lt;std::string, std::string\u0026gt;\u0026gt;; Advantages of alias declarations over typedefs:\nFor types involving function pointers, aliases are easier to read (for some people):\n// typedef typedef void (*FP)(int, const std::string\u0026amp;); // alias declaration using FP = void (*)(int, const std::string\u0026amp;); Alias declarations can be templatized, but typedefs can not.\n// MyAlloc is a custom allocator template \u0026lt;typename T\u0026gt; using MyAllocList = std::list\u0026lt;T, MyAlloc\u0026lt;T\u0026gt;\u0026gt;; MyAllocList\u0026lt;Widget\u0026gt; lw; // will create std::list\u0026lt;Widget, MyAlloc\u0026lt;Widget\u0026gt;\u0026gt; vs: typedef, a hack way:\n// templatized struct here template \u0026lt;typename T\u0026gt; struct MyAllocList { typedef std::list\u0026lt;T, MyAlloc\u0026lt;T\u0026gt;\u0026gt; type; }; MyAllocList\u0026lt;Widget\u0026gt;::type lw; In case you want to use a type specified by template parameter, with typedefs it gets complex:\n// MyAllocList is defined as mentioned in 2nd point template \u0026lt;typename T\u0026gt; class Widget { private: typename MyAllocList\u0026lt;T\u0026gt;::type list; // ... }; Here MyAllocList\u0026lt;T\u0026gt;::type is now a dependent type (dependent on type T from template paramater). C++ Rule: need to use typename before name of a dependent type. With alias declaration of MyAllocList, no need to use typename and ::type:\ntemplate \u0026lt;typename T\u0026gt; class Widget { private: // no typename and ::type MyAllocList\u0026lt;T\u0026gt; list; // ... } Explanation on the 3rd point:\nCompiler understands the alias declared MyAllocList when used inside a template class Widget as MyAllocList\u0026lt;T\u0026gt; is not a dependent type. A user can have type as a data member, and thus it\u0026rsquo;s important to mention typename when using MyAllocList\u0026lt;T\u0026gt;::type (as a type), so that compiler knows it\u0026rsquo;s a type. Creating revised types from template type paramaeters is a common practice in Template Meta Programming (TMP). A few important points to note:\nIn C++11 (in header: \u0026lt;type_traits\u0026gt;):\nstd::remove_const\u0026lt;T\u0026gt;::type // yields T from const T std::remove_reference\u0026lt;T\u0026gt;::type // yields T from T\u0026amp; and T\u0026amp;\u0026amp; std::add_lvalue_reference\u0026lt;T\u0026gt;::type // yields T\u0026amp; from T In case you are applying the above transformations inside a template to a type parameter, you\u0026rsquo;ll have to use typename. This is because they have been implemented as typedefs inside templatized structs.\nIn C++14, their alias equivalent were added which do not require you to prefix typename:\n// equivalents to the above 3 transformations std::remove_const_t\u0026lt;T\u0026gt; std::remove_reference_t\u0026lt;T\u0026gt; std::add_lvalue_reference\u0026lt;T\u0026gt; Acknowledgement\nThanks to Kshitij Kalambarkar for reviewing this blog. Thanks for reading!\n{{ template \u0026ldquo;_internal/disqus.html\u0026rdquo; . }}\n","permalink":"https://krshrimali.github.io/posts/2021/08/prefer-alias-declarations-to-typedefs-notes/","summary":"\u003cp\u003eOne solution to avoiding using long type names:\u003c/p\u003e\n\u003cdiv class=\"highlight\"\u003e\u003cpre tabindex=\"0\" style=\"color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;\"\u003e\u003ccode class=\"language-cpp\" data-lang=\"cpp\"\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#75715e\"\u003e// So C++98 like\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#75715e\"\u003e\u003c/span\u003e\u003cspan style=\"color:#66d9ef\"\u003etypedef\u003c/span\u003e std\u003cspan style=\"color:#f92672\"\u003e::\u003c/span\u003eunique_ptr\u003cspan style=\"color:#f92672\"\u003e\u0026lt;\u003c/span\u003estd\u003cspan style=\"color:#f92672\"\u003e::\u003c/span\u003eunordered_map\u003cspan style=\"color:#f92672\"\u003e\u0026lt;\u003c/span\u003estd\u003cspan style=\"color:#f92672\"\u003e::\u003c/span\u003estring, std\u003cspan style=\"color:#f92672\"\u003e::\u003c/span\u003estring\u003cspan style=\"color:#f92672\"\u003e\u0026gt;\u0026gt;\u003c/span\u003e UPtrMapSS;\n\u003c/span\u003e\u003c/span\u003e\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e","title":"Prefer Alias Declarations to Typedefs (Notes)"},{"content":"In today\u0026rsquo;s blog, we\u0026rsquo;ll talk about two important concepts in C++: Function Pointers and Function Objects.\nPlease note that, function objects are commonly referred as functors but we have failed to notice any official alias to the name. Hence, we\u0026rsquo;ll restrict ourselves to using Function Objects in this blog.\nFunction Pointers As the name sounds, a function pointer is simply a pointer to the memory address of a function. Consider a following function:\n// A function which returns true if a \u0026gt; b else false bool isGreater(int a, int b) { return a \u0026gt; b; } As we would expect, the function is stored in the memory starting with an address. You can print the memory address of a function by doing (we do this using printf, see: https://stackoverflow.com/a/2064722)\nauto fn_addresss = isGreater; // get the address of the function printf(\u0026#34;Function address: %p\\n\u0026#34;, fn_address); And you\u0026rsquo;ll notice a hex value as the output: 0x5649d675c139 (in my case). The syntax for creating a function pointer looks like this:\nreturn_type (*ptr_name)(arg1_type, arg2_type, ...); So in our case for isGreater, it will be:\nbool (*justApointer)(int, int); What this means is, the pointer justApointer will point to a function taking 2 integer arguments and returning a boolean value. But note that this doesn\u0026rsquo;t point to any function yet. If you will do:\nbool (*justApointer)(int, int); // De-reference the pointer and call (*justApointer)(3, 4); This will cause a segmentation fault (core dumped) error because it points to no valid address of an executable code. So let\u0026rsquo;s go ahead and point our pointer to the memory address of isGreater:\njustApointer = \u0026amp;isGreater; Here we have given the address of the function to the pointer, you can also do:\n// Note: It\u0026#39;s a good practice to avoid writing the type of function pointers again if it\u0026#39;s too verbose using FnType = bool (*)(int, int); FnType justApointer{ \u0026amp;isGreater }; // OK FnType yetAnotherPointer = \u0026amp;isGreater; // OK FnType yetAnotherFnPointer = isGreater; // OK, implicit conversion happens in C++ from function to function pointer, so you don\u0026#39;t need to use \u0026amp; operator You could also have declared the pointer first, and then initialized:\n// Declararation bool (*justApointer)(int, int); // Initialization justApointer { \u0026amp;isGreater }; justApointer { isGreater }; justApointer = \u0026amp;isGreater; justApointer = isGreater; All of this is valid and works in C++. If you\u0026rsquo;re coming from Modern C++, you might have realized that it\u0026rsquo;s OK to skip the syntax of a function pointer and use:\nauto justApointer = isGreater; Calling a function pointer is fairly straight forward, you can just do:\n// Will return 0 since 3 is not greater than 4 std::cout \u0026lt;\u0026lt; (*justApointer)(3, 4); Since it\u0026rsquo;s a pointer, so you have to de-reference it to get to the function address (executable code in the memory) and then call it using the () call operator. C++ does the implicit conversion, and you can skip de-referencing:\n// Will also return 0 since 3 is not greater than 4 std::cout \u0026lt;\u0026lt; justApointer(3, 4); If you are familiar with concepts of const pointers, you can also create a const function pointer, so that once initialized - it can not be pointed to a different function.\nbool (*const justApointer)(int, int) = \u0026amp;isGreater; // You can not re-initialize (aka assign) it to point to any other address justApointer = \u0026amp;isGreater; // NOT OK, ERROR: assignment of read-only variable \u0026#39;justApointer\u0026#39; If you have noticed, we used *const justApointer - since we wanted to indicate to the compiler - that the pointer is supposed to be const, not the output (const bool (*justApointer)(int, int)). You can play around with different specifiers and see how they work though.\nOne of the use-cases of function pointers is to be able to pass a function as an argument (often referred as Callback Functions). But well, you might have a question - you can use a global function in another function, right? Yes, that\u0026rsquo;s possible, but consider a case where you want to pass different callback functions depending on your requirement to a more generic function (like sorting).\nbool isGreater(int a, int b) { return a \u0026gt; b; } bool isLesser(int a, int b) { return a \u0026lt; b; } bool isEqual(int a, int b) { return a == b; } We have these 3 functions but we don\u0026rsquo;t know yet which one we want to use to sort an array, let\u0026rsquo;s say you want to sort an array in descending order, another array in ascending order.\nstd::vector\u0026lt;int\u0026gt; sample_vec = {0, 5, -3, 4, 9, 2}; void a_generic_sorting_function(std::vector\u0026lt;int\u0026gt; input_vec, bool (*comparisonFunction)(int, int)) { // ... sorting algorithm // use comparisonFunction for comparisons } a_generic_sorting_function(sample_vec, isGreater); // sorts in descending order a_generic_sorting_function(sample_vec, isLesser); // sorts in ascending order Observe that even though we passed a function as an argument, but eventually - that\u0026rsquo;s interpreted as a pointer (since that\u0026rsquo;s what the 3rd argument type is, in a_generic_sorting_function).\nFunction Objects Function Objects are types that implement call operator ().\nFunction Objects provide us 2 advantages over function pointers, which are mainly:\nCan be optimized by the compiler, if possible. Allows to store a state. How can compiler optimize function objects? You\u0026rsquo;ll see this definition almost everywhere, and hence the quote. There is no better and simpler way to define a function object. But we\u0026rsquo;ll also focus on how can they make things easier + faster. A struct or a class in C++ which defines/implements call operator () can be referred as function object. Interestingly, in C++:\nstd::plus is a function object implementing x + y. std::minus is a function object implementing x - y. and many more arithmetic operators like /, *, % and negation (-x). See Operator Function Objects section in https://en.cppreference.com/w/cpp/utility/functional.\nThere are other comparison, logical and bitwise operations as well which are provided as function objects in C++. Let\u0026rsquo;s take a look at std::greater and std::lesser function objects to maintain the consistency b/w function pointers and objects sections. Going by the documentation (https://en.cppreference.com/w/cpp/utility/functional/greater), the struct std::greater implements the call operator ():\nbool operator() (const T\u0026amp; lhs, const T\u0026amp; rhs) const; // (until C++14) (source: https://en.cppreference.com/w/cpp/utility/functional/greater)\nIf we had to define our own function object, something similar to std::greater but only for integer inputs. As mentioned earlier, it\u0026rsquo;s a type with the call operator defined, so let\u0026rsquo;s go ahead and define our own struct:\nstruct greater { bool operator()(int a, int b) { return a \u0026gt; b; } }; greater comparison; std::cout \u0026lt;\u0026lt; comparison(3, 4); // Returns 0 Now you can ask: this could have been accomplished with a function pointer as well, so why a function object? Well, so the answer boils down to optimization (in this case). A compiler can inline the function if it\u0026rsquo;s possible to optimize the execution - and that\u0026rsquo;s only possible with function objects, while for function pointers - you need to de-reference it to know the address which happens during the runtime (unless there is some real complex optimization - which I\u0026rsquo;m not aware of right now).\nA real example can be to see the compiled code on https://godbolt.org/ (an amazing compiler explorer). If I compile the following code (on x86-64, gcc 11.1 with no optimization flags):\n#include \u0026lt;iostream\u0026gt; struct greater { bool operator()(int a, int b) { return a \u0026gt; b; } }; int main() { struct greater obj; std::cout \u0026lt;\u0026lt; obj(3, 4); } The relevant assembly code of the main function looks like this:\nmain: push rbp mov rbp, rsp sub rsp, 16 lea rax, [rbp-1] mov edx, 4 mov esi, 3 mov rdi, rax call greater::operator()(int, int) movzx eax, al mov esi, eax mov edi, OFFSET FLAT:_ZSt4cout call std::basic_ostream\u0026lt;char, std::char_traits\u0026lt;char\u0026gt; \u0026gt;::operator\u0026lt;\u0026lt;(bool) mov eax, 0 leave ret The above assembly code may look overwhelming to some, but the most relevant instruction is: (link to the code: https://godbolt.org/z/WqYozn7qv)\ncall greater::operator(int, int) Now if I add the optimization flag, you\u0026rsquo;ll see the operator being inlined:\nmain: sub rsp, 8 xor esi, esi mov edi, OFFSET FLAT:_ZSt4cout call std::basic_ostream\u0026lt;char, std::char_traits\u0026lt;char\u0026gt; \u0026gt;\u0026amp; std::basic_ostream\u0026lt;char, std::char_traits\u0026lt;char\u0026gt; \u0026gt;::_M_insert\u0026lt;bool\u0026gt;(bool) xor eax, eax add rsp, 8 ret _GLOBAL__sub_I_main: sub rsp, 8 mov edi, OFFSET FLAT:_ZStL8__ioinit call std::ios_base::Init::Init() [complete object constructor] mov edx, OFFSET FLAT:__dso_handle mov esi, OFFSET FLAT:_ZStL8__ioinit mov edi, OFFSET FLAT:_ZNSt8ios_base4InitD1Ev add rsp, 8 jmp __cxa_atexit Though, it may not be visible on the first look, but a closer look to the following instruction:\ncall std::basic_ostream\u0026lt;char, std::char_traits\u0026lt;char\u0026gt; \u0026gt;\u0026amp; std::basic_ostream\u0026lt;char, std::char_traits\u0026lt;char\u0026gt; \u0026gt;::_M_insert\u0026lt;bool\u0026gt;(bool) tells us that it doesn\u0026rsquo;t call the operator of the object of type greater anymore! Instead, the compiler knows that the value is false and hence it inlines the value to the std::cout call. While this is possible for function objects, it\u0026rsquo;s not possible for function pointers (with the -O3 flag at least).\nStoring a state It\u0026rsquo;s more like a property of a class/struct in C++ that you can take arguments in the constructor and have different objects with different values for a member variable. Take an example:\n// Usage: // GreaterThan obj(10); // obj(11); // is 11 \u0026gt; 10? // // GreaterThan obj_(-10); // obj_(-9); // is -9 \u0026gt; -10? class GreaterThan { int compare_with; public: void greaterThan(int inp) : compare_with(inp) { } bool operator()(int another_number) { return another_number \u0026gt; compare_with; } }; int main() { GreaterThan obj(10); std::cout \u0026lt;\u0026lt; obj(11) \u0026lt;\u0026lt; \u0026#39;\\n\u0026#39;; GreaterThan obj_(-10); std::cout \u0026lt;\u0026lt; obj_(-9) \u0026lt;\u0026lt; \u0026#39;\\n\u0026#39;; } Here you have a member variable compare_with and you can have different values for each object instantiated. While it\u0026rsquo;s also possible for a function by using a static variable but you can\u0026rsquo;t have multiple values for it on a single run.\nFunction Objects and Function Pointers in the Standard Library Function Objects and Function Pointers, just like any other type/value can be passed as a type to a template:\ntemplate \u0026lt;typename T, typename ComparatorFunc\u0026gt; void sort(T vector_input, ComparatorFunc func) { // ... sorting logic using given comparator function: func } This allows you to use sort as a generic function with different types of comparators. Let\u0026rsquo;s take a look here, you have 2 function object types: isGreater and isLesser: (the same can be done for function pointers as well)\nstruct isGreater { bool operator()(int a, int b) { return a \u0026gt; b; } } struct isLesser { bool operator()(int a, int b) { return a \u0026lt; b; } } template \u0026lt;typename T, typename ComparatorFunc\u0026gt; T sort(T input, ComparatorFunc func) { // use func to decide sorting strategy (descending/ascending) } This is valid in C++, though I\u0026rsquo;ll like to add a disclaimer here, you could have just used std::sort instead of implementing your own sorting strategy (unless you know what you are doing ;)):\nstd::sort(input.begin(), input.end(), isGreater); std::sort(input.begin(), input.end(), isLesser); This is mostly it for this blog, there is a lot to discuss about function pointers and objects, but I guess this should be enough for you to get started and follow us along in future blogs.\nReferences and Good Reads Learn CPP\u0026rsquo;s Blog on Function Pointer. Function Objects in the STL (Microsoft Docs) CppReference: Function Objects. ","permalink":"https://krshrimali.github.io/posts/2021/07/function-pointers-and-function-objects-in-c-/","summary":"\u003cp\u003eIn today\u0026rsquo;s blog, we\u0026rsquo;ll talk about two important concepts in C++: Function Pointers and Function Objects.\u003c/p\u003e\n\u003cp\u003ePlease note that, function objects are commonly referred as \u003cem\u003efunctors\u003c/em\u003e but we have failed to notice any official alias to the name. Hence, we\u0026rsquo;ll restrict ourselves to using \u003cem\u003eFunction Objects\u003c/em\u003e in this blog.\u003c/p\u003e","title":"Function Pointers and Function Objects in C++"},{"content":"\nHi everyone! In the previous blog we implemented Portrait Bokeh using Face Detection in OpenCV. While the results were good for a start, we definitely want to be closer to the output we expect. The end goal is to blur everything except the face. The main problem we noticed was:\nThe face cropped was a rectangle, and it was clearly visible in the output result. To overcome this, we will be talking about cropping a circle in OpenCV today. This will enable us to get rid of \u0026ldquo;some\u0026rdquo; of the background noise we got earlier, for Portrait Bokeh. Let\u0026rsquo;s take this step by step, and first talk about the intuition.\nIntuition behind cropping a circle while pixels are just the brightness values for each channel at a particular coordinate, so you can\u0026rsquo;t really get half of the pixel and crop an exact circle. But the closest we can get to cropping a circle, is to imagine a circle circumscribed in a rectangle (face detection algorithm in OpenCV - CascadeClassifier returns a rectangle - can be a square as well). So if we are able to get a circle from our output of face detection (a rectangle), we will be closer to what we want.\nBut how do we get started? Clearly, since the circle is circumscribing the rectangle, the closest we can get to finding radius is: max(width, height)/2. While center will be: (top_left_x + width/2, top_left_y + height/2). Once we know these two properties of the circle, we will now have the circle equation.\nMethodology Let\u0026rsquo;s divide this problem statement into steps:\nGet face from Face Detection. Get circle circumscribing the face (rectangle). Crop the circle and store it in different array. Blur the whole image except the face. Essentially, the main goal is to get the face cropped as a circle. Once we have that, we can simply overlay this on the blurred image. The trick is to figure out on how we can crop the circle once we know it\u0026rsquo;s coordinates. Let\u0026rsquo;s talk about it\u0026rsquo;s solution in the next section.\nCropping a circle Usually, our images will have 3 channels (colored image): Blue, Green, Red (BGR). How about we add a transparency channel to our image? The idea behind this is to make all pixels transparent which are NOT in the face, and all the pixels opaque which are within/on the face (circle) boundary. The pseudo code for this should look something like this:\n# Assuming you got a circle equation representing the face face = circle_equation # Now iterate through all the pixel values in the imagge # Check if the pixel is outside the face, if yes - then make it transparent # Else - opaque for pixel_value in image: if pixel_value is outside the face: # Make this pixel transparent else: # Make this pixel opaque # This will be visible To have an option to add transparency, you need to convert the BGR input image to BGRA (4 channel image: Blue, Green, Red, Alpha) - here Alpha channel denotes transparency channel. When the transparency is set to 0, that represents opaque and when it\u0026rsquo;s set to 255, it represents transparent value. Let\u0026rsquo;s go ahead and use this for our application.\nVideo Tutorial I started a YouTube channel where I go live on the weekends, and upload videos on the week days (not so regularly) about Computer Vision, deploying models into production and more. If you haven\u0026rsquo;t seen it before, please check it out here. For this blog, I have already uploaded a detailed tutorial. Check it out here.\nStep 1: Get face from face detection We have discussed this before, so we won\u0026rsquo;t go in details but for the sake of continuity, I\u0026rsquo;ll add the code for Face Detection.\nimport cv2, sys # Get image path and read image img_path = sys.argv[1] if len(sys.argv) \u0026gt; 1 else \u0026#34;sample.png\u0026#34; img = cv2.imread(img_path, 1) # Convert to grayscale, since Face Detection takes gray scale image as input gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY) # Initialize face detector from the model file face_detector = cv2.CascadeClassifier(\u0026#34;haarcascade_frontalface_default.xml\u0026#34;) # Detect faces from gray-scaled image, using default parameters (scaleFactor) faces = face_detector.detectMultiScale(gray, scaleFactor = 1.1, minNeighbors = 5) # Note: Format of faces will be: [ [top_left_x, top_left_y, w, h (for face 1)], [... (for face 2)], ... ] Once you have ROI (Region Of Interest) of the faces in the image, we can go ahead and start cropping a circle (yay!).\nStep 2: Get circle circumscribing the face From Step-1, we got the faces. Let\u0026rsquo;s iterate through each face, and get the equation of the circle circumscribing that face. As we discussed before in the Intuition section, we\u0026rsquo;ll have to calculate the radius and center of the face.\n# Make sure to convert BGR to BGRA image first # To be used later imgTransp = cv2.cvtColor(img, cv2.COLOR_BGR2BGRA) # Iterate through each face one by one for face in faces: top_left_x, top_left_y, width, height = face[0], face[1], face[2], face[3] radius = max(width, height)/2 center = (top_left_x + width/2, top_left_y + height/2) # Cropping circle code will come here Here, we find the circle contained in the rectangle (closest) for each face. Let\u0026rsquo;s go ahead and crop this circle one by one, and see how we can use this for Portrait Bokeh!\nStep 3: Crop the circle and store it in different array We discussed the pseudo code of this in Cropping a Circle section of this blog. But before this, we have to figure out: How to find a point is within that circle? Think of this as a simple maths problem where you have to find a given coordinate is inside a circle or not. What would you do?\nFind distance between point and center of the circle. If distance is greater than radius, it\u0026rsquo;s outside. If distance is equal to radius, it\u0026rsquo;s on the boundary. If distance is less than radius, it\u0026rsquo;s inside. We can simplify this for circle as we know it\u0026rsquo;s equation: (point_x - center_x)^2 + (point_y - center_y)^2 - radius^2, which will be:\n0 if the point is on the boundary. greater than 0 if the point is outside the circle. less than 0 if the point is inside the circle. Let\u0026rsquo;s use this concept here:\ndef is_inside(point, center, radius): \u0026#34;\u0026#34;\u0026#34; This function returns True if point is inside/on the boundary, False otherwise \u0026#34;\u0026#34;\u0026#34; eqn = (point[0] - center[0]) ** 2 + (point[1] - center[1])**2 - radius**2 return eqn \u0026lt;= 0 # Iterate through all the pixels in the image for row in range(img.shape[0]): for col in range(img.shape[1]): if is_inside(img[row][col], center, radius): # Means the point is inside/on the face # Make it opaque img[row][col][3] = 0 else: # Means the point is outside the face # Make it transparent img[row][col][3] = 255 We will have to execute the for loop once for each face, which means the code becomes:\n# Make sure to convert BGR to BGRA image first # To be used later imgTransp = cv2.cvtColor(img, cv2.COLOR_BGR2BGRA) # Make the whole image transparent, by default imgTransp[..., 3] = 255 def is_inside(point, center, radius): \u0026#34;\u0026#34;\u0026#34; This function returns True if point is inside/on the boundary, False otherwise \u0026#34;\u0026#34;\u0026#34; eqn = (point[0] - center[0]) ** 2 + (point[1] - center[1])**2 - radius**2 return eqn \u0026lt;= 0 # Iterate through each face one by one for face in faces: top_left_x, top_left_y, width, height = face[0], face[1], face[2], face[3] radius = max(width, height)/2 center = (top_left_x + width/2, top_left_y + height/2) # Cropping circle code will come here # Iterate through all the pixels in the image for row in range(img.shape[0]): for col in range(img.shape[1]): if is_inside(img[row][col], center, radius): # Means the point is inside/on the face # Make it opaque imgTransp[row][col][3] = 255 else: # Means the point is outside the face # Make it transparent imgTransp[row][col][3] = 0 If you notice, we have 3 nested for loops, which will result into really non-efficient code for HD Images (1920x1080 images? Think of the number of computations happening in this case). Suppose we have 3 faces detected and our input image is 1920 x 1080 (width x height). Total number of times the function is_inside called will be: 3 x 1920 x 1080, which is 6220800 (approx. 6.2 Million or 62 Lacs). It\u0026rsquo;s a lot!\nWe can not avoid these loops though, but why iterate through the whole image when you know the circle is anyways gonna be within that rectangle (face)! Imagine the face is 200 x 200 now, and everything remains same (3 faces, HD input Image: 1920 x 1080). If we only iterate through the face everytime, the computations will be: 3 * 200 * 200, which is 120000 (120 thousand or 1.2 lacs). Much better. All we have to do is, pick the face ROI, and iterate through that region. Everything else remains same:\nfor row in range(tly, tly + height): for col in range(tlx, tlx + width): if is_inside(img[row][col], center, radius): # Means the point is inside/on the face # Make it opaque imgTransp[row][col][3] = 255 else: # Means the point is outside the face # Make it transparent imgTransp[row][col][3] = 0 So, the code should look like this:\n# Make sure to convert BGR to BGRA image first # To be used later imgTransp = cv2.cvtColor(img, cv2.COLOR_BGR2BGRA) # Make the whole image transparent by default imgTransp[..., 3] = 255 def is_inside(point, center, radius): \u0026#34;\u0026#34;\u0026#34; This function returns True if point is inside/on the boundary, False otherwise \u0026#34;\u0026#34;\u0026#34; eqn = (point[0] - center[0]) ** 2 + (point[1] - center[1])**2 - radius**2 return eqn \u0026lt;= 0 # Iterate through each face one by one for face in faces: top_left_x, top_left_y, width, height = face[0], face[1], face[2], face[3] radius = max(width, height)/2 center = (top_left_x + width/2, top_left_y + height/2) # Cropping circle code will come here # Iterate through all the pixels in the image for row in range(tly, tly + height): for col in range(tlx, tlx + width): if is_inside((col, row), center, radius): # Means the point is inside/on the face # Make it opaque imgTransp[row][col][3] = 255 else: # Means the point is outside the face # Make it transparent imgTransp[row][col][3] = 0 Let\u0026rsquo;s try to visualize imgTransp here and see how this looks like:\nThis looks good so far! We have cropped the circle (face), and all we need to do now is - overlay this image on a blurred image. Let\u0026rsquo;s head straight to Step 4.\nStep 4: Blurring and overlaying In Step 3, we were able to crop the circle. But think about this, whenever we know the pixel is inside the face, let\u0026rsquo;s just replace the blurred pixel with original image.\n# Blur the whole image first img_blurred = cv2.GaussianBlur(img, (11, 11), 0) # Iterate through the faces we were doing before # Whenever the pixel is inside, replace the point at img_blurred with original img # Make sure to convert BGR to BGRA image first # To be used later imgTransp = cv2.cvtColor(img, cv2.COLOR_BGR2BGRA) # Make the whole image transparent by default imgTransp[..., 3] = 255 def is_inside(point, center, radius): \u0026#34;\u0026#34;\u0026#34; This function returns True if point is inside/on the boundary, False otherwise \u0026#34;\u0026#34;\u0026#34; eqn = (point[0] - center[0]) ** 2 + (point[1] - center[1])**2 - radius**2 return eqn \u0026lt;= 0 # Iterate through each face one by one for face in faces: top_left_x, top_left_y, width, height = face[0], face[1], face[2], face[3] radius = max(width, height)/2 center = (top_left_x + width/2, top_left_y + height/2) # Cropping circle code will come here # Iterate through all the pixels in the image for row in range(tly, tly + height): for col in range(tlx, tlx + width): if is_inside((col, row), center, radius): # Means the point is inside/on the face # Make it opaque imgTransp[row][col][3] = 255 # Replace pixel of blurred image with original image imgBlurred[row][col] = img[row][col] else: # Means the point is outside the face # Make it transparent imgTransp[row][col][3] = 0 Note: The only reason we used imgTransp here, is to show how to crop a circle. For portrait bokeh, you don\u0026rsquo;t need to have imgTransp and transparency channels.\nHere is how the output looks like. While I understand that there is still some background, but we can definitely be better than this - and this will be our topic for the next blog!\nThis should be it for this blog, and I hope you learnt something new today. If you liked the content, please leave a comment below. I would love to read your feedbacks, suggestions and if this helped you out in any way. I also go live on weekends, and upload videos on weekdays on my YouTube Channel, so make sure to subscribe there and join me in if you find it interesting! Thank you for reading this blog.\n","permalink":"https://krshrimali.github.io/posts/2020/12/how-to-crop-a-circle-in-opencv-implementing-portrait-bokeh-part-2/","summary":"\u003cp\u003e\u003cimg loading=\"lazy\" src=\"https://raw.githubusercontent.com/krshrimali/blog/main/assets/cover-images/Dec_10_2020.jpg\"\u003e\u003c/p\u003e\n\u003cp\u003eHi everyone! In the previous blog we implemented Portrait Bokeh using Face Detection in OpenCV. While the results were good for a start, we definitely want to be closer to the output we expect. The end goal is to blur everything except the face. The main problem we noticed was:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eThe face cropped was a rectangle, and it was clearly visible in the output result.\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eTo overcome this, we will be talking about cropping a circle in OpenCV today. This will enable us to get rid of \u0026ldquo;some\u0026rdquo; of the background noise we got earlier, for Portrait Bokeh. Let\u0026rsquo;s take this step by step, and first talk about the intuition.\u003c/p\u003e","title":"How to crop a circle in OpenCV? Implementing Portrait Bokeh - Part 2"},{"content":"\nOpenCV: Using face detection for Portrait Bokeh (Background Blur) (Part - 1) This blog discusses using Face Detection in OpenCV for Portrait Bokeh. We\u0026rsquo;ll be implementing Portrait Bokeh (blurring everything but faces) using 3 different methods in this series:\nUsing Face Detection (cropping a rectangle) Using Face Detection (cropping a circle) Using Facial Landmark Detection and Convex Hull Don\u0026rsquo;t lose hopes if you are confused. We will be going through each method one by one, and hopefully the road will be crearer from here.\nPortrait Bokeh: Discussing Problem Statement Before moving ahead, let\u0026rsquo;s talk about \u0026ldquo;What is Portrait Bokeh?\u0026rdquo;. It\u0026rsquo;s important to talk about the problem before discussing solutions. Take a quick look at the two images below:\nAs you might have spotted the difference already, the image on the left is our input (/original) image while the image on the right is our output image. If you haven\u0026rsquo;t spotted the difference, everything except the face in the image on the right is blurred! This feature now comes in almost all smart phones, and is also termed as just Portrait mode. Whenever you want to highlight the people near to the camera (mostly you, your friends or anyone) and blur the background, this is the mode you will usually choose. While some blur everything except faces, others might choose to keep the body instead of just faces. Our problem statement will be limited to faces here.\nMethodology opted Let\u0026rsquo;s discuss on how we can go ahead to solve this problem. We surely need to know where the face is to avoid blurring it, so the first step has to be of face detection. And since we need to blur the background, so at some stage, we need to do blurring as well. Since this part is about the simplest step, we can just combine them and say:\nDetect face(s) from the given input image. Crop the faces and store them as separate objects. Blur the whole image. Overlay the cropped faces from step-2 on the output from step-3. Video Tutorial I started a YouTube channel where I go live on the weekends, and upload videos on the week days (not so regularly) about Computer Vision, deploying models into production and more. If you haven\u0026rsquo;t seen it before, please check it out here. For this blog, I have already uploaded a detailed tutorial. Check it out here.\nStep 1: Detecting Faces using Haarcascade We\u0026rsquo;ll be using haarcascade model files to detect face in the image. To ease the computation and satisfy the input to the model, we need to first convert the image to GrayScale (if it\u0026rsquo;s not already) - that is the image will now have only one channel instead of 3 (Blue, Green, Red). Download the model file to your directory from here. Let\u0026rsquo;s go ahead and initialize our Face Detector.\nmodel_path = \u0026#34;haarcascade_frontalface_default.xml\u0026#34; # Assuming this is in our current directory face_detector = cv2.CascadeClassifier(model_path) Once we have the model loaded, let\u0026rsquo;s go ahead and detect faces from the given image. Remember, that we will also convert the image to grayscale.\n# Read input image (get image path first from command line, else take sample.png - default) img_path = self.argv[1] if len(sys.argv) \u0026gt; 1 else \u0026#34;sample.png\u0026#34; img = cv2.imread(img_path, 1) # Convert the image to grayscale gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY) # Get faces # Use default arguments, scaleFactor can be tweaked depending on the image # The output will be in format: [ [\u0026lt;top left x coord\u0026gt;, \u0026lt;top left y\u0026gt;, \u0026lt;width\u0026gt;, \u0026lt;height\u0026gt; : for face 1], [ ... : for face 2], ... ] faces = face_detector.detectMultiScale(gray, scaleFactor=1.1, minNeighbors=5) Once we have the faces, we can crop them and use in the Step-4 again. The output from face detection should look like this:\nStep 2: Crop faces To crop them and store in another object:\ncropped_faces = [] for face in faces: # Get points: tlx (top left x), tly (top left y), w (width), h (height) tlx, tly, w, h = face[0], face[1], face[2], face[3] cropped_faces.append( face[tly:tly+h, tlx:tlx+w] ) The list cropped_faces will now contain only faces. We can use this list again in Step-4!\nStep 3 and Step 4: Blur the image and overlay faces Let\u0026rsquo;s blur the whole image, and then overlay the images on the top of it. To blur, we will be using Gaussian Blur which works just fine.\nblur = cv2.GaussianBlur(img, (11, 11)) # Here, (11, 11) is the kernel size Once the whole image has been blurred, let\u0026rsquo;s overlay the cropped faces from Step 2.\nfor face_index, cropped_face in enumerate(cropped_faces): # Get face coordinates, to get ROI face_coords = faces[face_index] tlx, tly, w, h = face_coords[0], face_coords[1], face_coords[2], face_coords[3] # Overlay the ROI of face to the cropped face blur[tly:tly+h, tlx:tlx+w] = cropped_face Following image explains the procedure in details with visualization.\nAnd this is how the output (on the right) will look like (see below).\nWhile I know many of you will be thinking that it\u0026rsquo;s not accurate at all (since we can see the rectangle there), and that will be a topic for the next blog where we will attempt to crop a circle. Make sure to leave a comment if you have any suggestions, feedback or if this blog helped you in any way - I would love to hear that!\n","permalink":"https://krshrimali.github.io/posts/2020/12/implementing-portrait-bokeh-in-opencv-using-face-detection-part-1/","summary":"\u003cp\u003e\u003cimg loading=\"lazy\" src=\"https://raw.githubusercontent.com/krshrimali/blog/main/assets/cover-images/Dec_7_2020.jpg\"\u003e\u003c/p\u003e\n\u003ch1 id=\"opencv-using-face-detection-for-portrait-bokeh-background-blur-part---1\"\u003eOpenCV: Using face detection for Portrait Bokeh (Background Blur) (Part - 1)\u003c/h1\u003e\n\u003cp\u003eThis blog discusses using Face Detection in OpenCV for Portrait Bokeh. We\u0026rsquo;ll be implementing Portrait Bokeh (blurring everything but faces) using 3 different methods in this series:\u003c/p\u003e\n\u003col\u003e\n\u003cli\u003eUsing Face Detection (cropping a rectangle)\u003c/li\u003e\n\u003cli\u003eUsing Face Detection (cropping a circle)\u003c/li\u003e\n\u003cli\u003eUsing Facial Landmark Detection and Convex Hull\u003c/li\u003e\n\u003c/ol\u003e\n\u003cp\u003eDon\u0026rsquo;t lose hopes if you are confused. We will be going through each method one by one, and hopefully the road will be crearer from here.\u003c/p\u003e","title":"Implementing Portrait Bokeh in OpenCV using Face Detection (Part-1)"},{"content":"Today, I am elated to share Docker image for OpenCV, Libtorch and Xeus-Cling. We\u0026rsquo;ll discuss how to use the dockerfile and binder.\nBefore I move on, the credits for creating and maintaining Docker image goes to Vishwesh Ravi Shrimali. He has been working on some cool stuff, please do get in touch with him if you\u0026rsquo;re interested to know.\nFirst question in your mind would be, Why use Docker or Binder? The answer to it lies in the frequency of queries on the discussion forum of PyTorch and Stackoverflow on Installation of Libtorch with OpenCV in Windows/Linux/OSX. I\u0026rsquo;ve had nightmares setting up the Windows system myself for Libtorch and nothing could be better than using Docker. Read on, to know why.\nInstalling Docker on Mac OS To install docker (community edition - CE) desktop in Mac OS system, simply navigate to the Stable Channel section here. Once setup, you can use docker (command line and desktop). Once done, navigate to Install and run Docker for Mac section and get used to the commands.\nInstalling Docker on Ubuntu Before moving on, please consider reading the requirements to install Docker Community Edition](https://docs.docker.com/v17.12/install/linux/docker-ce/ubuntu/). For the steps to install Docker CE, refer this.\nInstalling Docker on Windows To install Docker on Windows, download docker (stable channel) from here. The installation steps to install Docker Desktop on Windows can be found here.\nUsing Docker Image Fetch the docker image: docker pull vishwesh5/libtorch-opencv:opencv-4-1-0. This shall take a lot of time, so sit back and relax. Run: docker run -p 5000:5000 -p 8888:8888 -it vishwesh5/libtorch-opencv:opencv-4-1-0 /bin/bash. To know more about these commands, check out the references section.\nOnce done, you\u0026rsquo;ll see your terminal showing another username: jovyan. You\u0026rsquo;ve entered the docker image, congratulations! No need to setup OpenCV or Libtorch. Vishwesh has done it for you!\nNow since you have entered the docker container successfully, it should look something similar to this:\nTime to test Libtorch. Let\u0026rsquo;s go ahead and test a simple VGG-Net on MNIST dataset using Libtorch.\nTesting Docker Image Clone the repository containing code for Digit Classification using Libtorch on MNIST dataset: git clone https://github.com/krshrimali/Digit-Recognition-MNIST-SVHN-PyTorch-CPP.git. Change directory to the cloned repository. Download the MNIST data from http://yann.lecun.com/exdb/mnist/. Download train-images-idx3-ubyte.gz and train-labels-idx1-ubyte.gz files for training the VGG-Net. You can skip downloading the test data for now. Use gunzip \u0026lt;file_path\u0026gt; to extract the training images and labels, and put them in the data/ folder inside the clones repository. Create a build folder: mkdir build Run the CMake Configuration using: cmake -DCMAKE_PREFIX_PATH=/opt/libtorch ... The result should be similar to something in the figure below. Build the code using make command: make. Execute the code, and that\u0026rsquo;s it. Have fun learning. Testing Docker Image with Xeus-Cling Let\u0026rsquo;s test the Docker Image with Xeus-Cling.\nRun jupyter notebook command in the console and copy the token from the url provided. Open http://localhost:8888 in your browser. Note that the port address (8888) comes from -p 8888:8888 in the docker run command. You can change that if you want. Enter the token when asked. Start a new notebook using C++XX kernel. Include and load libraries in the first cell using: #include \u0026quot;includeLibraries.h\u0026quot;. This should do all the stuff for you. Start doing experiments using Xeus-Cling now. Using Binder And! What if you just want to try Libtorch or show it to the students? What if you are on a remote PC, and can\u0026rsquo;t install Docker? Well, here is the Binder: https://mybinder.org/v2/gh/vishwesh5/torch-binder/master.\nGo to the above link and a notebook shall open.\nCreate a new notebook and start with: #include \u0026quot;includeLibraries.h\u0026quot; first and then start testing.\nAcknowledgements Thanks to Vishwesh Ravi Shrimali, for creating the docker container and binder for this post.\nReferences Install OpenCV Docker Image on Ubuntu, MacOS or Windows by Vishwesh Ravi Shrimali. ","permalink":"https://krshrimali.github.io/posts/2020/09/releasing-docker-container-and-binder-for-using-xeus-cling-libtorch-and-opencv-in-c-/","summary":"\u003cp\u003eToday, I am elated to share Docker image for \u003ccode\u003eOpenCV\u003c/code\u003e, \u003ccode\u003eLibtorch\u003c/code\u003e and \u003ccode\u003eXeus-Cling\u003c/code\u003e. We\u0026rsquo;ll discuss how to use the \u003ccode\u003edockerfile\u003c/code\u003e and \u003ccode\u003ebinder\u003c/code\u003e.\u003c/p\u003e\n\u003cp\u003e\u003cimg loading=\"lazy\" src=\"https://raw.githubusercontent.com/krshrimali/blog/main/assets/cover-images/Cover-Docker-Binder.jpg\"\u003e\u003c/p\u003e\n\u003cp\u003eBefore I move on, the credits for creating and maintaining Docker image goes to \u003ca href=\"https://github.com/vishwesh5\"\u003eVishwesh Ravi Shrimali\u003c/a\u003e. He has been working on some cool stuff, please do get in touch with him if you\u0026rsquo;re interested to know.\u003c/p\u003e\n\u003cp\u003eFirst question in your mind would be, \u003cstrong\u003eWhy use Docker or Binder?\u003c/strong\u003e The answer to it lies in the frequency of queries on \u003ca href=\"http://www.discuss.pytorch.org\"\u003ethe discussion forum of PyTorch\u003c/a\u003e and Stackoverflow on \u003cstrong\u003eInstallation of Libtorch with OpenCV in Windows/Linux/OSX\u003c/strong\u003e. I\u0026rsquo;ve had nightmares setting up the Windows system myself for \u003ccode\u003eLibtorch\u003c/code\u003e and nothing could be better than using \u003ccode\u003eDocker\u003c/code\u003e. Read on, to know why.\u003c/p\u003e","title":"Releasing Docker Container and Binder for using Xeus-Cling, Libtorch and OpenCV in C++"},{"content":"In the last blog post, I realized there were a lot of methods inherited from the base struct _Vector_base_ and _Vector_impl_data. Instead of directly going to the source code of these structs, I\u0026rsquo;ll go through their methods and objects by explaining what happens when we initialize a vector.\nThat is, we will start from calling a vector constructor and then see how memory is allocated. If you haven\u0026rsquo;t looked at the previous blog post, please take a look here. I want to be thorough with the blog post, so I\u0026rsquo;ll divide this into multiple posts. By the end of this post, you\u0026rsquo;ll go through the following structs:\n_Vector_impl_data struct which contains pointers to memory locations (start, finish and end of storage). _Vector_impl struct (inherits _Vector_impl_data as well)). I usually opt for the bottom-up approach. Vectors can be initialized in many ways, three of them will be discussed in today\u0026rsquo;s blog. We\u0026rsquo;ll start from the very basic constructor of a vector using an initializer list and slowly reach to memory allocation and how the above 2 structs are used. Let\u0026rsquo;s start!\nUsing Initializer Lists So what happens when we initialize a vector with an initializer list?\nstd::vector\u0026lt;int\u0026gt; vec {1, 2, 3}; The vector class has many constructors in GCC depending on the type of inputs you give. Let\u0026rsquo;s take a look at the constructor when the input is an initializer list:\nvector(initializer_list\u0026lt;value_type\u0026gt; __l, const allocator_type\u0026amp; __a = allocator_type()) : _Base(__a) { _M_range_initialize(__l.begin(), __l.end(), random_access_iterator_tag()); } If you are curious what _Base is, _Base is declared as: typedef _Vector_base\u0026lt;_Tp, _Alloc\u0026gt; _Base;. Just so you know, where and how is _Vector_base used. When the constructor is called, it calls the constructor of _Vector_base with __a (allocator type). As you might have noticed, we are calling _M_range_initialize and passing 2 iterators (__l.begin(), __l.end()) and 1 forward iterator tag.\nNote that the iterators are Forward Iterators, that is: we can use these iterators to access elements from begin (accessed using .begin()) till the end (accessed using .end()).\nWe are using random_access_iterator_tag as forward_iterator_tag. This tag helps us to categorize the iterator as random-access iterator. Random-access iterators allow accessing elements by passing arbitrary offset position (see: documentation for more details).\nLet\u0026rsquo;s go ahead and see what _M_range_initialize does.\ntemplate \u0026lt;typename _ForwardIterator\u0026gt; void _M_range_initialize(_ForwardIterator __first, _ForwardIterator __last, std::forward_iterator_tag) { const size_type __n = std::distance(__first, __last); this-\u0026gt;_M_impl._M_start = this-\u0026gt;_M_allocate(_S_check_init_len(__n, _M_get_Tp_allocator())); this-\u0026gt;_M_impl._M_end_of_storage = this-\u0026gt;_M_impl._M_start + __n; this-\u0026gt;_M_impl._M_finish = std::__uninitialized_copy_a(__first, __last, this-\u0026gt;_M_impl._M_start, _M_get_Tp_allocator()); } Let\u0026rsquo;s go line by line.\nFirst we find the distance using std::distance which takes first and last iterators, and returns size such as: __last = __first + size. Next, we allocate memory for __n objects. The function this-\u0026gt;_M_allocate returns pointer to the starting location of the memory allocated. static size_type _S_check_init_len(size_type __n, const allocator_type\u0026amp; __a) { if (__n \u0026gt; _S_max_size(_Tp_alloc_type(__a))) __throw_length_error( __N(\u0026#34;cannot create std::vector larger than max_size()\u0026#34;)); return __n; } The function _S_check_init_len is called by constructors to check size. If the requested size is greater than the maximum size for the allocator type, it throws length error (\u0026quot;cannot create std::vector larger than max_size()\u0026quot;). Else, it returns __n. Once we have validated the size, this-\u0026gt;_M_allocate call allocates the memory. Note that, _M_allocate is a part of _Vector_base struct. _M_allocate allocates memory for __n number of objects. This returns a pointer to the memory location (starting), to _M_start. The end of storage pointer stores the end of memory location for the memory allocated for __n objects. The function std::__uninitialized_copy_a copies the range [__first, __last) into the this-\u0026gt;_M_impl._M_start. This returns a pointer to memory location starting at this-\u0026gt;_M_impl._M_start with length of __first - __last. To summarize, when we initialized vector with initializer list:\nIt first calculates the number of objects to allocate memory for. This is assigned to __n. Then, memory is allocated for __n objects (including a check if this much memory can be allocated based on the allocator type, if not then it returns a length error). The pointer _M_start points to the starting memory location. The end of storage is the end location of the storage. Since we have passed the initializer list, so it knows the end of storage is starting location + len(initializer_list). The elements are then copied the range [__first, __last) into the memory allocated. Depending on how you initialize your vectors, the process may change but overall, the intention is the same: to allocate memory (if valid) and set pointers (start, end of storage and finish).\nUsing similar value and specified number of elements (fill) Let\u0026rsquo;s take a look at an example of using\nstd::vector\u0026lt;int\u0026gt; vec(10, 0); The above constructor call will give you a vector of 10 elements with all zeros. You can print the elements using:\n// Instead of using auto, we can use // for (std::vector\u0026lt;int\u0026gt;::iterator it = vec.begin(); it != vec.end(); it++) { // std::cout \u0026lt;\u0026lt; *it \u0026lt;\u0026lt; \u0026#34; \u0026#34;; // } for (auto it = vec.begin(); it != vec.end(); it++) { std::cout \u0026lt;\u0026lt; *it \u0026lt;\u0026lt; \u0026#34; \u0026#34;; } std::cout \u0026lt;\u0026lt; std::endl; Let\u0026rsquo;s see what changes when the vector is constructed in the above mentioned way. Let\u0026rsquo;s take a look at the constructor which is called:\nvector(size_type __n, const value_type\u0026amp; __value, const allocator_type\u0026amp; __a = allocator_type()) : _Base(_S_check_init_len(__n, __a), __a { _M_fill_initialize(__n, __value); } As the documentation of the above constructor explains, this constructor fills the vector with __n copies of __a value. Note the use of _S_check_init_len here (we discussed this before). Instead of calling _M_range_initialize, _M_fill_initialize is called here. For our example, this function is passed with values: 10 (__n) and 0 (__value). Let\u0026rsquo;s take a look at the definition of _M_fill_initialize:\nvoid _M_fill_initialize(size_type __n, const value_type\u0026amp; __value) { this-\u0026gt;_M_impl._M_finish = std::__uninitialized_fill_n_a(this-\u0026gt;_M_impl._M_start, __n, __value, _M_get_Tp_allocator()); } The call __uninitialized_fill_n copies the value (__value, here 0) into the range [this-\u0026gt;_M_impl._M_start, this-\u0026gt;_M_impl._M_start + __n) and returns the end of it\u0026rsquo;s range. As per the documentation, it is similar to fill_n() but does not require an initialized output range. Wait, you might be wondering, we didn\u0026rsquo;t initialize this-\u0026gt;_M_impl._M_start! We did! Note that we called _Base(_S_check_init_len(__n, __a) when the constructor is called. _Base is nothing but a typedef of _Vector_base. Let\u0026rsquo;s take a look at this call:\n_Vector_base(size_t __n) : _M_impl() { _M_create_storage(__n); } _M_impl is an object of type _Vector_impl declared in _Vector_base struct. _M_create_storage(__n) is defined as: void _M_create_storage(size_t __n) { this-\u0026gt;_M_impl._M_start = this-\u0026gt;_M_allocate(__n); this-\u0026gt;_M_impl._M_finish = this-\u0026gt;_M_impl._M_start; this-\u0026gt;_M_impl._M_end_of_storage = this-\u0026gt;_M_impl._M_start + __n; } This will answer most of your queries. Let\u0026rsquo;s start line by line. this-\u0026gt;_M_allocate(__n) was discussed before, which allocates memory for __n objects. Please note that the constructor call _M_impl() had initialized these pointers for us. Here, the pointer is set to the starting memory location. Since the function _M_create_storage creates storage, and doesn\u0026rsquo;t copy elements to the memory location. So this-\u0026gt;_M_impl._M_finish is set to this-\u0026gt;_M_impl._M_start. The end of storage is, as before, set to this-\u0026gt;_M_impl._M_start + __n. So, eventually, it\u0026rsquo;s quite similar to what we saw when we initialized our vector with initializer list.\nUsing another vector (copy) Let\u0026rsquo;s take a look at another way to another initalize a vector:\nstd::vector\u0026lt;int\u0026gt; vec_copy {1, 2, 3}; std::vector\u0026lt;int\u0026gt; vec(vec_copy); // Try printing the elements of vec for (auto it = vec.begin(); it != vec.end(); it++) { std::cout \u0026lt;\u0026lt; *it \u0026lt;\u0026lt; std::endl; } When you call vec(vec_copy), the copy constructor is called. Let\u0026rsquo;s take a look at it\u0026rsquo;s definition:\nvector(const vector\u0026amp; __x) : _Base(__x.size(), _Alloc_traits::_S_select_on_copy(__x._M_get_Tp_allocator()) { this-\u0026gt;_M_impl._M_finish = std::__uninitialized_copy_a(__x.begin(), __x.end(), this-\u0026gt;_M_impl._M_start, _M_get_Tp_allocator()); } The function body is similar to what we saw in the constructor definition when we initialized vector using size_type __n, value_type value. Notice how we initialize the base struct here. Let\u0026rsquo;s take a look at _S_select_on_copy(__x._M_get_Tp_allocator()) first. _M_get_Tp_allocator() returns _M_impl object.\nconst _Tp_alloc_type\u0026amp; _M_get_Tp_allocator() { return this-\u0026gt;_M_impl; } Note that, here, this-\u0026gt;_M_impl will already have the pointers set to the memory locations for start, finish and end of storage (as we use the allocator of __x). The objective is to use the copy of allocator object used by __x. Let\u0026rsquo;s take a look at the constructor of Base struct:\n_Vector_base(size_t __n, const allocator_type\u0026amp; __a) : _M_impl(__a) { _M_create_storage(__n); } Overall, it\u0026rsquo;s the same to what we saw before except that we use the copy of the alloactor of vector __x. The call _M_create_storage(__n) does the same task of setting pointers _M_start, M_end_of_storage, _M_finish as we observed before.\nFor today\u0026rsquo;s blog, we discussed 3 popular ways to initialize a vector in C++ and went through how memory is allocated when the constructors are called. As we move forward, we will slowly get familiar with the design patterns and methods used in GCC.\nAs always, I would love to hear your feedback on my blogs. Correct me if I was wrong anywhere, no one is perfect afterall. If this helped you, please let me know - it keeps me going! See you all in the next blog!\n","permalink":"https://krshrimali.github.io/posts/2020/04/understanding-how-vectors-work-in-c-part-2-what-happens-when-you-initialize-a-vector/","summary":"\u003cp\u003eIn the last blog post, I realized there were a lot of methods inherited from the base struct \u003ccode\u003e_Vector_base_\u003c/code\u003e and \u003ccode\u003e_Vector_impl_data\u003c/code\u003e. Instead of directly going to the source code of these structs, I\u0026rsquo;ll go through their methods and objects by explaining what happens when we initialize a vector.\u003c/p\u003e\n\u003cp\u003e\u003cimg loading=\"lazy\" src=\"https://raw.githubusercontent.com/krshrimali/blog/main/assets/cover-images/Vector-Part-2.png\"\u003e\u003c/p\u003e\n\u003cp\u003eThat is, we will start from calling a vector constructor and then see how memory is allocated. If you haven\u0026rsquo;t looked at the previous blog post, please take a look \u003ca href=\"https://krshrimali.github.io/posts/2020/04/understanding-how-vectors-work-in-c-part-1-how-does-push_back-work/\"\u003ehere\u003c/a\u003e. I want to be thorough with the blog post, so I\u0026rsquo;ll divide this into multiple posts. By the end of this post, you\u0026rsquo;ll go through the following structs:\u003c/p\u003e","title":"Understanding how Vectors work in C++ (Part-2): What happens when you initialize a vector?"},{"content":"In this blog, we\u0026rsquo;ll continue diving deep into the source code of Vector Containers in GCC compiler. Today, we will be discussing some of the most commonly used methods of vectors, and how they are implemented.\nBefore we start, if you haven\u0026rsquo;t looked at the previous blogs in the C++ series, please take a look here. If you are already familiar with memory allocation in vector containers and vector\u0026rsquo;s base structs, then you can skip reading the previous blogs and continue here. If not, I suggest you reading them.\nLet\u0026rsquo;s start off with pop_back member function, which essentially deletes the last element from the vector and reduces the size by one. Let\u0026rsquo;s take a look how it is used:\n# Initialize a vector using initializer list std::vector\u0026lt;int\u0026gt; X {1, 2, 3}; X.pop_back(); for (auto const\u0026amp; element: X) { std::cout \u0026lt;\u0026lt; element \u0026lt;\u0026lt; \u0026#34; \u0026#34;; } You will see the output as: 1 2. If you are wondering how this works in the case of a 2D vector, let\u0026rsquo;s take a look:\n# Initialize a 2D vector using initializer list std::vector\u0026lt;std::vector\u0026lt;int\u0026gt;\u0026gt; X { {1, 2, 3}, {4, 5, 6} }; X.pop_back(); for (auto const\u0026amp; element: X) { for (auto const\u0026amp; _element: element) { std::cout \u0026lt;\u0026lt; _element \u0026lt;\u0026lt; \u0026#34; \u0026#34;; } std::cout \u0026lt;\u0026lt; \u0026#34;\\n\u0026#34;; } You will see the output as: 1 2 3. As you can notice, it popped back the last element which was indeed a vector. Let\u0026rsquo;s start diving deep in the source code now, starting with declaration:\nvoid pop_back() { __glibcxx_required_nonempty(); __this-\u0026gt;_M_impl._M_finish; _Alloc_traits::destroy(this-\u0026gt;_M_impl, this-\u0026gt;_M_impl._M_finish); _GLIBCXX_ASAN_ANNOTATE_SHRINK(1); } A short note on _GLIBCXX_NOEXCEPT operator (noexcept since C++11): It returns true if the expression or member function is required to not throw any exceptions. _GLIBCXX_NOEXCEPT is defined as noexcept for C++ versions \u0026gt;= 2011:\nif __cplusplus \u0026gt;= 201103L # define _GLIBCXX_NOEXCEPT noexcept You can use a condition by using _GLIBCXX_NOEXCEPT_IF(condition) which essentially calls noexcept(condition). One use of this is when you want to access a particular index in a vector, you can avoid check if the location exists or not by using noexcept.\nWhen you call pop_back the design rule first checks if the vector is empty or not. If it\u0026rsquo;s nonempty, only then it makes sense to pop the last element, right? This is done by using __glibcxx_required_nonempty() call. The definition of this macro is:\n# define __glibcxx_requires_nonempty() __glibcxx_check_nonempty() As you can see, it\u0026rsquo;s calling __glibcxx_check_nonempty() macro which checks using this-\u0026gt;empty() call:\n# define __glibcxx_check_nonempty() \\ _GLIBCXX_DEBUG_VERIFY(! this-\u0026gt;empty(), _M_message(::__gnu_debug::__msg_empty)._M_sequence(*this, \u0026#34;this)) These are typical GCC macros for assertions. If we the vector is nonempty, we now move forward in fetching the last location in the memory of our vector container (using _M_impl._M_finish pointer), please take a look at the previous blogs if you aren\u0026rsquo;t aware of _M_impl struct. As the term suggests, we attempt to destroy the memory location using _Alloc_traits::destroy(this-\u0026gt;_M_impl, this-\u0026gt;_M_impl._M_finish). _Alloc_traits allows us to access various properties of the allocator used.\n// This function destroys an object of type _Tp template \u0026lt;typename _Tp\u0026gt; static void destroy(_Alloc\u0026amp; __a, _Tp\u0026amp; __p) noexcept(noexcept(_S_destroy(__a, __p, 0)) { _S_destroy(__a, __p, 0); } According to the official documentation of destroy static function: It calls __a.destroy(__p) if that expression is well-formed, other wise calls __p-\u0026gt;~_Tp(). If we take a look at the existing overloads of _S_destroy:\ntemplate \u0026lt;typename _Alloc2, typename _Tp\u0026gt; static auto _S_destroy(_Alloc2\u0026amp; __a, _Tp* __p, int) noexcept(noexcept(__a.destroy(__p))) -\u0026gt; decltype(__a.destroy(__p)) { __a.destroy(__p); } template \u0026lt;typename _Alloc2, typename _Tp\u0026gt; static void _S-destroy(_Alloc2\u0026amp; __a, _Tp* __p, ...) noexcept(noexcept(__p-\u0026gt;~_Tp())) { __p-\u0026gt;~_Tp(); } So clearly, if the expression is well-formed, it will call our allocator\u0026rsquo;s destroy method and pass the pointer location in that call. Otherwise, it calls the destructor of the pointer itself (__p-\u0026gt;~_Tp()). Once successfully done, we reduce the size by 1 using:\n# define _GLIBCXX_ASAN_ANNOTATE_SHRINK(n) \\ _Base::_Vector_impl::template _Asan\u0026lt;\u0026gt;::_S_shrink(this-\u0026gt;_M_impl, n) As you would see, the macro calls _S_shrink function to sanitize the vector container (i.e. reduce the size by n, here 1):\ntemplate \u0026lt;typename _Up\u0026gt; struct _Asan\u0026lt;allocator\u0026lt;_Up\u0026gt;\u0026gt; { static void _S_adjust(_Vector_impl\u0026amp; __impl, pointer __prev, pointer _curr) { __sanitizer_annotate_contiguous_container(__impl._M_start, __impl._M_end_of_storage, __prev, __curr); } static void _S_shrink(_Vector_impl\u0026amp; __impl, size_type __n) { _S_adjust(__impl, __impl._M_finish + __n, __impl._M_finish); } } We don\u0026rsquo;t need to go deeper into these calls, but (as per official documentation), the call _S_adjust adjusts ASan annotation for [_M_start, _M_end_of_storage) to mark end of valid region as __curr instead of __prev (note that we already had deleted the last element, so __impl.__M_finish + __n (here __n is 1) will be the old pointer).\nA good useful note here is, that pop_back function isn\u0026rsquo;t marked noexcept as we already have conditions to check the container being non-empty. In case there is any failure, the debug macros are called and throw necessary exceptions.\nLet\u0026rsquo;s go ahead and take a look at a few other member functions (there are many, take a look here: https://en.cppreference.com/w/cpp/container/vector, I only discuss those which are commonly used)\nback(): Let\u0026rsquo;s take a look at back call. As the name suggests (and as we saw before), this returns the last element in the vector container. It can be used as X.back() where X is a valid vector container. Let\u0026rsquo;s take a look at how it is implemented in GCC:\nreference back() { _glibcxx_requires_nonempty(); return *(end() - 1); } // definition of end() iterator end() { return iterator(this-\u0026gt;_M_impl._M_finish); } Note that end() points to one past the last element in the vector. That\u0026rsquo;s why we do end()-1 in the definition of back function. This should now be pretty obvious, that why use assertion _glibcxx_requires_nonempty() as we want to make sure that we are returning valid memory location.\nfront(): It should be very similar to what we saw with back(). This returns reference to the first element of the vector.\nreference front() { _glibcxx_requires_nonempty(); return *begin(); } // definition of begin() iterator begin() { return iterator(this-\u0026gt;_M_impl._M_start); } Note how we use the pointers _M_start and _M_finish to access first and the last elements of the vector container respectively.\nreserve(): Some times we want to pre-allocate memory to a vector container. You can do that using X.reserve(10) to reserve enough size for 10 elements (integers if X is std::vector\u0026lt;int\u0026gt; type).\nvoid reserve(size_type __n) { if (__n \u0026gt; max_size()) _throw_length_error(__N(\u0026#34;vector::reserve\u0026#34;)); if (capacity() \u0026lt; __n) _M_reallocate(__n); } So when you want to pre-allocate memory, there are 3 possibilities:\nThere is already enough memory allocated. No need to allocate. (Case of capacity() \u0026gt; __n) There is not enough memory allocated. Need to reallocate memory. (Case of capacity() \u0026lt; __n) The required size is greater than maximum size possible, then lenght error is thrown. (Case of __n \u0026gt; max_size()) size(): This will return the size of the vector container:\nsize_type size() const { return size_type(end() - begin()); } So, let\u0026rsquo;s say you have reserved memory for 10 elements, then size() will return 10.\ncapacity(): This returns the size the container can store currently.\nsize_type capacity() const { return size_type(const_iterator(this-\u0026gt;_M_impl._M_end_addr(), 0) - begin()); } Here, _M_end_addr() returns address of (end of storage + 1) location (if the pointer to this-\u0026gt;_M_impl._M_end_of_storage exists).\nThere maybe a few member functions that I missed, but I\u0026rsquo;m sure the tutorials so far in the Vectors series are (hopefully) enough to help you out with understanding the source code.\nWith this blog post, we are also done with the vector series in C++, and coming up next, we will take a look on using all of this knowledge to implement useful utilities for vectors while implementing libraries and projects, and also other design patterns in C++.\nAcknowledgement I have received a lot of love and support for these blogs, and I am grateful to each and everyone of you! I write these blogs to share what I know with others and in a hope to motivate people to not fear when looking at the source code of any library. I think, reading codes is a good practice.\nI am thankful to Martin York (aka Loki Astari on stackoverflow) for his constructive feedback on my blogs. Special thanks to Ujval Kapasi for taking time to read through my blogs and giving valuable feedback.\nI was, am and will always be grateful to my elder brother Vishwesh Ravi Shrimali (also my all time mentor) who helped me getting started with C++, AI and whatever I have been doing recently. He inspires me.\n","permalink":"https://krshrimali.github.io/posts/2020/04/understanding-how-vectors-work-in-c-part-3-diving-deep-into-member-functions-of-vectors/","summary":"\u003cp\u003eIn this blog, we\u0026rsquo;ll continue diving deep into the source code of Vector Containers in GCC compiler. Today, we will be discussing some of the most commonly used methods of vectors, and how they are implemented.\u003c/p\u003e\n\u003cp\u003e\u003cimg loading=\"lazy\" src=\"https://raw.githubusercontent.com/krshrimali/blog/main/assets/cover-images/Vector-Part-3.png\"\u003e\u003c/p\u003e\n\u003cp\u003eBefore we start, if you haven\u0026rsquo;t looked at the previous blogs in the C++ series, please take a look \u003ca href=\"https://krshrimali.github.io/categories/cpp/\"\u003ehere\u003c/a\u003e. If you are already familiar with memory allocation in vector containers and vector\u0026rsquo;s base structs, then you can skip reading the previous blogs and continue here. If not, I suggest you reading them.\u003c/p\u003e","title":"Understanding how Vectors work in C++ (Part-3): Diving deep into member functions of vectors"},{"content":"This blog is focused to explain how vectors work in the backend, and we\u0026rsquo;ll specially look at push_back method of the vector container. Looking at the source code helps to understand the implementation, and how vectors can be used efficiently.\nVector Containers are type of sequenced containers in C++ commonly uses as a better alternative of arrays. They are also known as dynamic arrays, and as the term suggests - it\u0026rsquo;s one of the advantages they hold over native arrays in C++. You might have heard of Standard Library containers like vector, set, queue, priority_queue before. They all implement methods defined by the Container Concept.\nA few important notes before we start:\nI\u0026rsquo;m using GCC 10.0.1 which is in the development stage. I\u0026rsquo;ve built GCC 10.0.1 from source on my local system. But everything I discuss here, should be same with GCC 8.4 or GCC 9.3 releases. I assume you are at least using C++11. If for any reason you are using C++98, there might be a few differences (for example, variadic arguments were not present in C++98). To not include lots of macros to check C++ versions, I\u0026rsquo;ve at times assumed the reader is using C++11 or greater. This blog uses lots of C++ Design Patterns that many would not be aware of. I understand it might just be a good idea to explain them first in a blog, but for now - I assume you have at least heard of them and know a thing or two about C++. I\u0026rsquo;ll cover these in future. Let\u0026rsquo;s start with a basic comparison of using arrays and vectors in C++:\n// Create an array of fixed size: 10 int* InputArray = new int[10]; for (int i = 0; i \u0026lt; 10; i++) { // Let\u0026#39;s assign values to the array // Values are same as indices InputArray[i] = i; } We can do the same (from what you see above) using vector:\n// Include this to be able to use vector container #include \u0026lt;vector\u0026gt; std::vector\u0026lt;int\u0026gt; InputVector {}; for (int i = 0; i \u0026lt; 10; i++) { InputVector.push_back(i); } While both do the same, but there are many important differences that happen in the backend. Let\u0026rsquo;s start with performance.\nThe piece of code using vector containers in C++ took 23.834 microseconds. The piece of code using arrays in C++ took 3.26 microseconds. If we had to do this for 10k numbers, the performance might be significant:\nThe piece of code using vector containers in C++ (for 10k numbers) took 713 microseconds. The piece of code using arrays in C++ took 173 microseconds. As in software development, there is always a tradeoff. Since vectors aim to provide dynamic memory allocation, they lose some performance while trying to push_back elements in the vectors since the memory is not allocated before. This can be constant if memory is allocated before.\nLet\u0026rsquo;s try to infer this from the source code of vector container. The signature of a vector container looks like this:\ntemplate\u0026lt;typename _Tp, typename _Alloc = std::allocator\u0026lt;_Tp\u0026gt; \u0026gt; class vector : protected _Vector_base\u0026lt;_Tp, _Alloc\u0026gt; Where _Tp is the type of element, and _Alloc is the allocator type (defaults to std::allocator\u0026lt;_Tp\u0026gt;). Let\u0026rsquo;s start from the constructor of vector (when no parameter is passed):\n#if __cplusplus \u0026gt;= 201103L vector() = default; #else vector() { } #endif The constructor when called with no params, creates a vector with no elements. As always, there are various ways to initialize a vector object.\nI want to focus more on push_back today, so let\u0026rsquo;s take a look at it\u0026rsquo;s signature. It\u0026rsquo;s located in stl_vector.h file.\n// Note that value_type is defined as: typedef _Tp value_type as a public type void push_back(const value_type\u0026amp; __x) { if (this-\u0026gt;_M_impl._M_finish != this-\u0026gt;_M_impl._M_end_of_storage) { _GLIBCXX_ASAN_ANNOTATE_GROW(1); _Alloc_traits::construct(this-\u0026gt;_M_impl, this-\u0026gt;_M_impl._M_finish, __x); ++this-\u0026gt;_M_impl._M_finish; _GLIBCXX_ASAN_ANNOTATE_GREW(1); } else _M_realloc_insert(end(), __x); } A few notes to take:\nvalue_type: This is the type of the elements in the vector container. That is, if the vector is std::vector\u0026lt;std::vector\u0026lt;int\u0026gt; \u0026gt;, then value_type of the given vector will be std::vector\u0026lt;int\u0026gt;. This comes handy later for type checking and more.\n_GLIBCXX_ASAN_ANNOTATE_GROW(1): The definition of this macro is:\n#define _GLIBCXX_ASAN_ANNOTATE_GROW(n) \\ typename _Base::_Vector_impl::template _Asan\u0026lt;\u0026gt;::_Grow \\ __attribute__((__unused__)) __grow_guard(this-\u0026gt;_M_impl, (n)) The base struct _Vector_base defines these functions and structs. Let\u0026rsquo;s take a look at struct _Asan. Essentially, all we want to do with the above macro is to grow the vector container memory by n. Since when we insert an element, we only need to grow by 1, so we pass 1 to the macro call. template\u0026lt;typename = _Tp_alloc_type\u0026gt; struct _Asan { typedef typename __gnu_cxx::__alloc_traits\u0026lt;_Tp_alloc_type\u0026gt;::size_type size_type; struct _Grow { _Grow(_Vector_impl\u0026amp;, size_type) { } void _M_grew(size_type) { } }; // ... }; If usage of Macros is new to you, please leave it for now as we\u0026rsquo;ll discuss more about these design patterns in future.\nA note on usage of _M_impl. It is declared as: _Vector_impl\u0026amp; _M_impl in the header file. _Vector_impl is a struct defined as:\nstruct _Vector_impl : public _Tp_alloc_type, public _Vector_impl_data { _Vector_impl() _GLIBCXX_NOEXCEPT_IF(is_nothrow_default_constructible\u0026lt;_Tp_alloc_type\u0026gt;::value) : _Tp_alloc_type() { } } // more overloads for the constructor The base struct _Vector_impl_data gives you helpful pointers to access later on:\nstruct _Vector_impl_data { pointer _M_start; pointer _M_finish; pointer _M_end_of_storage; // overloads of constructors } To go deep into the details is not useful here, but as you would have sensed, this helps us to access pointer to the start, finish and end of storage of the vector.\nYou would have guessed by now, that push_back call will add the element to the end (observe _Alloc_traits::construct(this-\u0026gt;_M_impl, this-\u0026gt;_M_impl._M_finish, __x);) and will then increment the variable _M_finish by 1.\nNote how push_back first checks if there is memory available. Of course we have limited memory available with us, and it checks if the end location of the current vector container equals the end storage capacity:\nif (this-\u0026gt;_M_impl._M_finish != this-\u0026gt;_M_impl._M_end_of_storage) { // ... } else { _M_realloc_insert(end(), __x); } So if we have reached the end of storage, it calls _M_realloc_insert(end(), __x). Now what is this? Let\u0026rsquo;s take a look at it\u0026rsquo;s definition:\ntemplate \u0026lt;typename _Tp, typename _Alloc\u0026gt; template\u0026lt;typename... _Args\u0026gt; void vector\u0026lt;_Tp, _Alloc\u0026gt;::_M_realloc_insert(iterator __position, _Args\u0026amp;\u0026amp;... __args) { // ... pointer __old_start = this-\u0026gt;_M_impl._M_start; pointer __old_finish = this-\u0026gt;_M_impl._M_finish; // Here we have passed __position as end() // So __elems_before will be total number of elements in our original vector const size_type __elems_before = __position - begin(); // Declare new starting and finishing pointers pointer __new_start(this-\u0026gt;_M_allocate(__len)); pointer __new_finish(__new_start); __try { // Allocate memory and copy original vector to the new memory locations } __catch(...) // Destroy the original memory location std::_Destroy(__old_start, __old_finish, _M_get_Tp_allocator()); // Change starting, finishing and end of storage pointers to new pointers this-\u0026gt;_M_impl._M_start = __new_start; this-\u0026gt;_M_impl._M_finish = __new_finish; // here __len is 1 this-\u0026gt;_M_impl._M_end_of_storage = __new_start + __len; } Even though the above piece of code might scare a few (it did scare me when I looked at it for the first time), but just saying - this is just 10% of the definition of _M_realloc_insert.\nIf you haven\u0026rsquo;t noticed so far, there is something very puzzling in the code: template\u0026lt;typename... _Args\u0026gt; \u0026ndash; these are variadic arguments introduced in C++11. We\u0026rsquo;ll talk about them later in the series of blogs.\nIntuitively, by calling _M_realloc_insert(end(), __x) all we are trying to do is reallocate memory (end_of_storage + 1), copy the original vector data to the new memory locations, add __x and deallocate (or destroy) the original memory in the heap. This also allows to keep vector to have contiguous memory allocation.\nFor today, I think we discussed a lot about vectors and their implementation in GCC. We\u0026rsquo;ll continue to cover rest of the details in the next part of the blog. I\u0026rsquo;m sure, the next time you plan to use push_back - you\u0026rsquo;ll know how things are happening in the backend. Till then, have fun and take care! :)\nA request For the past year, I\u0026rsquo;ve been writing blogs on PyTorch C++ API. I\u0026rsquo;ve been overwhelmed with your feedback, help and comments. Thank you! This series of blogs on C++, is experimental for now. I love reading source codes, and explaining it to readers. I hope this helps. Please leave your comment and feedback here, or reach out to me at kushashwaravishrimali@gmail.com if you wish. Even if you don\u0026rsquo;t like this, say it! I promise, I\u0026rsquo;ll be better next time.\n","permalink":"https://krshrimali.github.io/posts/2020/04/understanding-how-vectors-work-in-c-part-1-how-does-push_back-work/","summary":"\u003cp\u003eThis blog is focused to explain how vectors work in the backend, and we\u0026rsquo;ll specially look at \u003ccode\u003epush_back\u003c/code\u003e method of the vector container. Looking at the source code helps to understand the implementation, and how vectors can be used efficiently.\u003c/p\u003e\n\u003cp\u003e\u003cimg loading=\"lazy\" src=\"https://raw.githubusercontent.com/krshrimali/blog/main/assets/cover-images/Vector-Part-1.png\"\u003e\u003c/p\u003e\n\u003cp\u003eVector Containers are type of sequenced containers in C++ commonly uses as a better alternative of arrays. They are also known as dynamic arrays, and as the term suggests - it\u0026rsquo;s one of the advantages they hold over native arrays in C++. You might have heard of Standard Library containers like \u003ccode\u003evector\u003c/code\u003e, \u003ccode\u003eset\u003c/code\u003e, \u003ccode\u003equeue\u003c/code\u003e, \u003ccode\u003epriority_queue\u003c/code\u003e before. They all implement methods defined by the Container Concept.\u003c/p\u003e","title":"Understanding how Vectors work in C++ (Part-1): How does push_back work?"},{"content":"It\u0026rsquo;s been around 5 months since I released my last blog on DCGAN Review and Implementation using PyTorch C++ API and I\u0026rsquo;ve missed writing blogs badly! Straight the to the point, I\u0026rsquo;m back!\nBut before we start, the PyTorch C++ Frontend has gone through several changes and thanks to the awesome contributors around the world, it resembles the Python API more than it ever did! Since a lot of things have changed, I have also updated my previous blogs (tested on 1.4 Stable build).\nWhat has changed? There have been major changes in the PyTorch C++ Frontend API (Libtorch) and we\u0026rsquo;ll be discussing some of them which were related to our implementation on DCGAN. Let\u0026rsquo;s see, what parts of our code have changed in the recent Libtorch version. Well, the frontend API of PyTorch in C++ resembles closely to Python now:\nFor what concerns our code on DCGAN, quoting the author (Will Feng) of PR #28917.\nIn Conv{1,2,3}dOptions: - with_bias is renamed to bias. - input_channels is renamed to in_channels. - output_channels is renamed to out_channels. - The value of transposed doesn\u0026rsquo;t affect the behavior of Conv{1,2,3}d layers anymore. Users should migrate their code to use ConvTranspose{1,2,3}d layers instead.\nSo, starting first, we need to change with_bias to bias in our model definitions. The generator class in DCGAN uses Transposed Convolutions, and that\u0026rsquo;s why we need to migrate from torch::nn::Conv2dOptions class to torch::nn::ConvTranspose2dOptions (this is because using .transposed(true/false) does not work anymore on torch::nn::Conv2dOptions).\nThat is all for the changes we needed to make. To make it easy to track changes and use the code I wrote, I\u0026rsquo;ve made the project public on GitHub. Feel free to file an issue in case you hit a bug/error.\nTime to talk about results!\nResults The aim of this blog is to get DCGAN running on our celebA dataset using PyTorch C++ Frontend API. I\u0026rsquo;m in no way aiming to produce the best possible results. I trained the DCGAN network on celebA dataset for 10 epochs. In order to visualize results, for every checkpoint (where we save our models), we pass a sample noise image (64 images here) to the generator and save the output:\n// equivalent to using torch.no_grad() in Python auto options = torch::TensorOptions().device(device).requires_grad(false); // netG is our sequential generator network // args.nz = 100 in my case torch::Tensor samples = netG-\u0026gt;forward(torch::randn({64, args.nz, 1, 1}, options)); // save the output torch::save(samples, torch::str(\u0026#34;dcgan-sample-\u0026#34;, ++checkpoint_counter, \u0026#34;.pt\u0026#34;)); Once we have the saved output, we can load the file and produce output (find the display_samples.py file in the GitHub repo for this blog). Here is how the output looks like, after 10 epochs of training:\nAnd how about an animation?\nIsn\u0026rsquo;t this amazing?\nThat\u0026rsquo;s it for this blog. See you around! :)\n","permalink":"https://krshrimali.github.io/posts/2020/02/training-and-results-deep-convolutional-generative-adversarial-networks-on-celeba-dataset-using-pytorch-c-api/","summary":"\u003cp\u003eIt\u0026rsquo;s been around 5 months since I released my last blog on \u003ca href=\"https://krshrimali.github.io/posts/2019/09/deep-convolutional-generative-adversarial-networks-review-and-implementation-using-pytorch-c-api/\"\u003eDCGAN Review and Implementation using PyTorch C++ API\u003c/a\u003e and I\u0026rsquo;ve missed writing blogs badly! Straight the to the point, I\u0026rsquo;m back!\u003c/p\u003e\n\u003cp\u003e\u003cimg loading=\"lazy\" src=\"https://raw.githubusercontent.com/krshrimali/blog/main/assets/cover-images/Cover-DCGAN-2.jpg\"\u003e\u003c/p\u003e\n\u003cp\u003eBut before we start, the PyTorch C++ Frontend has gone through several changes and thanks to the awesome contributors around the world, it resembles the Python API more than it ever did! Since a lot of things have changed, I have also updated my previous blogs (tested on 1.4 Stable build).\u003c/p\u003e","title":"[Training and Results] Deep Convolutional Generative Adversarial Networks on CelebA Dataset using PyTorch C++ API"},{"content":"I\u0026rsquo;m pleased to start a series of blogs on GANs and their implementation with PyTorch C++ API. We\u0026rsquo;ll be starting with one of the initial GANs - DCGANs (Deep Convolutional Generative Adversarial Networks).\nThe authors (Soumith Chintala, Radford and Luke Metz) in this Seminal Paper on DCGANs introduced DCGANs to the world like this:\nWe introduce a class of CNNs called deep convolutional generative adversarial networks (DCGANs), that have certain architectural constraints, and demonstrate that they are a strong candidate for unsupervised learning. Training on various image datasets, we show convincing evidence that our deep convolutional adversarial pair learns a hierarchy of representations from object parts to scenes in both the generator and discriminator. Additionally, we use the learned features for novel tasks - demonstrating their applicability as general image representations.\nEven though, the introduction to DCGANs is quite lucid, but here are some points to note:\nDCGANs are a class of Convolutional Neural Networks. They are a strong candidate for Unsupervised Learning. They are applicable as general image representations as well. Let\u0026rsquo;s go ahead and see what exactly is DCGAN?\nIntroduction to DCGAN At the time when this paper was released, there was quite a focus on Supervised Learning. The paper aimed at bridging the gap between Unsupervised Learning and Supervised Learning. DCGANs are a way to understand and extract important feature representations from a dataset and generate good image representations by training.\nAny Generative Adversarial Network has 2 major components: a Generator and a Discriminator. The tasks for both of them are simple.\nGenerator: Generates Images similar to the data distribution such that Discriminator can not distinguish it with the original data. Discriminator: Discriminator has a task on accurately distinguishing between the image from the generator and from the data distribution. It basically has to recognize an image as fake or real, correctly. Both Generator and Discriminator tasks can be represented beautifully with the following equation:\nThe above equation, shows how the Generator and Discriminator plays min-max game.\nThe Generator tries to minimize the loss function. It follows up with two cases: When the data is from the data distribution: Generator has a task of forcing the Discriminator to predict the data as fake. When data is from the Generator: Generator has a task of forcing the Discriminator to predict the data as real. The Discriminator tries to maximize the loss function. It follows up with two cases: When the data is from the data distribution: Discriminator tries to predict the data as real. When the data is from the Generator: Discriminator tries to predict the data as fake. Fundamentally, the Generator is trying to fool the Discriminator and the Discriminator is trying not to get fooled with. Because of it\u0026rsquo;s analogy, it\u0026rsquo;s also called a police-thief game. (Police is the Discriminator and thief is the Generator).\nWe have good enough discussion on GANs, to kickstart discussion on DCGANs. Let\u0026rsquo;s go ahead and see what changes they proposed on common CNNs:\nChanges in the Generator:\nSpatial Pooling Layers such as MaxPool Layers were replaced with Fractional-Strided Convolutions (a.k.a Transposed Convolutions). This allows the network to learn it\u0026rsquo;s own spatial downsampling, instead of explicitly mentioning the downsampling parameters by Max Pooling. Use BatchNorm in the Generator. Remove Fully Connected layers for deeper architectures. Use ReLU activation function for all the layers except the output layer (which uses Tanh activation function). Changes in the Discriminator:\nSpatial Pooling Layers such as MaxPool layers were replaced with Strided Convolutions. Use BatchNorm in the Discriminator. Remove FC layers for deeper architectures. Use LeakyReLU activation function for all the layers in the Discriminator. Generator of the DCGAN used for LSUN scene modeling. Source: https://arxiv.org/pdf/1511.06434.pdf\nAs you would note in the above architecture, there is absence of spatial pooling layers and fully connected layers.\nDiscriminator of the DCGAN used for LSUN scene modeling. Source: https://github.com/ChengBinJin/DCGAN-TensorFlow\nNotably again, there are no pooling and fully connected layers (except the last layer).\nLet\u0026rsquo;s start with defining the architectures of both Generators and Discriminators using PyTorch C++ API. I used the Object Oriented approach by making class, each for Generator and Discriminator. Note that each of them are a type of CNNs, and also inherit functions (or methods) from torch::nn::Module class.\nAs mentioned before, Generator uses Transposed Convolutional Layers and has no pooling and FC layers. It also uses ReLU Activation Function (except the last layer). The parameters used for the Generator include:\ndataroot: (type: std::string) Path of the dataset\u0026rsquo;s root directory. workers: (type: int) Having more workers will increase CPU memory usage. (Check this link for more details) batch_size: (type: int) Batch Size to consider. image_size: (type: int) Size of the image to resize it to. nc: (type: int) Number of channels in the Input Image. nz: (type: int) Length of latent vector, from which the input image is taken. ngf: (type int) Depth of feature maps carried through the generator. num_epochs: (type int) Number of epochs for which the model is trained. lr: (type float) Learning Rate for training. Authors described it to be 0.0002 beta1: (type: float) Hyperparameter for Optimizer used (Adam). ngpu: (type: int) Number of GPUs available to use. (use 0 if no GPU available) class Generator : public torch::nn::Module { private: std::string dataroot; int workers; int batch_size; int image_size; int nc; int nz; int ngf; int num_epochs; float lr; float beta1; int ngpu; public: torch::nn::Sequential main; Generator(std::string dataroot_ = \u0026#34;data/celeba\u0026#34;, int workers_ = 2, int batch_size_ = 128, int image_size_ = 64, int nc_ = 3, int nz_ = 100, int ngf_ = 64, int ndf_ = 64, int num_epochs_ = 5, float lr_ = 0.0002, float beta1_ = 0.5, int ngpu_ = 0) { // Set the arguments dataroot = dataroot_; workers = workers_; batch_size = batch_size_; image_size = image_size_; nc = nc_; nz = nz_; ngf = ngf_; ndf = ndf_; num_epochs = num_epochs_; lr = lr_; beta1 = beta1_; ngpu = ngpu_; main = torch::nn::Sequential( torch::nn::Conv2d(torch::nn::Conv2dOptions(nz, ngf*8, 4).stride(1).padding(0).with_bias(false).transposed(true)), torch::nn::BatchNorm(ngf*8), torch::nn::Functional(torch::relu), torch::nn::Conv2d(torch::nn::Conv2dOptions(ngf*8, ngf*4, 4).stride(2).padding(1).with_bias(false).transposed(true)), torch::nn::BatchNorm(ngf*4), torch::nn::Functional(torch::relu), torch::nn::Conv2d(torch::nn::Conv2dOptions(ngf*4, ngf*2, 4).stride(2).padding(1).with_bias(false).transposed(true)), torch::nn::BatchNorm(ngf*2), torch::nn::Functional(torch::relu), torch::nn::Conv2d(torch::nn::Conv2dOptions(ngf*2, ngf, 4).stride(2).padding(1).with_bias(false).transposed(true)), torch::nn::BatchNorm(ngf), torch::nn::Functional(torch::relu), torch::nn::Conv2d(torch::nn::Conv2dOptions(ngf, nc, 4).stride(2).padding(1).with_bias(false).transposed(true)), torch::nn::Functional(torch::tanh) ); } torch::nn::Sequential main_func() { // Returns Sequential Model of the Generator return main; } }; Note how we used Transposed Convolution, by passing .transposed(true).\nSimilarly, we define the class for Discriminator.\nclass Discriminator : public torch::nn::Module { private: std::string dataroot; int workers; int batch_size; int image_size; int nc; int nz; int ndf; int num_epochs; float lr; float beta1; int ngpu; public: torch::nn::Sequential main; Discriminator(std::string dataroot_ = \u0026#34;data/celeba\u0026#34;, int workers_ = 2, int batch_size_ = 128, int image_size_ = 64, int nc_ = 3, int nz_ = 100, int ngf_ = 64, int ndf_ = 64, int num_epochs_ = 5, float lr_ = 0.0002, float beta1_ = 0.5, int ngpu_ = 1) { dataroot = dataroot_; workers = workers_; batch_size = batch_size_; image_size = image_size_; nc = nc_; nz = nz_; ngf = ngf_; ndf = ndf_; num_epochs = num_epochs_; lr = lr_; beta1 = beta1_; ngpu = ngpu_; main = torch::nn::Sequential( torch::nn::Conv2d(torch::nn::Conv2dOptions(nc, ndf, 4).stride(2).padding(1).with_bias(false)), torch::nn::Functional(torch::leaky_relu, 0.2), torch::nn::Conv2d(torch::nn::Conv2dOptions(ndf, ndf*2, 4).stride(2).padding(1).with_bias(false)), torch::nn::BatchNorm(ndf*2), torch::nn::Functional(torch::leaky_relu, 0.2), torch::nn::Conv2d(torch::nn::Conv2dOptions(ndf*2, ndf*4, 4).stride(2).padding(1).with_bias(false)), torch::nn::BatchNorm(ndf*4), torch::nn::Functional(torch::leaky_relu, 0.2), torch::nn::Conv2d(torch::nn::Conv2dOptions(ndf*4, ndf*8, 4).stride(2).padding(1).with_bias(false)), torch::nn::BatchNorm(ndf*8), torch::nn::Functional(torch::leaky_relu, 0.2), torch::nn::Conv2d(torch::nn::Conv2dOptions(ndf*8, 1, 4).stride(1).padding(0).with_bias(false)), torch::nn::Functional(torch::sigmoid) ); } torch::nn::Sequential main_func() { return main; } }; We can initialize these networks as shown below:\n// Uses default arguments if no args passed Generator gen = Generator() Discriminator dis = Discriminator() torch::nn::Sequential gen_model = gen.main_func() torch::nn::Sequential dis_model = dis.main_func() In case you are using a GPU, you can convert the models:\ntorch::Device device = torch::kCPU; if(torch::cuda::is_available()) { device = torch::kCUDA; } gen_model-\u0026gt;to(device); dis_model-\u0026gt;to(device); Note on Data Loading: In the past blogs, I\u0026rsquo;ve discussed on loading custom data. Please refer to the previous blogs for a quick review on loading data.\nLet\u0026rsquo;s go ahead and define optimizers and train our model. We use the parameters defined by the authors, for optimizer (Adam, beta = 0.5) and learning rate of 2e-4.\ntorch::optim::Adam gen_optimizer(gen_model-\u0026gt;parameters(), torch::optim::AdamOptions(2e-4).beta1(0.5)); torch::optim::Adam dis_optimizer(dis_model-\u0026gt;parameters(), torch::optim::AdamOptions(2e-4).beta1(0.5)); Time to write our training code. We are using CelebA dataset which looks like this:\nSource: http://mmlab.ie.cuhk.edu.hk/projects/CelebA.html The dataset is huge, and contains 10,177 number of identities and around ~200k number of face images. It also contains annotations, but since GANs are a way of unsupervised learning, so they don\u0026rsquo;t actually consider annotations. Before we move on, we\u0026rsquo;ll see a quick step by step review on training the Discriminator and Generator simultaneously:\nStep-1: Train Discriminator. Remember from above, the discriminator tries to maximize the loss function such that it predicts the fake images as fake and real images as real. As the first step for every training process, we set the gradients to zero. This helps in calculating correct gradients, and not getting confused with gradients stored in the previous iteration. First calculate discriminator loss on real images (that is, data from our dataset). We do this by getting data from the batch and labels as anything between 0.8 and 1.0 (since it\u0026rsquo;s real, we approximate it from 0.8 to 1.0). Do a forward pass to the discriminator network, and calculate output on the batch of data from our dataset. Calculate loss by using torch::binary_cross_entropy and backpropagate the loss. We now calculate discriminator loss on fake images (that is, data from the generator). For this, we take a noise of shape similar to the batch of data, and pass that noise to the generator. The labels are given zero values (as the images are fake). Again, calculate the loss by using torch::binary_cross_entropy and backpropagate the loss. Sum both the losses, discriminator loss on real images + discriminator loss on fake images. This will be our discriminator loss. We then update our parameters using the optimizer. Step-2: Train Generator. The task of a Generator is to minimize the loss function. Since it has to produce images which can fool the discriminator, so it only has to consider fake images. As the first step for every training process, we set the gradients to zero. This helps in calculating correct gradients, and not getting confused with gradients stored in the previous iteration. We use the fake images produced in the Step-1 and pass it to the discriminator. Fill the labels with 1. (since generator wants to fool the discriminator, by making it predict as real images). Calculate loss, by using torch::binary_cross_entropy and backpropagate the loss. Update the parameters using optimizer of the Generator. for(int epoch=1; epoch\u0026lt;=10; epoch++) { // Store batch count in a variable int batch_count = 0; // You can use torch::data::Example\u0026lt;\u0026gt;\u0026amp; batch: *data_loader for(auto\u0026amp; batch: *data_loader) { // Step-1: Train the Discriminator // Set gradients to zero netD-\u0026gt;zero_grad(); // Calculating discriminator loss on real images torch::Tensor images_real = batch.data.to(device); torch::Tensor labels_real = torch::empty(batch.data.size(0), device).uniform_(0.8, 1.0)); // Do a forward pass to the Discriminator network torch::Tensor output_D_real = netD-\u0026gt;forward(images_real); // Calculate the loss torch::Tensor loss_real_D = torch::binary_cross_entropy(output_D_real, labels_real); loss_real_D.backward(); // Calculate discriminator loss on fake images // Generate noise and do forward pass to generate fake images torch:Tensor fake_random = torch::randn({batch.data.size(0), args.nz, 1, 1}, device); torch::Tensor images_fake = netG-\u0026gt;forward(images_fake); torch::Tensor labels_fake = torch::zeros(batch.data.size(0), device); // Do a forward pass to the Discriminator network torch::Tensor output_D_fake = netD-\u0026gt;forward(images_fake); // Calculate the loss torch::Tensor loss_fake_D = torch::binary_cross_entropy(output_D_fake, labels_fake); loss_fake_D.backward(); // Total discriminator loss torch::Tensor loss_discriminator = loss_real_D + loss_fake_D; // Update the parameters dis_optimizer.step(); // Step-2: Train the Generator // Set gradients to zero netG-\u0026gt;zero_grad(); // calculating generator loss on fake images // Change labels_fake from all zeros to all ones labels_fake.fill_(1); // Do forward pass to the Discriminator on the fake images generated above torch::Tensor output_G_fake = netD-\u0026gt;forward(images_fake); // Calculate loss torch::Tensor loss_generator = torch::binary_cross_entropy(output_G_fake, labels_fake); loss_generator.backward(); // Update the parameters gen_optimizer.step(); std::cout \u0026lt;\u0026lt; \u0026#34;Epoch: \u0026#34; \u0026lt;\u0026lt; epoch \u0026lt;\u0026lt; \u0026#34;, Batch: \u0026#34; \u0026lt;\u0026lt; batch_count \u0026lt;\u0026lt; \u0026#34;, Gen Loss: \u0026#34; \u0026lt;\u0026lt; loss_generator.item\u0026lt;float\u0026gt;() \u0026lt;\u0026lt; \u0026#34;, Discriminator Loss: \u0026#34; \u0026lt;\u0026lt; loss_discriminator.item\u0026lt;float\u0026gt;() \u0026lt;\u0026lt; std::endl; batch_count++; } } We are all set to train our first DCGAN in C++ using Libtorch. How amazing it is?\nIn the coming blog, I\u0026rsquo;ll share the results and answer a few common questions on the architecture of DCGAN.\nAcknowledgement and References I would like to thank Will Feng and Piotr for their useful suggestions. The code used in this blog, is partially analogous to the official PyTorch examples repo on DCGAN using LibTorch. I\u0026rsquo;ve also referred the original paper by Soumith Chintala and others. The sources of reference images (for Network architectures) have been acknowledged in the captions of respective images.\n","permalink":"https://krshrimali.github.io/posts/2019/09/deep-convolutional-generative-adversarial-networks-review-and-implementation-using-pytorch-c-api/","summary":"\u003cp\u003eI\u0026rsquo;m pleased to start a series of blogs on GANs and their implementation with PyTorch C++ API. We\u0026rsquo;ll be starting with one of the initial GANs - DCGANs (Deep Convolutional Generative Adversarial Networks).\u003c/p\u003e\n\u003cp\u003e\u003cimg loading=\"lazy\" src=\"https://raw.githubusercontent.com/krshrimali/blog/main/assets/cover-images/Cover-DCGAN.jpg\"\u003e\u003c/p\u003e\n\u003cp\u003eThe authors (Soumith Chintala, Radford and Luke Metz) in \u003ca href=\"https://arxiv.org/pdf/1511.06434.pdf\"\u003ethis\u003c/a\u003e Seminal Paper on DCGANs introduced DCGANs to the world like this:\u003c/p\u003e\n\u003cblockquote\u003e\n\u003cp\u003eWe introduce a class of CNNs called deep convolutional generative adversarial networks (DCGANs), that have certain architectural constraints, and demonstrate that they are a strong candidate for unsupervised learning. Training on various image datasets, we show convincing evidence that our deep convolutional adversarial pair learns a hierarchy of representations from object parts to scenes in both the generator and discriminator. Additionally, we use the learned features for novel tasks - demonstrating their applicability as general image representations.\u003c/p\u003e","title":"Deep Convolutional Generative Adversarial Networks: Review and Implementation using PyTorch C++ API"},{"content":"Introduction to Xeus Cling Today, we are going to run our C++ codes in the Jupyter Notebook. Sounds ambitious? Not much. Let\u0026rsquo;s see how we do it using Xeus Cling.\nI\u0026rsquo;ll quote the definition of Xeus Cling on the official documentation website.\nxeus-cling is a Jupyter kernel for C++ based on the C++ interpreter cling and the native implementation of the Jupyter protocol xeus.\nJust like we use Python Kernel in the Jupyter Notebook, we can also use a C++ based interpreter cling combined with a Jupyter protocol called Xeus to reach closer to implementing C++ code in the notebook.\nInstalling Xeus Cling using Anaconda It\u0026rsquo;s pretty straight forward to install Xeus Cling using Anaconda. I\u0026rsquo;m assuming the user has Anaconda installed. Use this command to install Xeus Cling using Anaconda: conda install -c conda-forge xeus-cling.\nNote: Before using conda commands, you need to have it in your PATH variable. Use this command to add the path to conda to your system PATH variable: export PATH=~/anaconda3/bin/:$PATH.\nThe conventional way to install any such library which can create conflicts with existing libraries, is to create an environment and then install it in the environment.\nCreate a conda environment: conda create -n cpp-xeus-cling. Activate the environment you just created: source activate cpp-xeus-cling. Install xeus-cling using conda: conda install -c conda-forge xeus-cling. Once setup, let\u0026rsquo;s go ahead and get started with Jupyter Notebook. When creating a new notebook, you will see different options for the kernel. One of them would be C++XX where XX is the C++ version.\nClick on any of the kernel for C++ and let\u0026rsquo;s start setting up environment for PyTorch C++ API.\nYou can try and implement some of the basic commands in C++.\nThis looks great, right? Let\u0026rsquo;s go ahead and set up the Deep Learning environment.\nSetting up Libtorch in Xeus Cling Just like we need to give path to Libtorch libraries in CMakeLists.txt or while setting up XCode (for OS X users) or Visual Studio (for Windows Users), we will also load the libraries in Xeus Cling.\nWe will first give the include_path of Header files and library_path for the libraries. We will also do the same for OpenCV as we need it to load images.\n#pragma cling add_library_path(\u0026#34;/Users/krshrimali/Downloads/libtorch/lib/\u0026#34;) #pragma cling add_include_path(\u0026#34;/Users/krshrimali/Downloads/libtorch/include\u0026#34;) #pragma cling add_include_path(\u0026#34;/Users/krshrimali/Downloads/libtorch/include/torch/csrc/api/include/\u0026#34;) #pragma cling add_library_path(\u0026#34;/usr/local/Cellar/opencv/4.1.0_2/lib\u0026#34;) #pragma cling add_include_path(\u0026#34;/usr/local/Cellar/opencv/4.1.0_2/include/opencv4\u0026#34;) For OS X, the libtorch libraries will be in the format of .dylib. Ignore the .a files as we only need to load the .dylib files. Similarly for Linux, load the libraries in .so format located in the lib/ folder.\nFor Mac\n#pragma cling load(\u0026#34;/Users/krshrimali/Downloads/libtorch/lib/libiomp5.dylib\u0026#34;) #pragma cling load(\u0026#34;/Users/krshrimali/Downloads/libtorch/lib/libmklml.dylib\u0026#34;) #pragma cling load(\u0026#34;/Users/krshrimali/Downloads/libtorch/lib/libc10.dylib\u0026#34;) #pragma cling load(\u0026#34;/Users/krshrimali/Downloads/libtorch/lib/libtorch.dylib\u0026#34;) #pragma cling load(\u0026#34;/Users/krshrimali/Downloads/libtorch/lib/libshm.dylib\u0026#34;) For Linux\n#pragma cling load(\u0026#34;/opt/libtorch/lib/libc10.so\u0026#34;) #pragma cling load(\u0026#34;/opt/libtorch/lib/libgomp-4f651535.so.1\u0026#34;) #pragma cling load(\u0026#34;/opt/libtorch/lib/libtorch.so\u0026#34;) For OpenCV, the list of libraries is long.\nFor Mac\n#pragma cling load(\u0026#34;/usr/local/Cellar/opencv/4.1.0_2/lib/libopencv_datasets.4.1.0.dylib\u0026#34;) #pragma cling load(\u0026#34;/usr/local/Cellar/opencv/4.1.0_2/lib/libopencv_aruco.4.1.0.dylib\u0026#34;) #pragma cling load(\u0026#34;/usr/local/Cellar/opencv/4.1.0_2/lib/libopencv_bgsegm.4.1.0.dylib\u0026#34;) #pragma cling load(\u0026#34;/usr/local/Cellar/opencv/4.1.0_2/lib/libopencv_bioinspired.4.1.0.dylib\u0026#34;) #pragma cling load(\u0026#34;/usr/local/Cellar/opencv/4.1.0_2/lib/libopencv_calib3d.4.1.0.dylib\u0026#34;) #pragma cling load(\u0026#34;/usr/local/Cellar/opencv/4.1.0_2/lib/libopencv_ccalib.4.1.0.dylib\u0026#34;) #pragma cling load(\u0026#34;/usr/local/Cellar/opencv/4.1.0_2/lib/libopencv_core.4.1.0.dylib\u0026#34;) #pragma cling load(\u0026#34;/usr/local/Cellar/opencv/4.1.0_2/lib/libopencv_dnn_objdetect.4.1.0.dylib\u0026#34;) #pragma cling load(\u0026#34;/usr/local/Cellar/opencv/4.1.0_2/lib/libopencv_dnn.4.1.0.dylib\u0026#34;) #pragma cling load(\u0026#34;/usr/local/Cellar/opencv/4.1.0_2/lib/libopencv_dpm.4.1.0.dylib\u0026#34;) #pragma cling load(\u0026#34;/usr/local/Cellar/opencv/4.1.0_2/lib/libopencv_face.4.1.0.dylib\u0026#34;) #pragma cling load(\u0026#34;/usr/local/Cellar/opencv/4.1.0_2/lib/libopencv_features2d.4.1.0.dylib\u0026#34;) #pragma cling load(\u0026#34;/usr/local/Cellar/opencv/4.1.0_2/lib/libopencv_flann.4.1.0.dylib\u0026#34;) #pragma cling load(\u0026#34;/usr/local/Cellar/opencv/4.1.0_2/lib/libopencv_freetype.4.1.0.dylib\u0026#34;) #pragma cling load(\u0026#34;/usr/local/Cellar/opencv/4.1.0_2/lib/libopencv_fuzzy.4.1.0.dylib\u0026#34;) #pragma cling load(\u0026#34;/usr/local/Cellar/opencv/4.1.0_2/lib/libopencv_gapi.4.1.0.dylib\u0026#34;) #pragma cling load(\u0026#34;/usr/local/Cellar/opencv/4.1.0_2/lib/libopencv_hfs.4.1.0.dylib\u0026#34;) #pragma cling load(\u0026#34;/usr/local/Cellar/opencv/4.1.0_2/lib/libopencv_highgui.4.1.0.dylib\u0026#34;) #pragma cling load(\u0026#34;/usr/local/Cellar/opencv/4.1.0_2/lib/libopencv_img_hash.4.1.0.dylib\u0026#34;) #pragma cling load(\u0026#34;/usr/local/Cellar/opencv/4.1.0_2/lib/libopencv_imgcodecs.4.1.0.dylib\u0026#34;) #pragma cling load(\u0026#34;/usr/local/Cellar/opencv/4.1.0_2/lib/libopencv_imgproc.4.1.0.dylib\u0026#34;) #pragma cling load(\u0026#34;/usr/local/Cellar/opencv/4.1.0_2/lib/libopencv_line_descriptor.4.1.0.dylib\u0026#34;) #pragma cling load(\u0026#34;/usr/local/Cellar/opencv/4.1.0_2/lib/libopencv_ml.4.1.0.dylib\u0026#34;) #pragma cling load(\u0026#34;/usr/local/Cellar/opencv/4.1.0_2/lib/libopencv_objdetect.4.1.0.dylib\u0026#34;) #pragma cling load(\u0026#34;/usr/local/Cellar/opencv/4.1.0_2/lib/libopencv_optflow.4.1.0.dylib\u0026#34;) #pragma cling load(\u0026#34;/usr/local/Cellar/opencv/4.1.0_2/lib/libopencv_phase_unwrapping.4.1.0.dylib\u0026#34;) #pragma cling load(\u0026#34;/usr/local/Cellar/opencv/4.1.0_2/lib/libopencv_photo.4.1.0.dylib\u0026#34;) #pragma cling load(\u0026#34;/usr/local/Cellar/opencv/4.1.0_2/lib/libopencv_plot.4.1.0.dylib\u0026#34;) #pragma cling load(\u0026#34;/usr/local/Cellar/opencv/4.1.0_2/lib/libopencv_quality.4.1.0.dylib\u0026#34;) #pragma cling load(\u0026#34;/usr/local/Cellar/opencv/4.1.0_2/lib/libopencv_reg.4.1.0.dylib\u0026#34;) #pragma cling load(\u0026#34;/usr/local/Cellar/opencv/4.1.0_2/lib/libopencv_rgbd.4.1.0.dylib\u0026#34;) #pragma cling load(\u0026#34;/usr/local/Cellar/opencv/4.1.0_2/lib/libopencv_saliency.4.1.0.dylib\u0026#34;) #pragma cling load(\u0026#34;/usr/local/Cellar/opencv/4.1.0_2/lib/libopencv_sfm.4.1.0.dylib\u0026#34;) #pragma cling load(\u0026#34;/usr/local/Cellar/opencv/4.1.0_2/lib/libopencv_shape.4.1.0.dylib\u0026#34;) #pragma cling load(\u0026#34;/usr/local/Cellar/opencv/4.1.0_2/lib/libopencv_stereo.4.1.0.dylib\u0026#34;) #pragma cling load(\u0026#34;/usr/local/Cellar/opencv/4.1.0_2/lib/libopencv_stitching.4.1.0.dylib\u0026#34;) #pragma cling load(\u0026#34;/usr/local/Cellar/opencv/4.1.0_2/lib/libopencv_structured_light.4.1.0.dylib\u0026#34;) #pragma cling load(\u0026#34;/usr/local/Cellar/opencv/4.1.0_2/lib/libopencv_superres.4.1.0.dylib\u0026#34;) #pragma cling load(\u0026#34;/usr/local/Cellar/opencv/4.1.0_2/lib/libopencv_surface_matching.4.1.0.dylib\u0026#34;) #pragma cling load(\u0026#34;/usr/local/Cellar/opencv/4.1.0_2/lib/libopencv_text.4.1.0.dylib\u0026#34;) #pragma cling load(\u0026#34;/usr/local/Cellar/opencv/4.1.0_2/lib/libopencv_tracking.4.1.0.dylib\u0026#34;) #pragma cling load(\u0026#34;/usr/local/Cellar/opencv/4.1.0_2/lib/libopencv_video.4.1.0.dylib\u0026#34;) #pragma cling load(\u0026#34;/usr/local/Cellar/opencv/4.1.0_2/lib/libopencv_videoio.4.1.0.dylib\u0026#34;) #pragma cling load(\u0026#34;/usr/local/Cellar/opencv/4.1.0_2/lib/libopencv_videostab.4.1.0.dylib\u0026#34;) #pragma cling load(\u0026#34;/usr/local/Cellar/opencv/4.1.0_2/lib/libopencv_xfeatures2d.4.1.0.dylib\u0026#34;) #pragma cling load(\u0026#34;/usr/local/Cellar/opencv/4.1.0_2/lib/libopencv_ximgproc.4.1.0.dylib\u0026#34;) #pragma cling load(\u0026#34;/usr/local/Cellar/opencv/4.1.0_2/lib/libopencv_xobjdetect.4.1.0.dylib\u0026#34;) #pragma cling load(\u0026#34;/usr/local/Cellar/opencv/4.1.0_2/lib/libopencv_xphoto.4.1.0.dylib\u0026#34;) For Linux\n#pragma cling load(\u0026#34;/usr/local/lib/libopencv_aruco.so.4.1.0\u0026#34;) #pragma cling load(\u0026#34;/usr/local/lib/libopencv_bgsegm.so.4.1.0\u0026#34;) #pragma cling load(\u0026#34;/usr/local/lib/libopencv_bioinspired.so.4.1.0\u0026#34;) #pragma cling load(\u0026#34;/usr/local/lib/libopencv_calib3d.so.4.1.0\u0026#34;) #pragma cling load(\u0026#34;/usr/local/lib/libopencv_ccalib.so.4.1.0\u0026#34;) #pragma cling load(\u0026#34;/usr/local/lib/libopencv_core.so.4.1.0\u0026#34;) #pragma cling load(\u0026#34;/usr/local/lib/libopencv_datasets.so.4.1.0\u0026#34;) #pragma cling load(\u0026#34;/usr/local/lib/libopencv_dnn_objdetect.so.4.1.0\u0026#34;) #pragma cling load(\u0026#34;/usr/local/lib/libopencv_dnn.so.4.1.0\u0026#34;) #pragma cling load(\u0026#34;/usr/local/lib/libopencv_dpm.so.4.1.0\u0026#34;) #pragma cling load(\u0026#34;/usr/local/lib/libopencv_face.so.4.1.0\u0026#34;) #pragma cling load(\u0026#34;/usr/local/lib/libopencv_features2d.so.4.1.0\u0026#34;) #pragma cling load(\u0026#34;/usr/local/lib/libopencv_flann.so.4.1.0\u0026#34;) #pragma cling load(\u0026#34;/usr/local/lib/libopencv_freetype.so.4.1.0\u0026#34;) #pragma cling load(\u0026#34;/usr/local/lib/libopencv_fuzzy.so.4.1.0\u0026#34;) #pragma cling load(\u0026#34;/usr/local/lib/libopencv_gapi.so.4.1.0\u0026#34;) #pragma cling load(\u0026#34;/usr/local/lib/libopencv_hdf.so.4.1.0\u0026#34;) #pragma cling load(\u0026#34;/usr/local/lib/libopencv_hfs.so.4.1.0\u0026#34;) #pragma cling load(\u0026#34;/usr/local/lib/libopencv_highgui.so.4.1.0\u0026#34;) #pragma cling load(\u0026#34;/usr/local/lib/libopencv_imgcodecs.so.4.1.0\u0026#34;) #pragma cling load(\u0026#34;/usr/local/lib/libopencv_img_hash.so.4.1.0\u0026#34;) #pragma cling load(\u0026#34;/usr/local/lib/libopencv_imgproc.so.4.1.0\u0026#34;) #pragma cling load(\u0026#34;/usr/local/lib/libopencv_line_descriptor.so.4.1.0\u0026#34;) #pragma cling load(\u0026#34;/usr/local/lib/libopencv_ml.so.4.1.0\u0026#34;) #pragma cling load(\u0026#34;/usr/local/lib/libopencv_objdetect.so.4.1.0\u0026#34;) #pragma cling load(\u0026#34;/usr/local/lib/libopencv_optflow.so.4.1.0\u0026#34;) #pragma cling load(\u0026#34;/usr/local/lib/libopencv_phase_unwrapping.so.4.1.0\u0026#34;) #pragma cling load(\u0026#34;/usr/local/lib/libopencv_photo.so.4.1.0\u0026#34;) #pragma cling load(\u0026#34;/usr/local/lib/libopencv_plot.so.4.1.0\u0026#34;) #pragma cling load(\u0026#34;/usr/local/lib/libopencv_reg.so.4.1.0\u0026#34;) #pragma cling load(\u0026#34;/usr/local/lib/libopencv_rgbd.so.4.1.0\u0026#34;) #pragma cling load(\u0026#34;/usr/local/lib/libopencv_saliency.so.4.1.0\u0026#34;) #pragma cling load(\u0026#34;/usr/local/lib/libopencv_sfm.so.4.1.0\u0026#34;) #pragma cling load(\u0026#34;/usr/local/lib/libopencv_shape.so.4.1.0\u0026#34;) #pragma cling load(\u0026#34;/usr/local/lib/libopencv_stereo.so.4.1.0\u0026#34;) #pragma cling load(\u0026#34;/usr/local/lib/libopencv_stitching.so.4.1.0\u0026#34;) #pragma cling load(\u0026#34;/usr/local/lib/libopencv_structured_light.so.4.1.0\u0026#34;) #pragma cling load(\u0026#34;/usr/local/lib/libopencv_superres.so.4.1.0\u0026#34;) #pragma cling load(\u0026#34;/usr/local/lib/libopencv_surface_matching.so.4.1.0\u0026#34;) #pragma cling load(\u0026#34;/usr/local/lib/libopencv_text.so.4.1.0\u0026#34;) #pragma cling load(\u0026#34;/usr/local/lib/libopencv_tracking.so.4.1.0\u0026#34;) #pragma cling load(\u0026#34;/usr/local/lib/libopencv_videoio.so.4.1.0\u0026#34;) #pragma cling load(\u0026#34;/usr/local/lib/libopencv_video.so.4.1.0\u0026#34;) #pragma cling load(\u0026#34;/usr/local/lib/libopencv_videostab.so.4.1.0\u0026#34;) #pragma cling load(\u0026#34;/usr/local/lib/libopencv_xfeatures2d.so.4.1.0\u0026#34;) #pragma cling load(\u0026#34;/usr/local/lib/libopencv_ximgproc.so.4.1.0\u0026#34;) #pragma cling load(\u0026#34;/usr/local/lib/libopencv_xobjdetect.so.4.1.0\u0026#34;) #pragma cling load(\u0026#34;/usr/local/lib/libopencv_xphoto.so.4.1.0\u0026#34;) Once done, run the cell and that\u0026rsquo;s it. We have successfully setup the environment for Libtorch and OpenCV.\nTesting Xeus Cling Notebook Let\u0026rsquo;s go ahead and include the libraries. I\u0026rsquo;ll be sharing the code snippets as well as the screenshots to make it easy for the readers to reproduce results.\n#include \u0026lt;torch/torch.h\u0026gt; #include \u0026lt;torch/script.h\u0026gt; #include \u0026lt;iostream\u0026gt; #include \u0026lt;dirent.h\u0026gt; #include \u0026lt;opencv2/opencv.hpp\u0026gt; After successfully importing libraries, we can define functions, write code and use the utilities Jupyter provides. Let\u0026rsquo;s start with playing with Tensors and the code snippets mentioned in the official PyTorch C++ Frontend Docs.\nStarting with using ATen tensor library. We\u0026rsquo;ll create two tensors and add them together. ATen comes up with functionalities of mathematical operations on the Tensors.\n#include \u0026lt;ATen/ATen.h\u0026gt; at::Tensor a = at::ones({2, 2}, at::kInt); at::Tensor b = at::randn({2, 2}); auto c = a + b.to(at::kInt); std::cout \u0026lt;\u0026lt; \u0026#34;a: \u0026#34; \u0026lt;\u0026lt; a \u0026lt;\u0026lt; std::endl; std::cout \u0026lt;\u0026lt; std::endl; std::cout \u0026lt;\u0026lt; \u0026#34;b: \u0026#34; \u0026lt;\u0026lt; b \u0026lt;\u0026lt; std::endl; std::cout \u0026lt;\u0026lt; std::endl; std::cout \u0026lt;\u0026lt; \u0026#34;c: \u0026#34; \u0026lt;\u0026lt; c \u0026lt;\u0026lt; std::endl; One of the reasons why Xeus-Cling is useful is, that you can print the outputs of intermediate steps and debug. Let\u0026rsquo;s go ahead and experiment with Autograd system of PyTorch C++ API.\nFor those who don\u0026rsquo;t know, automatic differentiation is the most important function of Deep Learning algorithms to backpropagte the loss we calculate.\n#include \u0026lt;torch/csrc/autograd/variable.h\u0026gt; #include \u0026lt;torch/csrc/autograd/function.h\u0026gt; torch::Tensor a_tensor = torch::ones({2, 2}, torch::requires_grad()); torch::Tensor b_tensor = torch::randn({2, 2}); std::cout \u0026lt;\u0026lt; a_tensor \u0026lt;\u0026lt; std::endl; std::cout \u0026lt;\u0026lt; b_tensor \u0026lt;\u0026lt; std::endl; auto c_tensor = a_tensor + b_tensor; c_tensor.backward(); // a.grad() will now hold the gradient of c w.r.t a std::cout \u0026lt;\u0026lt; c_tensor \u0026lt;\u0026lt; std::endl; How about debugging? As you can see in the figure below, I get an error stating no member named 'size' in namespace 'cv'. This is because namespace cv has member called Size and not size.\ntorch::Tensor read_images(std::string location) { cv::Mat img = cv::imread(location, 1); cv::resize(img, img, cv::size(224, 224), cv::INTER_CUBIC); torch::Tensor img_tensor = torch::from_blob(img.data, {img.rows, img.cols, 3}, torch::kByte); img_tensor = img_tensor.permute({2, 0, 1}); return img_tensor.clone(); } To solve, we can simply change the member from size to Size. One important point to consider is, that since this works on the top of Jupyter Interface, so whenever you re-run a cell, the variable names need to be changed as it will return an error of re-defining the variables which have already been defined.\nFor testing, I have implemented Transfer Learning example that we discussed in the previous blog. This comes handy as I don\u0026rsquo;t need to load the dataset again and again.\nBonus! With this blog, I\u0026rsquo;m also happy to share a Notebook file with implementation of Transfer Learning using ResNet18 Model on Dogs vs Cats Dataset. Additionally, I\u0026rsquo;m elated to open source the code for Transfer Learning using ResNet18 Model using PyTorch C++ API.\nThe source code and the notebook file can be found here.\nDebugging - OSX Systems In case of OSX Systems, if you see any errors similar to: You are probably missing the definition of \u0026lt;function_name\u0026gt;, then try any (or all) of the following points:\nUse Xeus-Cling on a virtual environment as this might be because of conflicts with the existing libraries. Although, OSX Systems shouldn\u0026rsquo;t have C++ ABI Compatability Issues but you can still try this if problem persists. Go to TorchCONFIG.cmake file (it should be present in \u0026lt;torch_folder\u0026gt;/share/cmake/Torch/). Change set(TORCH_CXX_FLAGS \u0026quot;-D_GLIBCXX_USE_CXX11_ABI=\u0026quot;) to set(TORCH_CXX_FLAGS \u0026quot;-D_GLIBCXX_USE_CXX11_ABI=1\u0026quot;) and reload the libraries and header files. References Xeus-Cling: Run C++ code in Jupyter Notebook by Vishwesh Ravi Shrimali. Documentation of Xeus-Cling. ","permalink":"https://krshrimali.github.io/posts/2019/08/setting-up-jupyter-notebook-xeus-cling-for-libtorch-and-opencv-libraries/","summary":"\u003ch2 id=\"introduction-to-xeus-cling\"\u003eIntroduction to Xeus Cling\u003c/h2\u003e\n\u003cp\u003eToday, we are going to run our C++ codes in the Jupyter Notebook. Sounds ambitious? Not much. Let\u0026rsquo;s see how we do it using Xeus Cling.\u003c/p\u003e\n\u003cp\u003e\u003cimg loading=\"lazy\" src=\"https://raw.githubusercontent.com/krshrimali/blog/main/assets/cover-images/Cover-Xeus-Cling.jpg\"\u003e\u003c/p\u003e\n\u003cp\u003eI\u0026rsquo;ll quote the definition of Xeus Cling on the official \u003ca href=\"https://xeus-cling.readthedocs.io/en/latest/#targetText=xeus%2Dcling%20is%20a%20Jupyter,of%20the%20Jupyter%20protocol%20xeus\"\u003edocumentation website\u003c/a\u003e.\u003c/p\u003e\n\u003cblockquote\u003e\n\u003cp\u003exeus-cling is a Jupyter kernel for C++ based on the C++ interpreter cling and the native implementation of the Jupyter protocol xeus.\u003c/p\u003e\u003c/blockquote\u003e\n\u003cp\u003eJust like we use Python Kernel in the Jupyter Notebook, we can also use a C++ based interpreter cling combined with a Jupyter protocol called Xeus to reach closer to implementing C++ code in the notebook.\u003c/p\u003e","title":"Setting up Jupyter Notebook (Xeus Cling) for Libtorch and OpenCV Libraries"},{"content":"Transfer Learning \u0026ndash; Before we go ahead and discuss the Why question of Transfer Learning, let\u0026rsquo;s have a look at What is Transfer Learning? Let\u0026rsquo;s have a look at the Notes from CS231n on Transfer Learning:\nIn practice, very few people train an entire Convolutional Network from scratch (with random initialization), because it is relatively rare to have a dataset of sufficient size. Instead, it is common to pretrain a ConvNet on a very large dataset (e.g. ImageNet, which contains 1.2 million images with 1000 categories), and then use the ConvNet either as an initialization or a fixed feature extractor for the task of interest.\nThere are 3 scenarios possible:\nWhen the data you have is similar (but not enough) to data trained on pre-trained model: Take an example of a pre-trained model trained on ImageNet dataset (containing 1000 classes). And the data we have has Dogs and Cats classes. Fortunate enough, ImageNet has some of the classes of Dog and Cat breeds and thus the model must have learned important features from the data. Let\u0026rsquo;s say we don\u0026rsquo;t have enough data but since the data is similar to the breeds in the ImageNet data set, we can simply use the ConvNet (except the last FC layer) to extract features from our dataset and train only the last Linear (FC) layer. When you have enough data (and is similar to the data trained with pre-trained model): Then you might go for fine tuning the weights of all the layers in the network. This is largely due to the reason that we know we won\u0026rsquo;t overfit because we have enough data. Using pre-trained models might just be enough if you have the data which matches the classes in the original data set. Transfer Learning came into existence (the answer of Why Transfer Learning?) because of some major reasons, which include:\nLack of resources or data set to train a CNN. At times, we either don\u0026rsquo;t have enough data or we don\u0026rsquo;t have enough resources to train a CNN from scratch. Random Initialization of weights vs Initialization of weights from the pre-trained model. Sometimes, it\u0026rsquo;s just better to initialize weights from the pre-trained model (as it must have learned the generic features from it\u0026rsquo;s data set) instead of randomly initializing the weights. Setting up the data with PyTorch C++ API At every stage, we will compare the Python and C++ codes to do the same thing, to make the analogy easier and understandable. Starting with setting up the data we have. Note that we do have enough data and it is also similar to the original data set of ImageNet, but since I don\u0026rsquo;t have enough resources to fine tune through the whole network, we perform Transfer Learning on the final FC layer only.\nStarting with loading the dataset, as discussed in the blogs before, I\u0026rsquo;ll just post a flow chart of procedure.\nOnce done, we can initialize the CustomDataset class:\nC++\n// List of images of Dogs and Cats, use load_data_from_folder function explained in previous blogs std::vector\u0026lt;std::string\u0026gt; list_images; // List of labels of the images std::vector\u0026lt;int\u0026gt; list_labels; auto custom_dataset = CustomDataset(list_images, list_labels).map(torch::data::transforms::Stack\u0026lt;\u0026gt;()); Python\nfrom torchvision import datasets, transforms import torch folder_path = \u0026#34;/Users/krshrimali/Documents/dataset/train/\u0026#34; transform = transforms.Compose([transforms.CenterCrop(224), transforms.ToTensor()) data = datasets.ImageFolder(root = os.path.join(folder_path), transform = transform) We then use RandomSampler to make our data loader: (Note: it\u0026rsquo;s important to use RandomSampler as we load the images sequentially and we want mixture of images in each batch of data passed to the network in an epoch)\nC++\nint batch_size = 4; auto data_loader = torch::data::make_data_loader\u0026lt;torch::data::samplers::RandomSampler\u0026gt;(std::move(custom_dataset), batch_size); Python\nbatch_size = 4 data_loader = torch.utils.data.DataLoader(dataset=data, batch_size = batch_size, shuffle = True) Loading the pre-trained model The steps to load the pre-trained model and perform Transfer Learning are listed below:\nDownload the pre-trained model of ResNet18. Load pre-trained model. Change output features of the final FC layer of the model loaded. (Number of classes would change from 1000 - ImageNet to 2 - Dogs vs Cats). Define optimizer on parameters from the final FC layer to be trained. Train the FC layer on Dogs vs Cats dataset. Save the model. Let\u0026rsquo;s go step by step.\nStep-1: Download the pre-trained model of ResNet18 Thanks to the developers, we do have C++ models available in torchvision (https://github.com/pytorch/vision/pull/728) but for this tutorial, transferring the pre- trained model from Python to C++ using torch.jit is a good idea, as most PyTorch models in the wild are written in Python right now, and people can use this tutorial to learn how to trace their Python model and transfer it to C++.\nFirst we download the pre-trained model and save it in the form of torch.jit.trace format to our local drive.\n# Reference: #TODO- Add Link from torchvision import models # Download and load the pre-trained model model = models.resnet18(pretrained=True) # Set upgrading the gradients to False for param in model.parameters(): param.requires_grad = False # Save the model except the final FC Layer resnet18 = torch.nn.Sequential(*list(resnet18.children())[:-1]) example_input = torch.rand(1, 3, 224, 224) script_module = torch.jit.trace(resnet18, example_input) script_module.save(\u0026#39;resnet18_without_last_layer.pt\u0026#39;) We will be using resnet18_without_last_layer.pt model file as our pre-trained model for transfer learning.\nStep-2: Load the pre-trained model Let\u0026rsquo;s go ahead and load the pre-trained model using torch::jit module. Note that the reason we have converted torch.nn.Module to torch.jit.ScriptModule type, is because C++ API currently does not support loading Python torch.nn.Module models directly.\nC++:\ntorch::jit::script::Module module; // argv[1] should be the path to the model module = torch::jit::load(argv[1]); /* We need to convert last layer input and output features from (512, 1000) to (512, 2) since we only have 2 classes */ torch::nn::Linear linear_layer(512, 2); // Define the optimizer on parameters of linear_layer with learning_rate = 1e-3 torch::optim::Adam optimizer(linear_layer-\u0026gt;parameters(), torch::optim::AdamOptions(1e-3)) Python:\n# We will directly load the torch.nn pre-trained model model = models.resnet18(pretrained = True) for param in model.parameters(): param.requires_grad = False model.fc = torch.nn.Linear(512, 2) for param in model.fc.parameters(): param.requires_grad = True optimizer = torch.optim.Adam(model.fc.parameters()) cost = torch.nn.CrossEntropyLoss() Trainining the FC Layer Let\u0026rsquo;s first have a look at ResNet18 Network Architecture\nThe final step is to train the Fully Connected layer that we inserted at the end of the network (linear_layer). This one should be pretty straight forward, let\u0026rsquo;s see how to do it.\nC++:\nvoid train(torch::jit::script::Module net, torch::nn::Linear lin, Dataloader\u0026amp; data_loader, torch::optim::Optimizer\u0026amp; optimizer, size_t dataset_size) { /* This function trains the network on our data loader using optimizer for given number of epochs. Parameters ================== torch::jit::script::Module net: Pre-trained model torch::nn::Linear lin: Linear layer DataLoader\u0026amp; data_loader: Training data loader torch::optim::Optimizer\u0026amp; optimizer: Optimizer like Adam, SGD etc. size_t dataset_size: Size of training dataset */ float batch_index = 0; for(int i=0; i\u0026lt;15; i++) { float mse = 0; float Acc = 0.0; for(auto\u0026amp; batch: *data_loader) { auto data = batch.data; auto target = batch.target.squeeze(); // Should be of length: batch_size data = data.to(torch::kF32); target = target.to(torch::kInt64); std::vector\u0026lt;torch::jit::IValue\u0026gt; input; input.push_back(data); optimizer.zero_grad(); auto output = net.forward(input).toTensor(); // For transfer learning output = output.view({output.size(0), -1}); output = lin(output); // Explicitly calculate torch::log_softmax of output from the FC Layer auto loss = torch::nll_loss(torch::log_softmax(output, 1), target); loss.backward(); optimizer.step(); auto acc = output.argmax(1).eq(target).sum(); Acc += acc.template item\u0026lt;float\u0026gt;(); mse += loss.template item\u0026lt;float\u0026gt;(); batch_index += 1; } mse = mse/float(batch_index); // Take mean of loss std::cout \u0026lt;\u0026lt; \u0026#34;Epoch: \u0026#34; \u0026lt;\u0026lt; i \u0026lt;\u0026lt; \u0026#34;, \u0026#34; \u0026lt;\u0026lt; \u0026#34;Accuracy: \u0026#34; \u0026lt;\u0026lt; Acc/dataset_size \u0026lt;\u0026lt; \u0026#34;, \u0026#34; \u0026lt;\u0026lt; \u0026#34;MSE: \u0026#34; \u0026lt;\u0026lt; mse \u0026lt;\u0026lt; std::endl; net.save(\u0026#34;model.pt\u0026#34;); } } Python:\nn_epochs = 15 for epoch in range(n_epochs): mse = 0.0 acc = 0 batch_index = 0 for data_batch in data_loader: batch_index += 1 image, label = data_batch optimizer.zero_grad() output = model(image) _, predicted_label = torch.max(output.data, 1) loss = cost(output, label) loss.backward() optimizer.step() mse += loss.item() # data[0] acc += torch.sum(predicted_label == label.data) mse = mse/len(data) acc = 100*acc/len(data) print(\u0026#34;Epoch: {}/{}, Loss: {:.4f}, Accuracy: {:.4f}\u0026#34;.format(epoch+1, n_epochs, mse, acc)) The code to test should also not change much except the need of optimizer.\nResults On a set of 400 images for training data, the maximum training Accuracy I could achieve was 91.25% in just less than 15 epochs using PyTorch C++ API and 89.0% using Python. (Note that this doesn\u0026rsquo;t conclude superiority in terms of accuracy between any of the two backends - C++ or Python)\nLet\u0026rsquo;s have a look at correct and wrong predictions.\nCorrect Predictions - Dogs Wrong Predictions - Dogs Correct Predictions - Cats Wrong Predictions - Cats Acknowledgements I would like to thank a few people to help me bring this out to the community. Thanks to Piotr for his comments and answers in the PyTorch Discussion forum. Thanks to Will Feng for reviewing the blog and the code and also his constant motivation to bring this out to you all. Would like to thank my constant motivation behind all my work, Vishwesh Ravi Shrimali for all his help to start with PyTorch C++ API and help the community. Special thanks to Krutika Bapat as well, for reviewing the Python equivalent code and suggesting modifications.\nAnd shout out to all the readers, please share your feedback with me in the comments below. I would love to hear if this blog helped you!\nIn the upcoming blog, I\u0026rsquo;ll be sharing something very exciting. Till then, happy learning!\n","permalink":"https://krshrimali.github.io/posts/2019/08/applying-transfer-learning-on-dogs-vs-cats-dataset-resnet18-using-pytorch-c-api/","summary":"\u003ch2 id=\"transfer-learning\"\u003eTransfer Learning\u003c/h2\u003e\n\u003cp\u003e\u0026ndash;\nBefore we go ahead and discuss the \u003cstrong\u003eWhy\u003c/strong\u003e question of Transfer Learning, let\u0026rsquo;s have a look at \u003cstrong\u003eWhat is Transfer Learning?\u003c/strong\u003e Let\u0026rsquo;s have a look at the \u003ca href=\"http://cs231n.github.io/transfer-learning\"\u003eNotes\u003c/a\u003e from CS231n on Transfer Learning:\u003c/p\u003e\n\u003cblockquote\u003e\n\u003cp\u003eIn practice, very few people train an entire Convolutional Network from scratch (with random initialization), because it is relatively rare to have a dataset of sufficient size. Instead, it is common to pretrain a ConvNet on a very large dataset (e.g. ImageNet, which contains 1.2 million images with 1000 categories), and then use the ConvNet either as an initialization or a fixed feature extractor for the task of interest.\u003c/p\u003e","title":"Applying Transfer Learning on Dogs vs Cats Dataset (ResNet18) using PyTorch C++ API"},{"content":"In the last blog, we had discussed all but training and results of our custom CNN network on Dogs vs Cats dataset. Today, we\u0026rsquo;ll be making some small changes in the network and discussing training and results of the task.\nI\u0026rsquo;ll start with the network overview again, where we used a network similar to VGG-16 (with one extra Fully Connected Layer in the end). While there are absolutely no problems with that network, but since the dataset contains a lot of images (25000 in training dataset) and we were using (200x200x3) input shape to the network (which is 120,000 floating point numbers), this leads to high memory consumption. In short, I was out of RAM to store these many images during program execution.\nSo, I decided to change some minute things:\nInput Shape to the network is now 64x64x3 (12,288 parameters - around 10 times lesser than for 200x200x3). So, all the images are now resized to 64x64x3. Now, we only use 2 Convolutional Layers and 2 Max Pooling Layers to train our dataset. This helps to reduce the parameters for training, and also fastens the training process. But this comes with a tradeoff in accuracy, which will suffice for now as our target is not to get some X accuracy, but to learn how to train the network on our dataset using PyTorch C++ API.\nQuestion: Does reducing input resolution, affects accuracy? Answer: In this case, it will. The objects in our dataset (dogs and cats) have both high level and low level features, which are visible (provides more details) more to the network with high resolution. In this way, the network is allowed to learn more features out of the dataset. However, in cases like of MNIST, it\u0026rsquo;s just fine to use 64x64 input resolution as it still allows the network to look at details of a digit and learn robust features.\nLet\u0026rsquo;s go ahead and see what has changed in the Network.\nNetwork Overview If you don\u0026rsquo;t remember from the last time, this is how our network looked.\nstruct NetImpl: public torch::nn::Module { NetImpl() { // Initialize the network // On how to pass strides and padding: https://github.com/pytorch/pytorch/issues/12649#issuecomment-430156160 conv1_1 = register_module(\u0026#34;conv1_1\u0026#34;, torch::nn::Conv2d(torch::nn::Conv2dOptions(1, 10, 3).padding(1))); conv1_2 = register_module(\u0026#34;conv1_2\u0026#34;, torch::nn::Conv2d(torch::nn::Conv2dOptions(10, 20, 3).padding(1))); // Insert pool layer conv2_1 = register_module(\u0026#34;conv2_1\u0026#34;, torch::nn::Conv2d(torch::nn::Conv2dOptions(20, 30, 3).padding(1))); conv2_2 = register_module(\u0026#34;conv2_2\u0026#34;, torch::nn::Conv2d(torch::nn::Conv2dOptions(30, 40, 3).padding(1))); // Insert pool layer conv3_1 = register_module(\u0026#34;conv3_1\u0026#34;, torch::nn::Conv2d(torch::nn::Conv2dOptions(40, 50, 3).padding(1))); conv3_2 = register_module(\u0026#34;conv3_2\u0026#34;, torch::nn::Conv2d(torch::nn::Conv2dOptions(50, 60, 3).padding(1))); conv3_3 = register_module(\u0026#34;conv3_3\u0026#34;, torch::nn::Conv2d(torch::nn::Conv2dOptions(60, 70, 3).padding(1))); // Insert pool layer conv4_1 = register_module(\u0026#34;conv4_1\u0026#34;, torch::nn::Conv2d(torch::nn::Conv2dOptions(70, 80, 3).padding(1))); conv4_2 = register_module(\u0026#34;conv4_2\u0026#34;, torch::nn::Conv2d(torch::nn::Conv2dOptions(80, 90, 3).padding(1))); conv4_3 = register_module(\u0026#34;conv4_3\u0026#34;, torch::nn::Conv2d(torch::nn::Conv2dOptions(90, 100, 3).padding(1))); // Insert pool layer conv5_1 = register_module(\u0026#34;conv5_1\u0026#34;, torch::nn::Conv2d(torch::nn::Conv2dOptions(100, 110, 3).padding(1))); conv5_2 = register_module(\u0026#34;conv5_2\u0026#34;, torch::nn::Conv2d(torch::nn::Conv2dOptions(110, 120, 3).padding(1))); conv5_3 = register_module(\u0026#34;conv5_3\u0026#34;, torch::nn::Conv2d(torch::nn::Conv2dOptions(120, 130, 3).padding(1))); // Insert pool layer fc1 = register_module(\u0026#34;fc1\u0026#34;, torch::nn::Linear(130*6*6, 2000)); fc2 = register_module(\u0026#34;fc2\u0026#34;, torch::nn::Linear(2000, 1000)); fc3 = register_module(\u0026#34;fc3\u0026#34;, torch::nn::Linear(1000, 100)); fc4 = register_module(\u0026#34;fc4\u0026#34;, torch::nn::Linear(100, 2)); } // Implement Algorithm torch::Tensor forward(torch::Tensor x) { x = torch::relu(conv1_1-\u0026gt;forward(x)); x = torch::relu(conv1_2-\u0026gt;forward(x)); x = torch::max_pool2d(x, 2); x = torch::relu(conv2_1-\u0026gt;forward(x)); x = torch::relu(conv2_2-\u0026gt;forward(x)); x = torch::max_pool2d(x, 2); x = torch::relu(conv3_1-\u0026gt;forward(x)); x = torch::relu(conv3_2-\u0026gt;forward(x)); x = torch::relu(conv3_3-\u0026gt;forward(x)); x = torch::max_pool2d(x, 2); x = torch::relu(conv4_1-\u0026gt;forward(x)); x = torch::relu(conv4_2-\u0026gt;forward(x)); x = torch::relu(conv4_3-\u0026gt;forward(x)); x = torch::max_pool2d(x, 2); x = torch::relu(conv5_1-\u0026gt;forward(x)); x = torch::relu(conv5_2-\u0026gt;forward(x)); x = torch::relu(conv5_3-\u0026gt;forward(x)); x = torch::max_pool2d(x, 2); x = x.view({-1, 130*6*6}); x = torch::relu(fc1-\u0026gt;forward(x)); x = torch::relu(fc2-\u0026gt;forward(x)); x = torch::relu(fc3-\u0026gt;forward(x)); x = fc4-\u0026gt;forward(x); return torch::log_softmax(x, 1); } // Declare layers torch::nn::Conv2d conv1_1{nullptr}; torch::nn::Conv2d conv1_2{nullptr}; torch::nn::Conv2d conv2_1{nullptr}; torch::nn::Conv2d conv2_2{nullptr}; torch::nn::Conv2d conv3_1{nullptr}; torch::nn::Conv2d conv3_2{nullptr}; torch::nn::Conv2d conv3_3{nullptr}; torch::nn::Conv2d conv4_1{nullptr}; torch::nn::Conv2d conv4_2{nullptr}; torch::nn::Conv2d conv4_3{nullptr}; torch::nn::Conv2d conv5_1{nullptr}; torch::nn::Conv2d conv5_2{nullptr}; torch::nn::Conv2d conv5_3{nullptr}; torch::nn::Linear fc1{nullptr}, fc2{nullptr}, fc3{nullptr}, fc4{nullptr}; }; As it\u0026rsquo;s visible, we had 13 Convolutional Layers, 5 Max Pooling Layers and 4 Fully Connected Layers. This leads of a lot of parameters to be trained.\nTherefore, our new network for experimentation purposes will be:\nstruct NetworkImpl : public torch::nn::Module { NetImpl(int64_t channels, int64_t height, int64_t width) { conv1_1 = register_module(\u0026#34;conv1\u0026#34;, torch::nn::Conv2d(torch::nn::Conv2dOptions(3, 50, 5).stride(2))); conv2_1 = register_module(\u0026#34;conv2\u0026#34;, torch::nn::Conv2d(torch::nn::Conv2dOptions(50, 100, 7).stride(2))); // Used to find the output size till previous convolutional layers n(get_output_shape(channels, height, width)); fc1 = register_module(\u0026#34;fc1\u0026#34;, torch::nn::Linear(n, 120)); fc2 = register_module(\u0026#34;fc2\u0026#34;, torch::nn::Linear(120, 100)); fc3 = register_module(\u0026#34;fc3\u0026#34;, torch::nn::Linear(100, 2)); register_module(\u0026#34;conv1\u0026#34;, conv1); register_module(\u0026#34;conv2\u0026#34;, conv2); register_module(\u0026#34;fc1\u0026#34;, fc1); register_module(\u0026#34;fc2\u0026#34;, fc2); register_module(\u0026#34;fc3\u0026#34;, fc3); } // Implement forward pass of each batch to the network torch::Tensor forward(torch::Tensor x) { x = torch::relu(torch::max_pool2d(conv1(x), 2)); x = torch::relu(torch::max_pool2d(conv2(x), 2)); // Flatten x = x.view({-1, n}); x = torch::relu(fc1(x)); x = torch::relu(fc2(x)); x = torch::log_softmax(fc3(x), 1); return x; }; // Function to calculate output size of input tensor after Convolutional layers int64_t get_output_shape(int64_t channels, int64_t height, int64_t width) { // Initialize a Tensor with zeros of input shape torch::Tensor x_sample = torch::zeros({1, channels, height, width}); x_sample = torch::max_pool2d(conv1(x_sample), 2); x_sample = torch::max_pool2d(conv2(x_sample), 2); // Return batch_size (here, 1) * channels * height * width of x_sample return x_sample.numel(); } }; In our new network, we use 2 Convolutional Layers with Max Pooling and ReLU Activation, and 3 Fully Connected Layers. This, as we mentioned above, reduces the number of parameters for training.\nLet us train our network on the dataset now.\nTraining Let\u0026rsquo;s discuss steps of training a CNN on our dataset:\nSet network to training mode using net-\u0026gt;train(). Iterate through every batch of our data loader: Extract data and labels using: auto data = batch.data; auto target = batch.target.squeeze(); Clear gradients of optimizer: optimizer.zero_grad() Forward pass the batch of data to the network: auto output = net-\u0026gt;forward(data); Calculate Negative Log Likelihood loss (since we use log_softmax() layer at the end): auto loss = torch::nll_loss(output, target); Backpropagate Loss: auto loss = loss.backward() Update the weights: optimizer.step(); Calculate Training Accuracy and Mean Squared Error: auto acc = output.argmax(1).eq(target).sum(); mse += loss.template item\u0026lt;float\u0026gt;(); Save the model using torch::save(net, \u0026quot;model.pt\u0026quot;);. Let\u0026rsquo;s try to convert the above steps to a train() function.\nvoid train(ConvNet\u0026amp; net, DataLoader\u0026amp; data_loader, torch::optim::Optimizer\u0026amp; optimizer, size_t dataset_size, int epoch) { /* This function trains the network on our data loader using optimizer for given number of epochs. Parameters ================== ConvNet\u0026amp; net: Network struct DataLoader\u0026amp; data_loader: Training data loader torch::optim::Optimizer\u0026amp; optimizer: Optimizer like Adam, SGD etc. size_t dataset_size: Size of training dataset int epoch: Number of epoch for training */ net-\u0026gt;train(); size_t batch_index = 0; float mse = 0; float Acc = 0.0; for(auto\u0026amp; batch: *data_loader) { auto data = batch.data; auto target = batch.target.squeeze(); // Should be of length: batch_size data = data.to(torch::kF32); target = target.to(torch::kInt64); optimizer.zero_grad(); auto output = net-\u0026gt;forward(data); auto loss = torch::nll_loss(output, target); loss.backward(); optimizer.step(); auto acc = output.argmax(1).eq(target).sum(); Acc += acc.template item\u0026lt;float\u0026gt;(); mse += loss.template item\u0026lt;float\u0026gt;(); batch_index += 1; count++; } mse = mse/float(batch_index); // Take mean of loss std::cout \u0026lt;\u0026lt; \u0026#34;Epoch: \u0026#34; \u0026lt;\u0026lt; epoch \u0026lt;\u0026lt; \u0026#34;, \u0026#34; \u0026lt;\u0026lt; \u0026#34;Accuracy: \u0026#34; \u0026lt;\u0026lt; Acc/dataset_size \u0026lt;\u0026lt; \u0026#34;, \u0026#34; \u0026lt;\u0026lt; \u0026#34;MSE: \u0026#34; \u0026lt;\u0026lt; mse \u0026lt;\u0026lt; std::endl; torch::save(net, \u0026#34;best_model_try.pt\u0026#34;); } Similarly, we also define a Test Function to test our network on the test dataset. The steps include:\nSet network to eval mode: network-\u0026gt;eval(). Iterate through every batch of test data. Extract data and labels. Forward pass the batch of data to the network. Calculate NLL Loss and Accuracy Print test loss and accuracy. The code for the test() function is below:\nvoid test(ConvNet\u0026amp; network, DataLoader\u0026amp; loader, size_t data_size) { size_t batch_index = 0; network-\u0026gt;eval(); float Loss = 0, Acc = 0; for (const auto\u0026amp; batch : *loader) { auto data = batch.data; auto targets = batch.target.view({-1}); data = data.to(torch::kF32); targets = targets.to(torch::kInt64); auto output = network-\u0026gt;forward(data); auto loss = torch::nll_loss(output, targets); auto acc = output.argmax(1).eq(targets).sum(); Loss += loss.template item\u0026lt;float\u0026gt;(); Acc += acc.template item\u0026lt;float\u0026gt;(); } cout \u0026lt;\u0026lt; \u0026#34;Test Loss: \u0026#34; \u0026lt;\u0026lt; Loss/data_size \u0026lt;\u0026lt; \u0026#34;, Acc:\u0026#34; \u0026lt;\u0026lt; Acc/data_size \u0026lt;\u0026lt; endl; } Results I trained my network on the dataset for 100 Epochs.\nThe best training and testing accuracies are given below:\nBest Training Accuracy: 99.82% Best Testing Accuracy: 82.43% Let\u0026rsquo;s look at some of the correct and wrong predictions.\nCorrect Predictions (Dogs) Correct Predictions (Cats) Wrong Predictions (Dogs) Wrong Predictions (Cats) Clearly, the network has done well for just a 2 Convolutional and 3 FC Layer Network. It seems to have focused more on the posture of the animal (and body). We can make the network learn more robust features, with a more deeper CNN (like VGG-16). We\u0026rsquo;ll be discussing on using pretrained weights on Dogs vs Cats Dataset using PyTorch C++ API and also Transfer Learning Approach in C++.\nHappy Learning!\n","permalink":"https://krshrimali.github.io/posts/2019/07/classifying-dogs-vs-cats-using-pytorch-c-part-2/","summary":"\u003cp\u003eIn the last blog, we had discussed all but training and results of our custom CNN network on Dogs vs Cats dataset. Today, we\u0026rsquo;ll be making some small changes in the network and discussing training and results of the task.\u003c/p\u003e\n\u003cp\u003e\u003cimg loading=\"lazy\" src=\"https://raw.githubusercontent.com/krshrimali/blog/main/assets/cover-images/Classify-Dogs-Cats-Blog-05.jpg\"\u003e\u003c/p\u003e\n\u003cp\u003eI\u0026rsquo;ll start with the network overview again, where we used a network similar to VGG-16 (with one extra Fully Connected Layer in the end). While there are absolutely no problems with that network, but since the dataset contains a lot of images (25000 in training dataset) and we were using (200x200x3) input shape to the network (which is 120,000 floating point numbers), this leads to high memory consumption. In short, I was out of RAM to store these many images during program execution.\u003c/p\u003e","title":"Classifying Dogs vs Cats using PyTorch C++: Part 2"},{"content":"Hi Everyone! So excited to be back with another blog in the series of PyTorch C++ Blogs.\nToday, we are going to see a practical example of applying a CNN to a Custom Dataset - Dogs vs Cats. This is going to be a short post of showing results and discussion about hyperparameters and loss functions for the task, as code snippets and explanation has been provided here, here and here.\nNote: This is Part-1 of the blog on Dogs vs Cats Classification using PyTorch C++ API.\nDataset Overview Let\u0026rsquo;s have a look at the dataset and it\u0026rsquo;s statistics. Dogs vs Cats dataset has been taken from the famous Kaggle Competition.\nThe training set contains 25k images combined of dogs and cats. The data can be downloaded from this link.\nLet\u0026rsquo;s have a look at sample of the data:\nAs we can see, the dataset contains images of cats and dogs with multiple instances in the same sample as well.\nLoading Data Although we have discussed this before (here), but let\u0026rsquo;s just see how we load the data. Since this is a binary classification problem (2 classes: Dog and Cat), we will have labels as 0 (for a Cat) and 1 (for a Dog). The data comes in two zip files:\ntrain.zip: Data to be used for training test.zip: Data to be used for testing The train.zip file contains files with filenames like \u0026lt;class\u0026gt;.\u0026lt;number\u0026gt;.jpg where:\nclass can be either cat or dog. number represents the count of the sample. For example: cat.100.jpg and dog.1.jpg. In order to load the data, we will move the cat images to train/cat folder and dog images to train/dog folder. You can accomplish this using shutil module:\n# This code moves cat and dog images to train/cat and train/dog folders respectively import shutil, os files = os.listdir(\u0026#39;train/\u0026#39;) count_cat = 0 # Number representing count of the cat image count_dog = 0 # Number representing count of the dog image for file in files: if(file.startswith(\u0026#39;cat\u0026#39;) and file.endswith(\u0026#39;jpg\u0026#39;)): count_cat += 1 shutil.copy(\u0026#39;train/\u0026#39; + file, \u0026#39;train/cat/\u0026#39; + str(count_cat) + \u0026#34;.jpg\u0026#34;) elif(file.startswith(\u0026#39;dog\u0026#39;) and file.endswith(\u0026#39;jpg\u0026#39;)): count_dog += 1 shutil.copy(\u0026#39;test/\u0026#39; + file, \u0026#39;train/dog/\u0026#39; + str(count_dog) + \u0026#39;.jpg\u0026#39;) Once done, let\u0026rsquo;s go ahead and load this data. Since we have discussed this before, I\u0026rsquo;ll just paste the snippet here.\ntorch::Tensor read_data(std::string loc) { // Read Image from the location of image cv::Mat img = cv::imread(loc, 0); cv::resize(img, img, cv::Size(200, 200), cv::INTER_CUBIC); std::cout \u0026lt;\u0026lt; \u0026#34;Sizes: \u0026#34; \u0026lt;\u0026lt; img.size() \u0026lt;\u0026lt; std::endl; torch::Tensor img_tensor = torch::from_blob(img.data, {img.rows, img.cols, 1}, torch::kByte); img_tensor = img_tensor.permute({2, 0, 1}); // Channels x Height x Width return img_tensor.clone(); }; torch::Tensor read_label(int label) { torch::Tensor label_tensor = torch::full({1}, label); return label_tensor.clone(); } vector\u0026lt;torch::Tensor\u0026gt; process_images(vector\u0026lt;string\u0026gt; list_images) { cout \u0026lt;\u0026lt; \u0026#34;Reading images...\u0026#34; \u0026lt;\u0026lt; endl; vector\u0026lt;torch::Tensor\u0026gt; states; for (std::vector\u0026lt;string\u0026gt;::iterator it = list_images.begin(); it != list_images.end(); ++it) { cout \u0026lt;\u0026lt; \u0026#34;Location being read: \u0026#34; \u0026lt;\u0026lt; *it \u0026lt;\u0026lt; endl; torch::Tensor img = read_data(*it); states.push_back(img); } cout \u0026lt;\u0026lt; \u0026#34;Reading and Processing images done!\u0026#34; \u0026lt;\u0026lt; endl; return states; } vector\u0026lt;torch::Tensor\u0026gt; process_labels(vector\u0026lt;int\u0026gt; list_labels) { cout \u0026lt;\u0026lt; \u0026#34;Reading labels...\u0026#34; \u0026lt;\u0026lt; endl; vector\u0026lt;torch::Tensor\u0026gt; labels; for (std::vector\u0026lt;int\u0026gt;::iterator it = list_labels.begin(); it != list_labels.end(); ++it) { torch::Tensor label = read_label(*it); labels.push_back(label); } cout \u0026lt;\u0026lt; \u0026#34;Labels reading done!\u0026#34; \u0026lt;\u0026lt; endl; return labels; } /* This function returns a pair of vector of images paths (strings) and labels (integers) */ std::pair\u0026lt;vector\u0026lt;string\u0026gt;,vector\u0026lt;int\u0026gt;\u0026gt; load_data_from_folder(vector\u0026lt;string\u0026gt; folders_name) { vector\u0026lt;string\u0026gt; list_images; vector\u0026lt;int\u0026gt; list_labels; int label = 0; for(auto const\u0026amp; value: folders_name) { string base_name = value + \u0026#34;/\u0026#34;; cout \u0026lt;\u0026lt; \u0026#34;Reading from: \u0026#34; \u0026lt;\u0026lt; base_name \u0026lt;\u0026lt; endl; DIR* dir; struct dirent *ent; if((dir = opendir(base_name.c_str())) != NULL) { while((ent = readdir(dir)) != NULL) { string filename = ent-\u0026gt;d_name; if(filename.length() \u0026gt; 4 \u0026amp;\u0026amp; filename.substr(filename.length() - 3) == \u0026#34;jpg\u0026#34;) { cout \u0026lt;\u0026lt; base_name + ent-\u0026gt;d_name \u0026lt;\u0026lt; endl; // cv::Mat temp = cv::imread(base_name + \u0026#34;/\u0026#34; + ent-\u0026gt;d_name, 1); list_images.push_back(base_name + ent-\u0026gt;d_name); list_labels.push_back(label); } } closedir(dir); } else { cout \u0026lt;\u0026lt; \u0026#34;Could not open directory\u0026#34; \u0026lt;\u0026lt; endl; // return EXIT_FAILURE; } label += 1; } return std::make_pair(list_images, list_labels); } The above snippet has the utility functions we need. Here is a quick summary of what they do:\nload_data_from_folder: This function returns a tuple of list of image paths (string) and list of labels (int). It takes a vector of folders names (string type) as parameter.\nprocess_images: This function returns a vector of Tensors (images). This function calls read_data function which reads, resizes and converts an image to a Torch Tensor. It takes a vector of image paths (string) as parameter.\nprocess_labels: Similar to process_images function, this function returns a vector of Tensors (labels). This function calls read_label function which takes an int as a parameter (label: 0 or 1) and returns a Tensor.\nLet\u0026rsquo;s now go ahead and see how we load the data. For this, we first need to define the Dataset class. This class should initialize two variables: one for images and one for labels. As discussed here, we\u0026rsquo;ll also define get() and size() functions.\nclass CustomDataset : public torch::data::Dataset\u0026lt;CustomDataset\u0026gt; { private: /* data */ // Should be 2 tensors vector\u0026lt;torch::Tensor\u0026gt; states, labels; public: CustomDataset(vector\u0026lt;string\u0026gt; list_images, vector\u0026lt;int\u0026gt; list_labels) { states = process_images(list_images); labels = process_labels(list_labels); }; torch::data::Example\u0026lt;\u0026gt; get(size_t index) override { /* This should return {torch::Tensor, torch::Tensor} */ torch::Tensor sample_img = states.at(index); torch::Tensor sample_label = labels.at(index); return {sample_img.clone(), sample_label.clone()}; }; torch::optional\u0026lt;size_t\u0026gt; size() const override { return states.size(); }; }; Once done, we can go ahead and initialize the Dataset in our main() function.\nint main(int argc, char const *argv[]) { // Load the model. // Read Data vector\u0026lt;string\u0026gt; folders_name; folders_name.push_back(\u0026#34;/home/krshrimali/Documents/data-dogs-cats/train/cat\u0026#34;); folders_name.push_back(\u0026#34;/home/krshrimali/Documents/data-dogs-cats/train/dog\u0026#34;); std::pair\u0026lt;vector\u0026lt;string\u0026gt;, vector\u0026lt;int\u0026gt;\u0026gt; pair_images_labels = load_data_from_folder(folders_name); vector\u0026lt;string\u0026gt; list_images = pair_images_labels.first; vector\u0026lt;int\u0026gt; list_labels = pair_images_labels.second; auto custom_dataset = CustomDataset(list_images, list_labels).map(torch::data::transforms::Stack\u0026lt;\u0026gt;()); } Network Overview To make things easy to read (a programmer\u0026rsquo;s mantra), we define our network in a header file. We will use a CNN network initially for this binary classification task. Since I\u0026rsquo;m not using a GPU, the training is slow and that\u0026rsquo;s why I experimented it only for 10 epochs. The whole objective is to discuss and show how to use PyTorch C++ API for this. You can always run it for more epochs or change the network parameters.\nstruct NetImpl: public torch::nn::Module { NetImpl() { // Initialize the network // On how to pass strides and padding: https://github.com/pytorch/pytorch/issues/12649#issuecomment-430156160 conv1_1 = register_module(\u0026#34;conv1_1\u0026#34;, torch::nn::Conv2d(torch::nn::Conv2dOptions(1, 10, 3).padding(1))); conv1_2 = register_module(\u0026#34;conv1_2\u0026#34;, torch::nn::Conv2d(torch::nn::Conv2dOptions(10, 20, 3).padding(1))); // Insert pool layer conv2_1 = register_module(\u0026#34;conv2_1\u0026#34;, torch::nn::Conv2d(torch::nn::Conv2dOptions(20, 30, 3).padding(1))); conv2_2 = register_module(\u0026#34;conv2_2\u0026#34;, torch::nn::Conv2d(torch::nn::Conv2dOptions(30, 40, 3).padding(1))); // Insert pool layer conv3_1 = register_module(\u0026#34;conv3_1\u0026#34;, torch::nn::Conv2d(torch::nn::Conv2dOptions(40, 50, 3).padding(1))); conv3_2 = register_module(\u0026#34;conv3_2\u0026#34;, torch::nn::Conv2d(torch::nn::Conv2dOptions(50, 60, 3).padding(1))); conv3_3 = register_module(\u0026#34;conv3_3\u0026#34;, torch::nn::Conv2d(torch::nn::Conv2dOptions(60, 70, 3).padding(1))); // Insert pool layer conv4_1 = register_module(\u0026#34;conv4_1\u0026#34;, torch::nn::Conv2d(torch::nn::Conv2dOptions(70, 80, 3).padding(1))); conv4_2 = register_module(\u0026#34;conv4_2\u0026#34;, torch::nn::Conv2d(torch::nn::Conv2dOptions(80, 90, 3).padding(1))); conv4_3 = register_module(\u0026#34;conv4_3\u0026#34;, torch::nn::Conv2d(torch::nn::Conv2dOptions(90, 100, 3).padding(1))); // Insert pool layer conv5_1 = register_module(\u0026#34;conv5_1\u0026#34;, torch::nn::Conv2d(torch::nn::Conv2dOptions(100, 110, 3).padding(1))); conv5_2 = register_module(\u0026#34;conv5_2\u0026#34;, torch::nn::Conv2d(torch::nn::Conv2dOptions(110, 120, 3).padding(1))); conv5_3 = register_module(\u0026#34;conv5_3\u0026#34;, torch::nn::Conv2d(torch::nn::Conv2dOptions(120, 130, 3).padding(1))); // Insert pool layer fc1 = register_module(\u0026#34;fc1\u0026#34;, torch::nn::Linear(130*6*6, 2000)); fc2 = register_module(\u0026#34;fc2\u0026#34;, torch::nn::Linear(2000, 1000)); fc3 = register_module(\u0026#34;fc3\u0026#34;, torch::nn::Linear(1000, 100)); fc4 = register_module(\u0026#34;fc4\u0026#34;, torch::nn::Linear(100, 2)); } // Implement Algorithm torch::Tensor forward(torch::Tensor x) { x = torch::relu(conv1_1-\u0026gt;forward(x)); x = torch::relu(conv1_2-\u0026gt;forward(x)); x = torch::max_pool2d(x, 2); x = torch::relu(conv2_1-\u0026gt;forward(x)); x = torch::relu(conv2_2-\u0026gt;forward(x)); x = torch::max_pool2d(x, 2); x = torch::relu(conv3_1-\u0026gt;forward(x)); x = torch::relu(conv3_2-\u0026gt;forward(x)); x = torch::relu(conv3_3-\u0026gt;forward(x)); x = torch::max_pool2d(x, 2); x = torch::relu(conv4_1-\u0026gt;forward(x)); x = torch::relu(conv4_2-\u0026gt;forward(x)); x = torch::relu(conv4_3-\u0026gt;forward(x)); x = torch::max_pool2d(x, 2); x = torch::relu(conv5_1-\u0026gt;forward(x)); x = torch::relu(conv5_2-\u0026gt;forward(x)); x = torch::relu(conv5_3-\u0026gt;forward(x)); x = torch::max_pool2d(x, 2); x = x.view({-1, 130*6*6}); x = torch::relu(fc1-\u0026gt;forward(x)); x = torch::relu(fc2-\u0026gt;forward(x)); x = torch::relu(fc3-\u0026gt;forward(x)); x = fc4-\u0026gt;forward(x); return torch::log_softmax(x, 1); } // Declare layers torch::nn::Conv2d conv1_1{nullptr}; torch::nn::Conv2d conv1_2{nullptr}; torch::nn::Conv2d conv2_1{nullptr}; torch::nn::Conv2d conv2_2{nullptr}; torch::nn::Conv2d conv3_1{nullptr}; torch::nn::Conv2d conv3_2{nullptr}; torch::nn::Conv2d conv3_3{nullptr}; torch::nn::Conv2d conv4_1{nullptr}; torch::nn::Conv2d conv4_2{nullptr}; torch::nn::Conv2d conv4_3{nullptr}; torch::nn::Conv2d conv5_1{nullptr}; torch::nn::Conv2d conv5_2{nullptr}; torch::nn::Conv2d conv5_3{nullptr}; torch::nn::Linear fc1{nullptr}, fc2{nullptr}, fc3{nullptr}, fc4{nullptr}; }; We will initialize this network and pass each batch of our data once in an epoch.\n// Initialize the Network auto net = std::make_shared\u0026lt;NetImpl\u0026gt;(); Training the Network on Dogs vs Cats Dataset We had before discussed code for training here. I suggest the reader to go through that blog in order to train the dataset. I\u0026rsquo;ll give more insights on training in the next blog!\nThat\u0026rsquo;s it for today. I\u0026rsquo;ll be back with Part-2 of this \u0026ldquo;Dogs vs Cats Classification\u0026rdquo; with training, experimentation and results. We\u0026rsquo;ll also discuss on using different networks, and in the Part-3, we\u0026rsquo;ll discuss using Transfer Learning for this classification task.\n","permalink":"https://krshrimali.github.io/posts/2019/07/classifying-dogs-vs-cats-using-pytorch-c-api-part-1/","summary":"\u003cp\u003eHi Everyone! So excited to be back with another blog in the series of PyTorch C++ Blogs.\u003c/p\u003e\n\u003cp\u003eToday, we are going to see a practical example of applying a CNN to a Custom Dataset - Dogs vs Cats. This is going to be a short post of showing results and discussion about hyperparameters and loss functions for the task, as code snippets and explanation has been provided \u003ca href=\"https://krshrimali.github.io/Training-Network-Using-Custom-Dataset-PyTorch-CPP/\"\u003ehere\u003c/a\u003e, \u003ca href=\"https://krshrimali.github.io/Custom-Data-Loading-Using-PyTorch-CPP-API/\"\u003ehere\u003c/a\u003e and \u003ca href=\"https://krshrimali.github.io/PyTorch-C\u0026#43;\u0026#43;-API/\"\u003ehere\u003c/a\u003e.\u003c/p\u003e\n\u003cp\u003e\u003cimg loading=\"lazy\" src=\"https://raw.githubusercontent.com/krshrimali/blog/main/assets/cover-images/Cover-Dogs-Cats.jpg\"\u003e\u003c/p\u003e","title":"Classifying Dogs vs Cats using PyTorch C++ API: Part-1"},{"content":"Recap of the last blog Before we move on, it\u0026rsquo;s important what we covered in the last blog. We\u0026rsquo;ll be going forward from loading Custom Dataset to now using the dataset to train our VGG-16 Network. Previously, we were able to load our custom dataset using the following template:\nNote: Those who are already aware of loading a custom dataset can skip this section.\nclass CustomDataset : public torch::data::dataset\u0026lt;CustomDataset\u0026gt; { private: // Declare 2 vectors of tensors for images and labels vector\u0026lt;torch::Tensor\u0026gt; images, labels; public: // Constructor CustomDataset(vector\u0026lt;string\u0026gt; list_images, vector\u0026lt;string\u0026gt; list_labels) { images = process_images(list_images); labels = process_labels(list_labels); }; // Override get() function to return tensor at location index torch::data::Example\u0026lt;\u0026gt; get(size_t index) override { torch::Tensor sample_img = images.at(index); torch::Tensor sample_label = labels.at(index); return {sample_img.clone(), sample_label.clone()}; }; // Return the length of data torch::optional\u0026lt;size_t\u0026gt; size() const override { return labels.size(); }; }; int main(int argc, char** argv) { vector\u0026lt;string\u0026gt; list_images; // list of path of images vector\u0026lt;int\u0026gt; list_labels; // list of integer labels // Dataset init and apply transforms - None! auto custom_dataset = CustomDataset(list_images, list_labels).map(torch::data::transforms::Stack\u0026lt;\u0026gt;()); } These were the steps we followed last time:\nOverview: How to pass batches to our network? Since we have our dataset loaded, let\u0026rsquo;s see how to pass batches of data to our network. Before we go on and see how PyTorch C++ API does it, let\u0026rsquo;s see how it\u0026rsquo;s done in Python.\ndataset_loader = torch.utils.data.DataLoader(custom_dataset, batch_size=4, shuffle=True) Just a short review of what DataLoader() class does: It loads the data and returns single or multiple iterators over the dataset. We pass in our object from Dataset class (here, custom_dataset). We will do the same process in C++ using the following template:\nauto data_loader = torch::data::make_data_loader\u0026lt;torch::data::samplers::SequentialSampler\u0026gt;( std::move(custom_dataset), batch_size ); In brief, we are loading our data using SequentialSampler class which samples our data in the same order that we provided it with. Have a look at the SequentialSampler class here.\nFor the definition of this function: torch::data::make_data_loader here. A short screenshot from the documentation is given below.\nLet\u0026rsquo;s go ahead and learn to iterate through our data loader and pass each batch of data and labels to our network. For once, imagine that we have a struct named Net which defines our network and forward() function which parses the data through each layer and returns the output.\nfor(auto\u0026amp; batch: *data_loader) { auto data = batch.data; auto target = batch.target.squeeze(); } So we have retrieved our data and label (target) - which also depends on the batch size. If you have batch_size as 4 in the torch::data::make_data_loader() function, then size of the target and data will be 4.\nDefining the Hyperparameters in Libtorch Remember the Hyperparameters we need to define for training? Let\u0026rsquo;s take a quick review of what they are:\nBatch Size Optimizer Loss Function We have used batch_size parameter above while making the data loader. For defining optimizer, we\u0026rsquo;ll go for Adam Optimizer here:\n// We need to define the network first auto net = std::make_shared\u0026lt;Net\u0026gt;(); torch::optim::Adam optimizer(net-\u0026gt;parameters(), torch::optim::AdamOptions(1e-3)); Note that the PyTorch C++ API supports below listed optimizers:\nRMSprop SGD Adam Adagrad LBFGS LossClosureOptimizer As mentioned in the documentation of torch.optim package:\nThe documentation is self explanatory, so all we need to do is pass parameters of our Network which will be optimized using our optimizer, and pass in the learning rate like above. To know about parameters we can pass through AdamOptions, check out this documentation page.\nLet\u0026rsquo;s go ahead and learn to define Loss Function in PyTorch C++ API. For an example, we\u0026rsquo;ll define nll_loss (Negative Log Likelihood Loss Function):\nauto output = net-\u0026gt;forward(data); auto loss = torch::nll_loss(output, target); // To backpropagate loss loss.backward() If you need to output the loss, use: loss.item\u0026lt;float\u0026gt;().\nTraining the Network We are close to our last step! Training the network is almost similar to the way we do in Python. That\u0026rsquo;s why, I\u0026rsquo;ll include the code snippet here which should be self explanatory (since we have already discussed the core parts of it).\ndataset_size = custom_dataset.size().value(); int n_epochs = 10; // Number of epochs for(int epoch=1; epoch\u0026lt;=n_epochs; epoch++) { for(auto\u0026amp; batch: *data_loader) { auto data = batch.data; auto target = batch.target.squeeze(); // Convert data to float32 format and target to Int64 format // Assuming you have labels as integers data = data.to(torch::kF2); target = target.to(torch::kInt64); // Clear the optimizer parameters optimizer.zero_grad(); auto output = net-\u0026gt;forward(data); auto loss = torch::nll_loss(output, target); // Backpropagate the loss loss.backward(); // Update the parameters optimizer.step(); cout \u0026lt;\u0026lt; \u0026#34;Train Epoch: %d/%ld [%5ld/%5d] Loss: %.4f\u0026#34; \u0026lt;\u0026lt; epoch \u0026lt;\u0026lt; n_epochs \u0026lt;\u0026lt; batch_index * batch.data.size(0) \u0026lt;\u0026lt; dataset_size \u0026lt;\u0026lt; loss.item\u0026lt;float\u0026gt;() \u0026lt;\u0026lt; endl; } } // Save the model torch::save(net, \u0026#34;best_model.pt\u0026#34;); In the next blog (coming soon), we\u0026rsquo;ll be discussing about Making Predictions using our network and will also show an example of training our network on a benchmark dataset.\n","permalink":"https://krshrimali.github.io/posts/2019/07/training-a-network-on-custom-dataset-using-pytorch-c-api/","summary":"\u003ch2 id=\"recap-of-the-last-blog\"\u003eRecap of the last blog\u003c/h2\u003e\n\u003cp\u003eBefore we move on, it\u0026rsquo;s important what we covered in the last blog. We\u0026rsquo;ll be going forward from loading Custom Dataset to now using the dataset to train our VGG-16 Network. Previously, we were able to load our custom dataset using the following template:\u003c/p\u003e","title":"Training a Network on Custom Dataset using PyTorch C++ API"},{"content":"I\u0026rsquo;m happy to announce a Series of Blog Posts on PyTorch C++ API. Check out the blogs in the series here.\nHappy Reading!\n","permalink":"https://krshrimali.github.io/posts/2019/07/announcing-a-series-of-blogs-on-pytorch-c-api/","summary":"\u003cp\u003e\u003cstrong\u003eI\u0026rsquo;m happy to announce a Series of Blog Posts on PyTorch C++ API\u003c/strong\u003e. Check out the blogs in the series \u003ca href=\"https://krshrimali.github.io/categories/pytorch/\"\u003ehere\u003c/a\u003e.\u003c/p\u003e\n\u003cp\u003eHappy Reading!\u003c/p\u003e","title":"Announcing a series of blogs on PyTorch C++ API"},{"content":"Overview: How C++ API loads data? In the last blog, we discussed application of a VGG-16 Network on MNIST Data. For those, who are reading this blog for the first time, here is how we had loaded MNIST data:\nauto data_loader = torch::data::make_data_loader\u0026lt;torch::data::samplers::SequentialSampler\u0026gt;( std::move(torch::data::datasets::MNIST(\u0026#34;../../data\u0026#34;).map(torch::data::transforms::Normalize\u0026lt;\u0026gt;(0.13707, 0.3081))).map( torch::data::transforms::Stack\u0026lt;\u0026gt;()), 64); Let\u0026rsquo;s break this piece by piece, as for beginners, this may be unclear. First, we ask the C++ API to load data (images and labels) into tensors.\n// Your data should be at: ../data position auto data_set = torch::data::datasets::MNIST(\u0026#34;../data\u0026#34;); If you have this question on how the API loads the images and labels to tensors - we\u0026rsquo;ll get to that. For now, just take it as a black box, which loads the data. Next, we apply transforms (like normalizing to ImageNet standards):\nauto data_set = data_set.map(torch::data::transforms::Normalize\u0026lt;\u0026gt;(0.13707, 0.3081)).map(torch::data::transforms::Stack\u0026lt;\u0026gt;()) For the sake of batch size, let\u0026rsquo;s divide the data for batch size as 64.\nstd::move(data_set, 64); Once this all is done, we can iterate through the data loader and pass each batch to the network. It\u0026rsquo;s time to understand how this all works, let\u0026rsquo;s go ahead and look at the source code of torch::data::datasets::MNIST class.\nnamespace torch { namespace data { namespace datasets { /// The MNIST dataset. class TORCH_API MNIST : public Dataset\u0026lt;MNIST\u0026gt; { public: /// The mode in which the dataset is loaded. enum class Mode { kTrain, kTest }; /// Loads the MNIST dataset from the root path. /// /// The supplied root path should contain the content of the unzipped /// MNIST dataset, available from http://yann.lecun.com/exdb/mnist. explicit MNIST(const std::string\u0026amp; root, Mode mode = Mode::kTrain); /// Returns the Example at the given index. Example\u0026lt;\u0026gt; get(size_t index) override; /// Returns the size of the dataset. optional\u0026lt;size_t\u0026gt; size() const override; /// Returns true if this is the training subset of MNIST. bool is_train() const noexcept; /// Returns all images stacked into a single tensor. const Tensor\u0026amp; images() const; /// Returns all targets stacked into a single tensor. const Tensor\u0026amp; targets() const; private: Tensor images_, targets_; }; } // namespace datasets } // namespace data } // namespace torch Reference: https://github.com/pytorch/pytorch/blob/master/torch/csrc/api/include/torch/data/datasets/mnist.h\nAssuming the reader has done some basic C++ before reading this, they will be very well aware of how to initialize a C++ Class. Let\u0026rsquo;s go step by step. What happens when we initialize the class? Let\u0026rsquo;s look at the definition of constructor of the class MNIST at mnist.cpp:\nMNIST::MNIST(const std::string\u0026amp; root, Mode mode) : images_(read_images(root, mode == Mode::kTrain)), targets_(read_targets(root, mode == Mode::kTrain)) {} Observing the above snippet, it\u0026rsquo;s clear that the constructor calls read_images(root, mode) and read_targets for loading images and labels into tensors. Let\u0026rsquo;s go to the source code of read_images() and read_targets().\nread_images(): Tensor read_images(const std::string\u0026amp; root, bool train) { // kTrainImagesFilename and kTestImagesFilename are specific to MNIST dataset here // No need for using join_paths here const auto path = join_paths(root, train ? kTrainImagesFilename : kTestImagesFilename); // Load images std::ifstream images(path, std::ios::binary); TORCH_CHECK(images, \u0026#34;Error opening images file at \u0026#34;, path); // kTrainSize = len(training data) // kTestSize = len(testing_data) const auto count = train ? kTrainSize : kTestSize; // Specific to MNIST data // From http://yann.lecun.com/exdb/mnist/ expect_int32(images, kImageMagicNumber); expect_int32(images, count); expect_int32(images, kImageRows); expect_int32(images, kImageColumns); // This converts images to tensors // Allocate an empty tensor of size of image (count, channels, height, width) auto tensor = torch::empty({count, 1, kImageRows, kImageColumns}, torch::kByte); // Read image and convert to tensor images.read(reinterpret_cast\u0026lt;char*\u0026gt;(tensor.data_ptr()), tensor.numel()); // Normalize the image from 0 to 255 to 0 to 1 return tensor.to(torch::kFloat32).div_(255); } read_targets(): Tensor read_targets(const std::string\u0026amp; root, bool train) { // Specific to MNIST dataset (kTrainImagesFilename and kTestTargetsFilename) const auto path = join_paths(root, train ? kTrainTargetsFilename : kTestTargetsFilename); // Read the labels std::ifstream targets(path, std::ios::binary); TORCH_CHECK(targets, \u0026#34;Error opening targets file at \u0026#34;, path); // kTrainSize = len(training_labels) // kTestSize = len(testing_labels) const auto count = train ? kTrainSize : kTestSize; expect_int32(targets, kTargetMagicNumber); expect_int32(targets, count); // Allocate an empty tensor of size of number of labels auto tensor = torch::empty(count, torch::kByte); // Convert to tensor targets.read(reinterpret_cast\u0026lt;char*\u0026gt;(tensor.data_ptr()), count); return tensor.to(torch::kInt64); } Since we are now done with how the constructor works, let\u0026rsquo;s go ahead and see what other functions does the class inherit.\nExample\u0026lt;\u0026gt; MNIST::get(size_t index) { return {images_[index], targets_[index]}; } optional\u0026lt;size_t\u0026gt; MNIST::size() const { return images_.size(0); } The above two functions: get(size_t) and size() are used to get a sample image and label and length of the data respectively.\nThe Pipeline Since we are now clear with the possible pipeline of loading custom data:\nRead Images and Labels Convert to Tensors Write get() and size() functions Initialize the class with paths of images and labels Pass it to the data loader Coding your own Custom Data Loader Let\u0026rsquo;s first write the template of our custom data loader:\n// Include libraries #include \u0026lt;ATen/ATen.h\u0026gt; #include \u0026lt;torch/torch.h\u0026gt; #include \u0026lt;iostream\u0026gt; #include \u0026lt;vector\u0026gt; #include \u0026lt;tuple\u0026gt; #include \u0026lt;opencv2/opencv.hpp\u0026gt; #include \u0026lt;string\u0026gt; /* Convert and Load image to tensor from location argument */ torch::Tensor read_data(std::string location) { // Read Data here // Return tensor form of the image return torch::Tensor; } /* Converts label to tensor type in the integer argument */ torch::Tensor read_label(int label) { // Read label here // Convert to tensor and return return torch::Tensor; } /* Loads images to tensor type in the string argument */ vector\u0026lt;torch::Tensor\u0026gt; process_images(vector\u0026lt;string\u0026gt; list_images) { cout \u0026lt;\u0026lt; \u0026#34;Reading Images...\u0026#34; \u0026lt;\u0026lt; endl; // Return vector of Tensor form of all the images return vector\u0026lt;torch::Tensor\u0026gt;; } /* Loads labels to tensor type in the string argument */ vector\u0026lt;torch::Tensor\u0026gt; process_labels(vector\u0026lt;string\u0026gt; list_labels) { cout \u0026lt;\u0026lt; \u0026#34;Reading Labels...\u0026#34; \u0026lt;\u0026lt; endl; // Return vector of Tensor form of all the labels return vector\u0026lt;torch::Tensor\u0026gt;; } class CustomDataset : public torch::data::dataset\u0026lt;CustomDataset\u0026gt; { private: // Declare 2 vectors of tensors for images and labels vector\u0026lt;torch::Tensor\u0026gt; images, labels; public: // Constructor CustomDataset(vector\u0026lt;string\u0026gt; list_images, vector\u0026lt;string\u0026gt; list_labels) { images = process_images(list_images); labels = process_labels(list_labels); }; // Override get() function to return tensor at location index torch::data::Example\u0026lt;\u0026gt; get(size_t index) override { torch::Tensor sample_img = images.at(index); torch::Tensor sample_label = labels.at(index); return {sample_img.clone(), sample_label.clone()}; }; // Return the length of data torch::optional\u0026lt;size_t\u0026gt; size() const override { return labels.size(); }; }; We are almost there, all we need to do is - Read Images and Labels to torch::Tensor type. I\u0026rsquo;ll be using OpenCV to read images, as it also helps later on to visualize results.\nReading Images:\nThe process to read an image in OpenCV is trivial: cv::imread(std::string location, int). We then convert it to a tensor. Note that a tensor is of form (batch size, channels, height, width), so we also permute the tensor to that form.\ntorch::Tensor read_data(std::string loc) { // Read Image from the location of image cv::Mat img = cv::imread(loc, 1); // Convert image to tensor torch::Tensor img_tensor = torch::from_blob(img.data, {img.rows, img.cols, 3}, torch::kByte); img_tensor = img_tensor.permute({2, 0, 1}); // Channels x Height x Width return img_tensor.clone(); }; Reading Labels:\n// Read Label (int) and convert to torch::Tensor type torch::Tensor read_label(int label) { torch::Tensor label_tensor = torch::full({1}, label); return label_tensor.clone(); } Final Code Let\u0026rsquo;s wrap up the code!\n// Include libraries #include \u0026lt;ATen/ATen.h\u0026gt; #include \u0026lt;torch/torch.h\u0026gt; #include \u0026lt;iostream\u0026gt; #include \u0026lt;vector\u0026gt; #include \u0026lt;tuple\u0026gt; #include \u0026lt;opencv2/opencv.hpp\u0026gt; #include \u0026lt;string\u0026gt; /* Convert and Load image to tensor from location argument */ torch::Tensor read_data(std::string loc) { // Read Data here // Return tensor form of the image cv::Mat img = cv::imread(loc, 1); cv::resize(img, img, cv::Size(1920, 1080), cv::INTER_CUBIC); std::cout \u0026lt;\u0026lt; \u0026#34;Sizes: \u0026#34; \u0026lt;\u0026lt; img.size() \u0026lt;\u0026lt; std::endl; torch::Tensor img_tensor = torch::from_blob(img.data, {img.rows, img.cols, 3}, torch::kByte); img_tensor = img_tensor.permute({2, 0, 1}); // Channels x Height x Width return img_tensor.clone(); } /* Converts label to tensor type in the integer argument */ torch::Tensor read_label(int label) { // Read label here // Convert to tensor and return torch::Tensor label_tensor = torch::full({1}, label); return label_tensor.clone(); } /* Loads images to tensor type in the string argument */ vector\u0026lt;torch::Tensor\u0026gt; process_images(vector\u0026lt;string\u0026gt; list_images) { cout \u0026lt;\u0026lt; \u0026#34;Reading Images...\u0026#34; \u0026lt;\u0026lt; endl; // Return vector of Tensor form of all the images vector\u0026lt;torch::Tensor\u0026gt; states; for (std::vector\u0026lt;string\u0026gt;::iterator it = list_images.begin(); it != list_images.end(); ++it) { torch::Tensor img = read_data(*it); states.push_back(img); } return states; } /* Loads labels to tensor type in the string argument */ vector\u0026lt;torch::Tensor\u0026gt; process_labels(vector\u0026lt;string\u0026gt; list_labels) { cout \u0026lt;\u0026lt; \u0026#34;Reading Labels...\u0026#34; \u0026lt;\u0026lt; endl; // Return vector of Tensor form of all the labels vector\u0026lt;torch::Tensor\u0026gt; labels; for (std::vector\u0026lt;int\u0026gt;::iterator it = list_labels.begin(); it != list_labels.end(); ++it) { torch::Tensor label = read_label(*it); labels.push_back(label); } return labels; } class CustomDataset : public torch::data::dataset\u0026lt;CustomDataset\u0026gt; { private: // Declare 2 vectors of tensors for images and labels vector\u0026lt;torch::Tensor\u0026gt; images, labels; public: // Constructor CustomDataset(vector\u0026lt;string\u0026gt; list_images, vector\u0026lt;string\u0026gt; list_labels) { images = process_images(list_images); labels = process_labels(list_labels); }; // Override get() function to return tensor at location index torch::data::Example\u0026lt;\u0026gt; get(size_t index) override { torch::Tensor sample_img = images.at(index); torch::Tensor sample_label = labels.at(index); return {sample_img.clone(), sample_label.clone()}; }; // Return the length of data torch::optional\u0026lt;size_t\u0026gt; size() const override { return labels.size(); }; }; int main(int argc, char** argv) { vector\u0026lt;string\u0026gt; list_images; // list of path of images vector\u0026lt;int\u0026gt; list_labels; // list of integer labels // Dataset init and apply transforms - None! auto custom_dataset = CustomDataset(list_images, list_labels).map(torch::data::transforms::Stack\u0026lt;\u0026gt;()); } That\u0026rsquo;s it for today! In the next blog, we\u0026rsquo;ll use this custom data loader and implement a CNN on our data. By then, happy learning. Hope you liked this blog. :)\n","permalink":"https://krshrimali.github.io/posts/2019/07/custom-data-loading-using-pytorch-c-api/","summary":"\u003ch2 id=\"overview-how-c-api-loads-data\"\u003eOverview: How C++ API loads data?\u003c/h2\u003e\n\u003cp\u003e\u003cimg loading=\"lazy\" src=\"https://raw.githubusercontent.com/krshrimali/blog/main/assets/cover-images/Cover-Custom-Data.png\"\u003e\u003c/p\u003e\n\u003cp\u003eIn the last blog, we discussed application of a VGG-16 Network on MNIST Data. For those, who are reading this blog for the first time, here is how we had loaded MNIST data:\u003c/p\u003e\n\u003cdiv class=\"highlight\"\u003e\u003cpre tabindex=\"0\" style=\"color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;\"\u003e\u003ccode class=\"language-cpp\" data-lang=\"cpp\"\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#66d9ef\"\u003eauto\u003c/span\u003e data_loader \u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e torch\u003cspan style=\"color:#f92672\"\u003e::\u003c/span\u003edata\u003cspan style=\"color:#f92672\"\u003e::\u003c/span\u003emake_data_loader\u003cspan style=\"color:#f92672\"\u003e\u0026lt;\u003c/span\u003etorch\u003cspan style=\"color:#f92672\"\u003e::\u003c/span\u003edata\u003cspan style=\"color:#f92672\"\u003e::\u003c/span\u003esamplers\u003cspan style=\"color:#f92672\"\u003e::\u003c/span\u003eSequentialSampler\u003cspan style=\"color:#f92672\"\u003e\u0026gt;\u003c/span\u003e(\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\t\t\tstd\u003cspan style=\"color:#f92672\"\u003e::\u003c/span\u003emove(torch\u003cspan style=\"color:#f92672\"\u003e::\u003c/span\u003edata\u003cspan style=\"color:#f92672\"\u003e::\u003c/span\u003edatasets\u003cspan style=\"color:#f92672\"\u003e::\u003c/span\u003eMNIST(\u003cspan style=\"color:#e6db74\"\u003e\u0026#34;../../data\u0026#34;\u003c/span\u003e).map(torch\u003cspan style=\"color:#f92672\"\u003e::\u003c/span\u003edata\u003cspan style=\"color:#f92672\"\u003e::\u003c/span\u003etransforms\u003cspan style=\"color:#f92672\"\u003e::\u003c/span\u003eNormalize\u003cspan style=\"color:#f92672\"\u003e\u0026lt;\u0026gt;\u003c/span\u003e(\u003cspan style=\"color:#ae81ff\"\u003e0.13707\u003c/span\u003e, \u003cspan style=\"color:#ae81ff\"\u003e0.3081\u003c/span\u003e))).map(\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\t\t\t\ttorch\u003cspan style=\"color:#f92672\"\u003e::\u003c/span\u003edata\u003cspan style=\"color:#f92672\"\u003e::\u003c/span\u003etransforms\u003cspan style=\"color:#f92672\"\u003e::\u003c/span\u003eStack\u003cspan style=\"color:#f92672\"\u003e\u0026lt;\u0026gt;\u003c/span\u003e()), \u003cspan style=\"color:#ae81ff\"\u003e64\u003c/span\u003e);\n\u003c/span\u003e\u003c/span\u003e\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003cp\u003eLet\u0026rsquo;s break this piece by piece, as for beginners, this may be unclear. First, we ask the C++ API to load data (images and labels) into tensors.\u003c/p\u003e","title":"Custom Data Loading using PyTorch C++ API"},{"content":"Environment Setup [Ubuntu 16.04, 18.04] Note: If you have already finished installing PyTorch C++ API, please skip this section.\nDownload libtorch:\nCPU Version: wget https://download.pytorch.org/libtorch/cpu/libtorch-shared-with-deps-latest.zip -O libtorch.zip GPU Version (CUDA 9.0): wget https://download.pytorch.org/libtorch/cu90/libtorch-shared-with-deps-latest.zip -O libtorch.zip GPU Version (CUDA 10.0): wget https://download.pytorch.org/libtorch/cu100/libtorch-shared-with-deps-latest.zip Unzip libtorch.zip:\nunzip libtorch.zip We\u0026rsquo;ll use the absolute path of extracted directory (libtorch) later on.\nImplementation The VGG-16 Network is shown in the Figure below.\nWe\u0026rsquo;ll start of by first including libtorch header file.\n#include \u0026lt;torch/torch.h\u0026gt;\nWe\u0026rsquo;ll then go ahead and define the network. We\u0026rsquo;ll inherit layers from torch::nn::Module.\n/* Sample code for training a FCN on MNIST dataset using PyTorch C++ API */ /* This code uses VGG-16 Layer Network */ struct Net: torch::nn::Module { // VGG-16 Layer // conv1_1 - conv1_2 - pool 1 - conv2_1 - conv2_2 - pool 2 - conv3_1 - conv3_2 - conv3_3 - pool 3 - // conv4_1 - conv4_2 - conv4_3 - pool 4 - conv5_1 - conv5_2 - conv5_3 - pool 5 - fc6 - fc7 - fc8 // Note: pool 5 not implemented as no need for MNIST dataset Net() { // Initialize VGG-16 // On how to pass strides and padding: https://github.com/pytorch/pytorch/issues/12649#issuecomment-430156160 conv1_1 = register_module(\u0026#34;conv1_1\u0026#34;, torch::nn::Conv2d(torch::nn::Conv2dOptions(1, 10, 3).padding(1))); conv1_2 = register_module(\u0026#34;conv1_2\u0026#34;, torch::nn::Conv2d(torch::nn::Conv2dOptions(10, 20, 3).padding(1))); // Insert pool layer conv2_1 = register_module(\u0026#34;conv2_1\u0026#34;, torch::nn::Conv2d(torch::nn::Conv2dOptions(20, 30, 3).padding(1))); conv2_2 = register_module(\u0026#34;conv2_2\u0026#34;, torch::nn::Conv2d(torch::nn::Conv2dOptions(30, 40, 3).padding(1))); // Insert pool layer conv3_1 = register_module(\u0026#34;conv3_1\u0026#34;, torch::nn::Conv2d(torch::nn::Conv2dOptions(40, 50, 3).padding(1))); conv3_2 = register_module(\u0026#34;conv3_2\u0026#34;, torch::nn::Conv2d(torch::nn::Conv2dOptions(50, 60, 3).padding(1))); conv3_3 = register_module(\u0026#34;conv3_3\u0026#34;, torch::nn::Conv2d(torch::nn::Conv2dOptions(60, 70, 3).padding(1))); // Insert pool layer conv4_1 = register_module(\u0026#34;conv4_1\u0026#34;, torch::nn::Conv2d(torch::nn::Conv2dOptions(70, 80, 3).padding(1))); conv4_2 = register_module(\u0026#34;conv4_2\u0026#34;, torch::nn::Conv2d(torch::nn::Conv2dOptions(80, 90, 3).padding(1))); conv4_3 = register_module(\u0026#34;conv4_3\u0026#34;, torch::nn::Conv2d(torch::nn::Conv2dOptions(90, 100, 3).padding(1))); // Insert pool layer conv5_1 = register_module(\u0026#34;conv5_1\u0026#34;, torch::nn::Conv2d(torch::nn::Conv2dOptions(100, 110, 3).padding(1))); conv5_2 = register_module(\u0026#34;conv5_2\u0026#34;, torch::nn::Conv2d(torch::nn::Conv2dOptions(110, 120, 3).padding(1))); conv5_3 = register_module(\u0026#34;conv5_3\u0026#34;, torch::nn::Conv2d(torch::nn::Conv2dOptions(120, 130, 3).padding(1))); // Insert pool layer fc1 = register_module(\u0026#34;fc1\u0026#34;, torch::nn::Linear(130, 50)); fc2 = register_module(\u0026#34;fc2\u0026#34;, torch::nn::Linear(50, 20)); fc3 = register_module(\u0026#34;fc3\u0026#34;, torch::nn::Linear(20, 10)); } // Implement Algorithm torch::Tensor forward(torch::Tensor x) { x = torch::relu(conv1_1-\u0026gt;forward(x)); x = torch::relu(conv1_2-\u0026gt;forward(x)); x = torch::max_pool2d(x, 2); x = torch::relu(conv2_1-\u0026gt;forward(x)); x = torch::relu(conv2_2-\u0026gt;forward(x)); x = torch::max_pool2d(x, 2); x = torch::relu(conv3_1-\u0026gt;forward(x)); x = torch::relu(conv3_2-\u0026gt;forward(x)); x = torch::relu(conv3_3-\u0026gt;forward(x)); x = torch::max_pool2d(x, 2); x = torch::relu(conv4_1-\u0026gt;forward(x)); x = torch::relu(conv4_2-\u0026gt;forward(x)); x = torch::relu(conv4_3-\u0026gt;forward(x)); x = torch::max_pool2d(x, 2); x = torch::relu(conv5_1-\u0026gt;forward(x)); x = torch::relu(conv5_2-\u0026gt;forward(x)); x = torch::relu(conv5_3-\u0026gt;forward(x)); x = x.view({-1, 130}); x = torch::relu(fc1-\u0026gt;forward(x)); x = torch::relu(fc2-\u0026gt;forward(x)); x = fc3-\u0026gt;forward(x); return torch::log_softmax(x, 1); } // Declare layers torch::nn::Conv2d conv1_1{nullptr}; torch::nn::Conv2d conv1_2{nullptr}; torch::nn::Conv2d conv2_1{nullptr}; torch::nn::Conv2d conv2_2{nullptr}; torch::nn::Conv2d conv3_1{nullptr}; torch::nn::Conv2d conv3_2{nullptr}; torch::nn::Conv2d conv3_3{nullptr}; torch::nn::Conv2d conv4_1{nullptr}; torch::nn::Conv2d conv4_2{nullptr}; torch::nn::Conv2d conv4_3{nullptr}; torch::nn::Conv2d conv5_1{nullptr}; torch::nn::Conv2d conv5_2{nullptr}; torch::nn::Conv2d conv5_3{nullptr}; torch::nn::Linear fc1{nullptr}, fc2{nullptr}, fc3{nullptr}; }; Once done, we can go ahead and test the network on our sample dataset. Let\u0026rsquo;s go ahead and load data first. We\u0026rsquo;ll be using 10 epochs, learning rate (0.01), and nll_loss as loss functio.\nint main() { // Create multi-threaded data loader for MNIST data auto data_loader = torch::data::make_data_loader\u0026lt;torch::data::samplers::SequentialSampler\u0026gt;( std::move(torch::data::datasets::MNIST(\u0026#34;/absolute/path/to/data\u0026#34;).map(torch::data::transforms::Normalize\u0026lt;\u0026gt;(0.13707, 0.3081)).map( torch::data::transforms::Stack\u0026lt;\u0026gt;())), 64); // Build VGG-16 Network auto net = std::make_shared\u0026lt;Net\u0026gt;(); torch::optim::SGD optimizer(net-\u0026gt;parameters(), 0.01); // Learning Rate 0.01 // net.train(); for(size_t epoch=1; epoch\u0026lt;=10; ++epoch) { size_t batch_index = 0; // Iterate data loader to yield batches from the dataset for (auto\u0026amp; batch: *data_loader) { // Reset gradients optimizer.zero_grad(); // Execute the model torch::Tensor prediction = net-\u0026gt;forward(batch.data); // Compute loss value torch::Tensor loss = torch::nll_loss(prediction, batch.target); // Compute gradients loss.backward(); // Update the parameters optimizer.step(); // Output the loss and checkpoint every 100 batches if (++batch_index % 2 == 0) { std::cout \u0026lt;\u0026lt; \u0026#34;Epoch: \u0026#34; \u0026lt;\u0026lt; epoch \u0026lt;\u0026lt; \u0026#34; | Batch: \u0026#34; \u0026lt;\u0026lt; batch_index \u0026lt;\u0026lt; \u0026#34; | Loss: \u0026#34; \u0026lt;\u0026lt; loss.item\u0026lt;float\u0026gt;() \u0026lt;\u0026lt; std::endl; torch::save(net, \u0026#34;net.pt\u0026#34;); } } } } For code, check out my repo here: https://github.com/krshrimali/Digit-Recognition-MNIST-SVHN-PyTorch-CPP\nIn the next blog, we will discuss about another network on MNIST and SVHN Dataset.\nReferences https://pytorch.org/cppdocs/ http://yann.lecun.com/exdb/mnist/ ","permalink":"https://krshrimali.github.io/posts/2019/06/introduction-to-pytorch-c-api-mnist-digit-recognition-using-vgg-16-network/","summary":"\u003ch1 id=\"environment-setup-ubuntu-1604-1804\"\u003eEnvironment Setup [Ubuntu 16.04, 18.04]\u003c/h1\u003e\n\u003cp\u003e\u003cem\u003eNote: If you have already finished installing PyTorch C++ API, please skip this section.\u003c/em\u003e\u003c/p\u003e\n\u003col\u003e\n\u003cli\u003e\n\u003cp\u003eDownload \u003ccode\u003elibtorch\u003c/code\u003e:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eCPU Version: \u003ccode\u003ewget https://download.pytorch.org/libtorch/cpu/libtorch-shared-with-deps-latest.zip -O libtorch.zip\u003c/code\u003e\u003c/li\u003e\n\u003cli\u003eGPU Version (CUDA 9.0): \u003ccode\u003ewget https://download.pytorch.org/libtorch/cu90/libtorch-shared-with-deps-latest.zip -O libtorch.zip\u003c/code\u003e\u003c/li\u003e\n\u003cli\u003eGPU Version (CUDA 10.0): \u003ccode\u003ewget https://download.pytorch.org/libtorch/cu100/libtorch-shared-with-deps-latest.zip\u003c/code\u003e\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003eUnzip \u003ccode\u003elibtorch.zip\u003c/code\u003e:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\u003ccode\u003eunzip libtorch.zip\u003c/code\u003e\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003c/ol\u003e\n\u003cp\u003eWe\u0026rsquo;ll use the \u003ccode\u003eabsolute\u003c/code\u003e path of extracted directory (\u003ccode\u003elibtorch\u003c/code\u003e) later on.\u003c/p\u003e\n\u003ch1 id=\"implementation\"\u003eImplementation\u003c/h1\u003e\n\u003cp\u003eThe VGG-16 Network is shown in the Figure below.\u003c/p\u003e\n\u003cp\u003e\u003cimg loading=\"lazy\" src=\"https://raw.githubusercontent.com/krshrimali/blog/main/assets/blogs/VGG-16-Architecture-resized.png\"\u003e\u003c/p\u003e\n\u003cp\u003eWe\u0026rsquo;ll start of by first including \u003ccode\u003elibtorch\u003c/code\u003e header file.\u003c/p\u003e","title":"Introduction to PyTorch C++ API: MNIST Digit Recognition using VGG-16 Network"},{"content":"Hello World! I\u0026rsquo;m Kushashwa Ravi Shrimali (friends call me Kush, world knows me as krshrimali), a Computer Science and Engineering Graduate from IIIT-NR. I work as a Research Engineer at Lightning AI, mostly on PyTorch Lightning Ecosystem. Previously, I work as a Software Developer (PyTorch dev team) at Quansight Labs and as an Intern at NVIDIA\u0026rsquo;s GPU Development Frameworks team. I love development, optimization and exploring the possibilities of computer science! Have explored a lot of fields during my graduation, development is what I found the most intuitive for myself.\nI try going live on my YouTube channel whenever I find the time. Most of my projects and work are demonstrated there, in case it interests you! :)\nOver the weekends - I contribute to open source, and write blogs (where you are right now). But that\u0026rsquo;s not it, I like singing (not that I sing well xD), dancing on any random music I like and discussing technology (mostly Linux, vim, and programming) \u0026amp; politics.\nI love writing impactful codes! My work doesn\u0026rsquo;t stop at getting the accuracy. I love optimizing the models to help deploy in real-time.\nPlease navigate to categories, to start reading my blogs (category-wise).\nWork Experience Lightning AI (Research Engineer) (February 2022 to Present) Lead of Lightning Flash library. One of the core maintainers of PyTorch Lightning and Lightning framework. Quansight Labs (Software Developer) (June 2021 to February 2022) Contributed to PyTorch, check my work here: https://github.com/pytorch/pytorch/pulls/krshrimali NVIDIA (Deep Learning Frameworks Team, PyTorch), Santa Clara, US On site internship at NVIDIA Head Quarters, Santa Clara. See the following PRs if you want to know what I worked on. https://github.com/pytorch/pytorch/pull/33322 https://github.com/pytorch/pytorch/pull/33063 https://github.com/pytorch/pytorch/pull/33063 Mentor: Michael Carilli (Senior Software Developer, NVIDIA) Manager: Christian Sarofeen Care AI (Software Developer Intern, AI), Florida, US (April 2020 to August 2020) Remotely working (due to COVID-19 pandemic). Major responsibilities include: Optimizing workflow using TensorRT (mainly using C++ API). Applying Deep Learning, Speech Recognition and Machine Learning models on Android Apps. Rapid Rich Object Search (ROSE) Labs, NTU Singapore On-site internship at ROSE Labs, NTU Singapore under Prof. Alex Kot (Director, ROSE Labs). Worked on Automatic License Plate Recognition technique in real time using PyTorch in both C++ and Python. Used YOLO based network and achieved 85% accuracy for Single Line License Plates and Double Line License Plates (tested on real time dataset of Singaporean Vehicles). Mentor: Prof. Alex Kot, Dennis Sng (Deputy Director \u0026amp; Principal Scientist) and Devadeep Shyam (Project Mangaer, AI) Big Vision LLC and LearnOpenCV, California Worked from home in writing blogs and working for clients for Big Vision LLC. Became OpenCV Contributor during my internship, and worked on several projects using C, C++ and Python. Mentor: Dr. Satya Mallick (CEO, LearnOpenCV) Indian Institute of Information Technology and Management, Gwalior On-site internship at IIITM Gwalior and worked on A* Search Algorithm to understand communication between a robot and a UAV. Co-authored a paper with Ashish Upadhyay and Prof. Anupam Shukla: https://www.sciencedirect.com/science/article/pii/S1877050918309979. Mentor: Prof. Anupam Shukla (now Director, IIIT Pune) OpenStudy, Palo Alto The responsibilities included to actively promote the website and it‚Äôs cause amongst the students and to guide them through the beginning steps for the website. Made sure the smooth following of Code of Conduct of OpenStudy. Also worked as a leader of OpenStudy Newsletter Programme for two editions as well as Designer in one edition. Worked under Preetha Ram and Ashwin Ram (now: Technical Director of AI, Google Cloud) Student Activity Concil, IIIT Naya Raipur Coordinated clubs running under SAC: Artificial Intelligence and Machine Learning Club, The Society of Coders. Also became the Technical Coordinator of Annual Technical and Cultural Fest - TechNovate 2019. Twitch and YouTube Channels I stream every day at 9 PM IST (8:30 AM PT), so if you are interested - please hop in and say hi!\nTwitch: https://twitch.tv/buffetcodes YouTube: https://youtube.com/c/kushashwaraviShrimali Non-Academic Interests and Achievements I involve myself in non-academic activities like Debating, blogging, playing Table Tennis and more. I was also a Scout in my high school. I love World History and love discussing on social and international topics. I\u0026rsquo;ve won several competitions in Debates (more about them in my CV).\nContact Me Feel free to reach out to me on Twitter, or on email: kushashwaravishrimali@gmail.com\n","permalink":"https://krshrimali.github.io/about/","summary":"\u003cp\u003eHello World! I\u0026rsquo;m Kushashwa Ravi Shrimali (friends call me Kush, world knows me as krshrimali), a Computer Science and Engineering Graduate from IIIT-NR. I work as a Research Engineer at \u003ca href=\"http://lightning.ai/\"\u003eLightning AI\u003c/a\u003e, mostly on \u003ca href=\"https://github.com/PyTorchLightning\"\u003ePyTorch Lightning Ecosystem\u003c/a\u003e. Previously, I work as a Software Developer (PyTorch dev team) at Quansight Labs and as an Intern at NVIDIA\u0026rsquo;s GPU Development Frameworks team. I love development, optimization and exploring the possibilities of computer science! Have explored a lot of fields during my graduation, development is what I found the most intuitive for myself.\u003c/p\u003e","title":"About"},{"content":"","permalink":"https://krshrimali.github.io/categories/","summary":"","title":"Categories"}]